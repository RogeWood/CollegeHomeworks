Computer
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
For other uses, see Computer (disambiguation).
Computer
Acer Aspire 8920 Gemstone.jpgColumbia Supercomputer - NASA Advanced 
Supercomputing Facility.jpgIntertec Superbrain.jpg
2010-01-26-technikkrempel-by-RalfR-05.jpgThinking Machines Connection Machine 
CM-5 Frostburg 2.jpgG5 supplying Wikipedia via Gigabit at the Lange Nacht der 
Wissenschaften 2006 in Dresden.JPG
DM IBM S360.jpgAcorn BBC Master Series Microcomputer.jpgDell PowerEdge 
Servers.jpg
Computers and computing devices from different eras
A computer is a machine that can be instructed to carry out sequences of 
arithmetic or logical operations automatically via computer programming. Modern 
computers have the ability to follow generalized sets of operations, called 
programs. These programs enable computers to perform an extremely wide range of 
tasks. A "complete" computer including the hardware, the operating system (main 
software), and peripheral equipment required and used for "full" operation can 
be referred to as a computer system. This term may as well be used for a group 
of computers that are connected and work together, in particular a computer 
network or computer cluster.

Computers are used as control systems for a wide variety of industrial and 
consumer devices. This includes simple special purpose devices like microwave 
ovens and remote controls, factory devices such as industrial robots and 
computer-aided design, and also general purpose devices like personal computers 
and mobile devices such as smartphones. The Internet is run on computers and it 
connects hundreds of millions of other computers and their users.

Early computers were only conceived as calculating devices. Since ancient 
times, simple manual devices like the abacus aided people in doing 
calculations. Early in the Industrial Revolution, some mechanical devices were 
built to automate long tedious tasks, such as guiding patterns for looms. More 
sophisticated electrical machines did specialized analog calculations in the 
early 20th century. The first digital electronic calculating machines were 
developed during World War II. The first semiconductor transistors in the late 
1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic 
integrated circuit (IC) chip technologies in the late 1950s, leading to the 
microprocessor and the microcomputer revolution in the 1970s. The speed, power 
and versatility of computers have been increasing dramatically ever since then, 
with MOS transistor counts increasing at a rapid pace (as predicted by Moore's 
law), leading to the Digital Revolution during the late 20th to early 21st 
centuries.

Conventionally, a modern computer consists of at least one processing element, 
typically a central processing unit (CPU) in the form of a 
metal-oxide-semiconductor (MOS) microprocessor, along with some type of 
computer memory, typically MOS semiconductor memory chips. The processing 
element carries out arithmetic and logical operations, and a sequencing and 
control unit can change the order of operations in response to stored 
information. Peripheral devices include input devices (keyboards, mice, 
joystick, etc.), output devices (monitor screens, printers, etc.), and 
input/output devices that perform both functions (e.g., the 2000s-era 
touchscreen). Peripheral devices allow information to be retrieved from an 
external source and they enable the result of operations to be saved and 
retrieved.


Contents
1	Etymology
2	History
2.1	Pre-20th century
2.2	First computing device
2.3	Analog computers
2.4	Digital computers
2.5	Modern computers
2.6	Mobile computers
3	Types
3.1	By architecture
3.2	By size and form-factor
4	Hardware
4.1	History of computing hardware
4.2	Other hardware topics
4.3	Input devices
4.4	Output devices
4.5	Control unit
4.6	Central processing unit (CPU)
4.7	Arithmetic logic unit (ALU)
4.8	Memory
4.9	Input/output (I/O)
4.10	Multitasking
4.11	Multiprocessing
5	Software
5.1	Languages
5.2	Programs
6	Networking and the Internet
7	Unconventional computers
8	Future
8.1	Computer architecture paradigms
8.2	Artificial intelligence
9	Professions and organizations
10	See also
11	References
12	Notes
13	External links
Etymology
A human computer.
A female computer, with microscope and calculator, 1952
According to the Oxford English Dictionary, the first known use of the word 
"computer" was in 1613 in a book called The Yong Mans Gleanings by English 
writer Richard Braithwait: "I haue [sic] read the truest computer of Times, and 
the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into 
a short number." This usage of the term referred to a human computer, a person 
who carried out calculations or computations. The word continued with the same 
meaning until the middle of the 20th century. During the latter part of this 
period women were often hired as computers because they could be paid less than 
their male counterparts.[1] By 1943, most human computers were women.[2]

The Online Etymology Dictionary gives the first attested use of "computer" in 
the 1640s, meaning "one who calculates"; this is an "agent noun from compute 
(v.)". The Online Etymology Dictionary states that the use of the term to mean 
"'calculating machine' (of any type) is from 1897." The Online Etymology 
Dictionary indicates that the "modern use" of the term, to mean "programmable 
digital electronic computer" dates from "1945 under this name; [in a] 
theoretical [sense] from 1937, as Turing machine".[3]

History
Main article: History of computing hardware
Pre-20th century

The Ishango bone, a bone tool dating back to prehistoric Africa.
Devices have been used to aid computation for thousands of years, mostly using 
one-to-one correspondence with fingers. The earliest counting device was 
probably a form of tally stick. Later record keeping aids throughout the 
Fertile Crescent included calculi (clay spheres, cones, etc.) which represented 
counts of items, probably livestock or grains, sealed in hollow unbaked clay 
containers.[4][5] The use of counting rods is one example.


The Chinese suanpan (算盘). The number represented on this abacus is 
6,302,715,408.
The abacus was initially used for arithmetic tasks. The Roman abacus was 
developed from devices used in Babylonia as early as 2400 BC. Since then, many 
other forms of reckoning boards or tables have been invented. In a medieval 
European counting house, a checkered cloth would be placed on a table, and 
markers moved around on it according to certain rules, as an aid to calculating 
sums of money.


The Antikythera mechanism, dating back to ancient Greece circa 150–100 BC, is 
an early analog computing device.
The Antikythera mechanism is believed to be the earliest mechanical analog 
"computer", according to Derek J. de Solla Price.[6] It was designed to 
calculate astronomical positions. It was discovered in 1901 in the Antikythera 
wreck off the Greek island of Antikythera, between Kythera and Crete, and has 
been dated to c. 100 BC. Devices of a level of complexity comparable to that 
of the Antikythera mechanism would not reappear until a thousand years later.

Many mechanical aids to calculation and measurement were constructed for 
astronomical and navigation use. The planisphere was a star chart invented by 
Abū Rayhān al-Bīrūnī in the early 11th century.[7] The astrolabe was 
invented in the Hellenistic world in either the 1st or 2nd centuries BC and is 
often attributed to Hipparchus. A combination of the planisphere and dioptra, 
the astrolabe was effectively an analog computer capable of working out several 
different kinds of problems in spherical astronomy. An astrolabe incorporating 
a mechanical calendar computer[8][9] and gear-wheels was invented by Abi Bakr 
of Isfahan, Persia in 1235.[10] Abū Rayhān al-Bīrūnī invented the first 
mechanical geared lunisolar calendar astrolabe,[11] an early fixed-wired 
knowledge processing machine[12] with a gear train and gear-wheels,[13] 
c. 1000 AD.

The sector, a calculating instrument used for solving problems in proportion, 
trigonometry, multiplication and division, and for various functions, such as 
squares and cube roots, was developed in the late 16th century and found 
application in gunnery, surveying and navigation.

The planimeter was a manual instrument to calculate the area of a closed figure 
by tracing over it with a mechanical linkage.


A slide rule.
The slide rule was invented around 1620–1630, shortly after the publication 
of the concept of the logarithm. It is a hand-operated analog computer for 
doing multiplication and division. As slide rule development progressed, added 
scales provided reciprocals, squares and square roots, cubes and cube roots, as 
well as transcendental functions such as logarithms and exponentials, circular 
and hyperbolic trigonometry and other functions. Slide rules with special 
scales are still used for quick performance of routine calculations, such as 
the E6B circular slide rule used for time and distance calculations on light 
aircraft.

In the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll 
(automaton) that could write holding a quill pen. By switching the number and 
order of its internal wheels different letters, and hence different messages, 
could be produced. In effect, it could be mechanically "programmed" to read 
instructions. Along with two other complex machines, the doll is at the Musée 
d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.[14]

The tide-predicting machine invented by Sir William Thomson in 1872 was of 
great utility to navigation in shallow waters. It used a system of pulleys and 
wires to automatically calculate predicted tide levels for a set period at a 
particular location.

The differential analyser, a mechanical analog computer designed to solve 
differential equations by integration, used wheel-and-disc mechanisms to 
perform the integration. In 1876, Lord Kelvin had already discussed the 
possible construction of such calculators, but he had been stymied by the 
limited output torque of the ball-and-disk integrators.[15] In a differential 
analyzer, the output of one integrator drove the input of the next integrator, 
or a graphing output. The torque amplifier was the advance that allowed these 
machines to work. Starting in the 1920s, Vannevar Bush and others developed 
mechanical differential analyzers.

First computing device

A portion of Babbage's Difference engine.
Charles Babbage, an English mechanical engineer and polymath, originated the 
concept of a programmable computer. Considered the "father of the 
computer",[16] he conceptualized and invented the first mechanical computer in 
the early 19th century. After working on his revolutionary difference engine, 
designed to aid in navigational calculations, in 1833 he realized that a much 
more general design, an Analytical Engine, was possible. The input of programs 
and data was to be provided to the machine via punched cards, a method being 
used at the time to direct mechanical looms such as the Jacquard loom. For 
output, the machine would have a printer, a curve plotter and a bell. The 
machine would also be able to punch numbers onto cards to be read in later. The 
Engine incorporated an arithmetic logic unit, control flow in the form of 
conditional branching and loops, and integrated memory, making it the first 
design for a general-purpose computer that could be described in modern terms 
as Turing-complete.[17][18]

The machine was about a century ahead of its time. All the parts for his 
machine had to be made by hand – this was a major problem for a device with 
thousands of parts. Eventually, the project was dissolved with the decision of 
the British Government to cease funding. Babbage's failure to complete the 
analytical engine can be chiefly attributed to political and financial 
difficulties as well as his desire to develop an increasingly sophisticated 
computer and to move ahead faster than anyone else could follow. Nevertheless, 
his son, Henry Babbage, completed a simplified version of the analytical 
engine's computing unit (the mill) in 1888. He gave a successful demonstration 
of its use in computing tables in 1906.

Analog computers
Main article: Analog computer

Sir William Thomson's third tide-predicting machine design, 1879–81
During the first half of the 20th century, many scientific computing needs were 
met by increasingly sophisticated analog computers, which used a direct 
mechanical or electrical model of the problem as a basis for computation. 
However, these were not programmable and generally lacked the versatility and 
accuracy of modern digital computers.[19] The first modern analog computer was 
a tide-predicting machine, invented by Sir William Thomson in 1872. The 
differential analyser, a mechanical analog computer designed to solve 
differential equations by integration using wheel-and-disc mechanisms, was 
conceptualized in 1876 by James Thomson, the brother of the more famous Lord 
Kelvin.[15]

The art of mechanical analog computing reached its zenith with the differential 
analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This 
built on the mechanical integrators of James Thomson and the torque amplifiers 
invented by H. W. Nieman. A dozen of these devices were built before their 
obsolescence became obvious. By the 1950s, the success of digital electronic 
computers had spelled the end for most analog computing machines, but analog 
computers remained in use during the 1950s in some specialized applications 
such as education (control systems) and aircraft (slide rule).

Digital computers
Electromechanical
By 1938, the United States Navy had developed an electromechanical analog 
computer small enough to use aboard a submarine. This was the Torpedo Data 
Computer, which used trigonometry to solve the problem of firing a torpedo at a 
moving target. During World War II similar devices were developed in other 
countries as well.


Replica of Zuse's Z3, the first fully automatic, digital (electromechanical) 
computer.
Early digital computers were electromechanical; electric switches drove 
mechanical relays to perform the calculation. These devices had a low operating 
speed and were eventually superseded by much faster all-electric computers, 
originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse 
in 1939, was one of the earliest examples of an electromechanical relay 
computer.[20]

In 1941, Zuse followed his earlier machine up with the Z3, the world's first 
working electromechanical programmable, fully automatic digital 
computer.[21][22] The Z3 was built with 2000 relays, implementing a 22 bit word 
length that operated at a clock frequency of about 5–10 Hz.[23] Program code 
was supplied on punched film while data could be stored in 64 words of memory 
or supplied from the keyboard. It was quite similar to modern machines in some 
respects, pioneering numerous advances such as floating point numbers. Rather 
than the harder-to-implement decimal system (used in Charles Babbage's earlier 
design), using a binary system meant that Zuse's machines were easier to build 
and potentially more reliable, given the technologies available at that 
time.[24] The Z3 was Turing complete.[25][26]

Vacuum tubes and digital electronic circuits
Purely electronic circuit elements soon replaced their mechanical and 
electromechanical equivalents, at the same time that digital calculation 
replaced analog. The engineer Tommy Flowers, working at the Post Office 
Research Station in London in the 1930s, began to explore the possible use of 
electronics for the telephone exchange. Experimental equipment that he built in 
1934 went into operation five years later, converting a portion of the 
telephone exchange network into an electronic data processing system, using 
thousands of vacuum tubes.[19] In the US, John Vincent Atanasoff and Clifford 
E. Berry of Iowa State University developed and tested the Atanasoff–Berry 
Computer (ABC) in 1942,[27] the first "automatic electronic digital 
computer".[28] This design was also all-electronic and used about 300 vacuum 
tubes, with capacitors fixed in a mechanically rotating drum for memory.[29]

Two women are seen by the Colossus computer.
Colossus, the first electronic digital programmable computing device, was used 
to break German ciphers during World War II.
During World War II, the British at Bletchley Park achieved a number of 
successes at breaking encrypted German military communications. The German 
encryption machine, Enigma, was first attacked with the help of the 
electro-mechanical bombes which were often run by women.[30][31] To crack the 
more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army 
communications, Max Newman and his colleagues commissioned Flowers to build the 
Colossus.[29] He spent eleven months from early February 1943 designing and 
building the first Colossus.[32] After a functional test in December 1943, 
Colossus was shipped to Bletchley Park, where it was delivered on 18 January 
1944[33] and attacked its first message on 5 February.[29]

Colossus was the world's first electronic digital programmable computer.[19] It 
used a large number of valves (vacuum tubes). It had paper-tape input and was 
capable of being configured to perform a variety of boolean logical operations 
on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The 
Mk I was converted to a Mk II making ten machines in total). Colossus Mark I 
contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was 
both 5 times faster and simpler to operate than Mark I, greatly speeding the 
decoding process.[34][35]


ENIAC was the first electronic, Turing-complete device, and performed 
ballistics trajectory calculations for the United States Army.
The ENIAC[36] (Electronic Numerical Integrator and Computer) was the first 
electronic programmable computer built in the U.S. Although the ENIAC was 
similar to the Colossus, it was much faster, more flexible, and it was 
Turing-complete. Like the Colossus, a "program" on the ENIAC was defined by the 
states of its patch cables and switches, a far cry from the stored program 
electronic machines that came later. Once a program was written, it had to be 
mechanically set into the machine with manual resetting of plugs and switches. 
The programmers of the ENIAC were six women, often known collectively as the 
"ENIAC girls".[37][38]

It combined the high speed of electronics with the ability to be programmed for 
many complex problems. It could add or subtract 5000 times a second, a thousand 
times faster than any other machine. It also had modules to multiply, divide, 
and square root. High speed memory was limited to 20 words (about 80 bytes). 
Built under the direction of John Mauchly and J. Presper Eckert at the 
University of Pennsylvania, ENIAC's development and construction lasted from 
1943 to full operation at the end of 1945. The machine was huge, weighing 30 
tons, using 200 kilowatts of electric power and contained over 18,000 vacuum 
tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and 
inductors.[39]

Modern computers
Concept of modern computer
The principle of the modern computer was proposed by Alan Turing in his seminal 
1936 paper,[40] On Computable Numbers. Turing proposed a simple device that he 
called "Universal Computing machine" and that is now known as a universal 
Turing machine. He proved that such a machine is capable of computing anything 
that is computable by executing instructions (program) stored on tape, allowing 
the machine to be programmable. The fundamental concept of Turing's design is 
the stored program, where all the instructions for computing are stored in 
memory. Von Neumann acknowledged that the central concept of the modern 
computer was due to this paper.[41] Turing machines are to this day a central 
object of study in theory of computation. Except for the limitations imposed by 
their finite memory stores, modern computers are said to be Turing-complete, 
which is to say, they have algorithm execution capability equivalent to a 
universal Turing machine.

Stored programs
Main article: Stored-program computer
Three tall racks containing electronic circuit boards
A section of the Manchester Baby, the first electronic stored-program computer.
Early computing machines had fixed programs. Changing its function required the 
re-wiring and re-structuring of the machine.[29] With the proposal of the 
stored-program computer this changed. A stored-program computer includes by 
design an instruction set and can store in memory a set of instructions (a 
program) that details the computation. The theoretical basis for the 
stored-program computer was laid by Alan Turing in his 1936 paper. In 1945, 
Turing joined the National Physical Laboratory and began work on developing an 
electronic stored-program digital computer. His 1945 report "Proposed 
Electronic Calculator" was the first specification for such a device. John von 
Neumann at the University of Pennsylvania also circulated his First Draft of a 
Report on the EDVAC in 1945.[19]

The Manchester Baby was the world's first stored-program computer. It was built 
at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn 
and Geoff Tootill, and ran its first program on 21 June 1948.[42] It was 
designed as a testbed for the Williams tube, the first random-access digital 
storage device.[43] Although the computer was considered "small and primitive" 
by the standards of its time, it was the first working machine to contain all 
of the elements essential to a modern electronic computer.[44] As soon as the 
Baby had demonstrated the feasibility of its design, a project was initiated at 
the university to develop it into a more usable computer, the Manchester Mark 
1. Grace Hopper was the first person to develop a compiler for programming 
language.[2]

The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the 
world's first commercially available general-purpose computer.[45] Built by 
Ferranti, it was delivered to the University of Manchester in February 1951. At 
least seven of these later machines were delivered between 1953 and 1957, one 
of them to Shell labs in Amsterdam.[46] In October 1947, the directors of 
British catering company J. Lyons & Company decided to take an active role in 
promoting the commercial development of computers. The LEO I computer became 
operational in April 1951[47] and ran the world's first regular routine office 
computer job.

Transistors
Main articles: Transistor and History of the transistor
Further information: Transistor computer and MOSFET

Bipolar junction transistor (BJT).
The concept of a field-effect transistor was proposed by Julius Edgar 
Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under 
William Shockley at Bell Labs, built the first working transistor, the 
point-contact transistor, in 1947, which was followed by Shockley's bipolar 
junction transistor in 1948.[48][49] From 1955 onwards, transistors replaced 
vacuum tubes in computer designs, giving rise to the "second generation" of 
computers. Compared to vacuum tubes, transistors have many advantages: they are 
smaller, and require less power than vacuum tubes, so give off less heat. 
Junction transistors were much more reliable than vacuum tubes and had longer, 
indefinite, service life. Transistorized computers could contain tens of 
thousands of binary logic circuits in a relatively compact space. However, 
early junction transistors were relatively bulky devices that were difficult to 
manufacture on a mass-production basis, which limited them to a number of 
specialised applications.[50]

At the University of Manchester, a team under the leadership of Tom Kilburn 
designed and built a machine using the newly developed transistors instead of 
valves.[51] Their first transistorised computer and the first in the world, was 
operational by 1953, and a second version was completed there in April 1955. 
However, the machine did make use of valves to generate its 125 kHz clock 
waveforms and in the circuitry to read and write on its magnetic drum memory, 
so it was not the first completely transistorized computer. That distinction 
goes to the Harwell CADET of 1955,[52] built by the electronics division of the 
Atomic Energy Research Establishment at Harwell.[52][53]


MOSFET (MOS transistor), showing gate (G), body (B), source (S) and drain (D) 
terminals. The gate is separated from the body by an insulating layer (pink).
The metal–oxide–silicon field-effect transistor (MOSFET), also known as the 
MOS transistor, was invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs 
in 1959.[54] It was the first truly compact transistor that could be 
miniaturised and mass-produced for a wide range of uses.[50] With its high 
scalability,[55] and much lower power consumption and higher density than 
bipolar junction transistors,[56] the MOSFET made it possible to build 
high-density integrated circuits.[57][58] In addition to data processing, it 
also enabled the practical use of MOS transistors as memory cell storage 
elements, leading to the development of MOS semiconductor memory, which 
replaced earlier magnetic-core memory in computers.[59] The MOSFET led to the 
microcomputer revolution,[60] and became the driving force behind the computer 
revolution.[61][62] The MOSFET is the most widely used transistor in 
computers,[63][64] and is the fundamental building block of digital 
electronics.[65]

Integrated circuits
Main articles: Integrated circuit and Invention of the integrated circuit
Further information: Planar process and Microprocessor
The next great advance in computing power came with the advent of the 
integrated circuit (IC). The idea of the integrated circuit was first conceived 
by a radar scientist working for the Royal Radar Establishment of the Ministry 
of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description 
of an integrated circuit at the Symposium on Progress in Quality Electronic 
Components in Washington, D.C. on 7 May 1952.[66]

The first working ICs were invented by Jack Kilby at Texas Instruments and 
Robert Noyce at Fairchild Semiconductor.[67] Kilby recorded his initial ideas 
concerning the integrated circuit in July 1958, successfully demonstrating the 
first working integrated example on 12 September 1958.[68] In his patent 
application of 6 February 1959, Kilby described his new device as "a body of 
semiconductor material ... wherein all the components of the electronic circuit 
are completely integrated".[69][70] However, Kilby's invention was a hybrid 
integrated circuit (hybrid IC), rather than a monolithic integrated circuit 
(IC) chip.[71] Kilby's IC had external wire connections, which made it 
difficult to mass-produce.[72]

Noyce also came up with his own idea of an integrated circuit half a year later 
than Kilby.[73] Noyce's invention was the first true monolithic IC 
chip.[74][72] His chip solved many practical problems that Kilby's had not. 
Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's 
chip was made of germanium. Noyce's monolithic IC was fabricated using the 
planar process, developed by his colleague Jean Hoerni in early 1959. In turn, 
the planar process was based on the silicon surface passivation and thermal 
oxidation processes developed by Mohamed Atalla at Bell Labs in the late 
1950s.[75][76][77]

Modern monolithic ICs are predominantly MOS (metal-oxide-semiconductor) 
integrated circuits, built from MOSFETs (MOS transistors).[78] After the first 
MOSFET was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959,[79] 
Atalla first proposed the concept of the MOS integrated circuit in 1960, 
followed by Kahng in 1961, both noting that the MOS transistor's ease of 
fabrication made it useful for integrated circuits.[50][80] The earliest 
experimental MOS IC to be fabricated was a 16-transistor chip built by Fred 
Heiman and Steven Hofstein at RCA in 1962.[81] General Microelectronics later 
introduced the first commercial MOS IC in 1964,[82] developed by Robert 
Norman.[81] Following the development of the self-aligned gate (silicon-gate) 
MOS transistor by Robert Kerwin, Donald Klein and John Sarace at Bell Labs in 
1967, the first silicon-gate MOS IC with self-aligned gates was developed by 
Federico Faggin at Fairchild Semiconductor in 1968.[83] The MOSFET has since 
become the most critical device component in modern ICs.[84]

The development of the MOS integrated circuit led to the invention of the 
microprocessor,[85][86] and heralded an explosion in the commercial and 
personal use of computers. While the subject of exactly which device was the 
first microprocessor is contentious, partly due to lack of agreement on the 
exact definition of the term "microprocessor", it is largely undisputed that 
the first single-chip microprocessor was the Intel 4004,[87] designed and 
realized by Federico Faggin with his silicon-gate MOS IC technology,[85] along 
with Ted Hoff, Masatoshi Shima and Stanley Mazor at Intel.[88][89] In the early 
1970s, MOS IC technology enabled the integration of more than 10,000 
transistors on a single chip.[58]

System on a Chip (SoCs) are complete computers on a microchip (or chip) the 
size of a coin.[90] They may or may not have integrated RAM and flash memory. 
If not integrated, The RAM is usually placed directly above (known as Package 
on package) or below (on the opposite side of the circuit board) the SoC, and 
the flash memory is usually placed right next to the SoC, this all done to 
improve data transfer speeds, as the data signals don't have to travel long 
distances. Since ENIAC in 1945, computers have advanced enormously, with modern 
SoCs being the size of a coin while also being hundreds of thousands of times 
more powerful than ENIAC, integrating billions of transistors, and consuming 
only a few watts of power.

Mobile computers
The first mobile computers were heavy and ran from mains power. The 50lb IBM 
5100 was an early example. Later portables such as the Osborne 1 and Compaq 
Portable were considerably lighter but still needed to be plugged in. The first 
laptops, such as the Grid Compass, removed this requirement by incorporating 
batteries – and with the continued miniaturization of computing resources and 
advancements in portable battery life, portable computers grew in popularity in 
the 2000s.[91] The same developments allowed manufacturers to integrate 
computing resources into cellular mobile phones by the early 2000s.

These smartphones and tablets run on a variety of operating systems and 
recently became the dominant computing device on the market.[92] These are 
powered by System on a Chip (SoCs), which are complete computers on a microchip 
the size of a coin.[90]

Types
Computers can be classified in a number of different ways, including:

By architecture
Analog computer
Digital computer
Hybrid computer
Harvard architecture
Von Neumann architecture
Reduced instruction set computer
By size and form-factor
Mainframe computer
Supercomputer
Minicomputer
Microcomputer
Workstation
Personal computer
Laptop
Tablet computer
Smartphone
Single-board computer
Hardware
Main articles: Computer hardware, Personal computer hardware, Central 
processing unit, and Microprocessor
File:Computer Components.webm
Video demonstrating the standard components of a "slimline" computer
The term hardware covers all of those parts of a computer that are tangible 
physical objects. Circuits, computer chips, graphic cards, sound cards, memory 
(RAM), motherboard, displays, power supplies, cables, keyboards, printers and 
"mice" input devices are all hardware.

History of computing hardware
Main article: History of computing hardware
First generation (mechanical/electromechanical)	Calculators	Pascal's 
calculator, Arithmometer, Difference engine, Quevedo's analytical machines
Programmable devices	Jacquard loom, Analytical engine, IBM ASCC/Harvard Mark I, 
Harvard Mark II, IBM SSEC, Z1, Z2, Z3
Second generation (vacuum tubes)	Calculators	Atanasoff–Berry Computer, IBM 
604, UNIVAC 60, UNIVAC 120
Programmable devices	Colossus, ENIAC, Manchester Baby, EDSAC, Manchester Mark 
1, Ferranti Pegasus, Ferranti Mercury, CSIRAC, EDVAC, UNIVAC I, IBM 701, IBM 
702, IBM 650, Z22
Third generation (discrete transistors and SSI, MSI, LSI integrated circuits)	
Mainframes	IBM 7090, IBM 7080, IBM System/360, BUNCH
Minicomputer	HP 2116A, IBM System/32, IBM System/36, LINC, PDP-8, PDP-11
Desktop Computer	HP 9100
Fourth generation (VLSI integrated circuits)	Minicomputer	VAX, IBM System i
4-bit microcomputer	Intel 4004, Intel 4040
8-bit microcomputer	Intel 8008, Intel 8080, Motorola 6800, Motorola 6809, MOS 
Technology 6502, Zilog Z80
16-bit microcomputer	Intel 8088, Zilog Z8000, WDC 65816/65802
32-bit microcomputer	Intel 80386, Pentium, Motorola 68000, ARM
64-bit microcomputer[93]	Alpha, MIPS, PA-RISC, PowerPC, SPARC, x86-64, ARMv8-A
Embedded computer	Intel 8048, Intel 8051
Personal computer	Desktop computer, Home computer, Laptop computer, Personal 
digital assistant (PDA), Portable computer, Tablet PC, Wearable computer
Theoretical/experimental	Quantum computer, Chemical computer, DNA computing, 
Optical computer, Spintronics-based computer, Wetware/Organic computer	
Other hardware topics
Peripheral device (input/output)	Input	Mouse, keyboard, joystick, image 
scanner, webcam, graphics tablet, microphone
Output	Monitor, printer, loudspeaker
Both	Floppy disk drive, hard disk drive, optical disc drive, teleprinter
Computer buses	Short range	RS-232, SCSI, PCI, USB
Long range (computer networking)	Ethernet, ATM, FDDI
A general purpose computer has four main components: the arithmetic logic unit 
(ALU), the control unit, the memory, and the input and output devices 
(collectively termed I/O). These parts are interconnected by buses, often made 
of groups of wires. Inside each of these parts are thousands to trillions of 
small electrical circuits which can be turned off or on by means of an 
electronic switch. Each circuit represents a bit (binary digit) of information 
so that when the circuit is on it represents a "1", and when off it represents 
a "0" (in positive logic representation). The circuits are arranged in logic 
gates so that one or more of the circuits may control the state of one or more 
of the other circuits.

Input devices
When unprocessed data is sent to the computer with the help of input devices, 
the data is processed and sent to output devices. The input devices may be 
hand-operated or automated. The act of processing is mainly regulated by the 
CPU. Some examples of input devices are:

Computer keyboard
Digital camera
Digital video
Graphics tablet
Image scanner
Joystick
Microphone
Mouse
Overlay keyboard
Real-time clock
Trackball
Touchscreen
Output devices
The means through which computer gives output are known as output devices. Some 
examples of output devices are:

Computer monitor
Printer
PC speaker
Projector
Sound card
Video card
Control unit
Main articles: CPU design and Control unit

Diagram showing how a particular MIPS architecture instruction would be decoded 
by the control system
The control unit (often called a control system or central controller) manages 
the computer's various components; it reads and interprets (decodes) the 
program instructions, transforming them into control signals that activate 
other parts of the computer.[94] Control systems in advanced computers may 
change the order of execution of some instructions to improve performance.

A key component common to all CPUs is the program counter, a special memory 
cell (a register) that keeps track of which location in memory the next 
instruction is to be read from.[95]

The control system's function is as follows—note that this is a simplified 
description, and some of these steps may be performed concurrently or in a 
different order depending on the type of CPU:

Read the code for the next instruction from the cell indicated by the program 
counter.
Decode the numerical code for the instruction into a set of commands or signals 
for each of the other systems.
Increment the program counter so it points to the next instruction.
Read whatever data the instruction requires from cells in memory (or perhaps 
from an input device). The location of this required data is typically stored 
within the instruction code.
Provide the necessary data to an ALU or register.
If the instruction requires an ALU or specialized hardware to complete, 
instruct the hardware to perform the requested operation.
Write the result from the ALU back to a memory location or to a register or 
perhaps an output device.
Jump back to step (1).
Since the program counter is (conceptually) just another set of memory cells, 
it can be changed by calculations done in the ALU. Adding 100 to the program 
counter would cause the next instruction to be read from a place 100 locations 
further down the program. Instructions that modify the program counter are 
often known as "jumps" and allow for loops (instructions that are repeated by 
the computer) and often conditional instruction execution (both examples of 
control flow).

The sequence of operations that the control unit goes through to process an 
instruction is in itself like a short computer program, and indeed, in some 
more complex CPU designs, there is another yet smaller computer called a 
microsequencer, which runs a microcode program that causes all of these events 
to happen.

Central processing unit (CPU)
Main articles: Central processing unit and Microprocessor
The control unit, ALU, and registers are collectively known as a central 
processing unit (CPU). Early CPUs were composed of many separate components. 
Since the 1970s, CPUs have typically been constructed on a single MOS 
integrated circuit chip called a microprocessor.

Arithmetic logic unit (ALU)
Main article: Arithmetic logic unit
The ALU is capable of performing two classes of operations: arithmetic and 
logic.[96] The set of arithmetic operations that a particular ALU supports may 
be limited to addition and subtraction, or might include multiplication, 
division, trigonometry functions such as sine, cosine, etc., and square roots. 
Some can only operate on whole numbers (integers) while others use floating 
point to represent real numbers, albeit with limited precision. However, any 
computer that is capable of performing just the simplest operations can be 
programmed to break down the more complex operations into simple steps that it 
can perform. Therefore, any computer can be programmed to perform any 
arithmetic operation—although it will take more time to do so if its ALU does 
not directly support the operation. An ALU may also compare numbers and return 
boolean truth values (true or false) depending on whether one is equal to, 
greater than or less than the other ("is 64 greater than 65?"). Logic 
operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful 
for creating complicated conditional statements and processing boolean logic.

Superscalar computers may contain multiple ALUs, allowing them to process 
several instructions simultaneously.[97] Graphics processors and computers with 
SIMD and MIMD features often contain ALUs that can perform arithmetic on 
vectors and matrices.

Memory
Main articles: Computer memory and Computer data storage

Magnetic-core memory (using magnetic cores) was the computer memory of choice 
in the 1960s, until it was replaced by semiconductor memory (using MOS memory 
cells).
A computer's memory can be viewed as a list of cells into which numbers can be 
placed or read. Each cell has a numbered "address" and can store a single 
number. The computer can be instructed to "put the number 123 into the cell 
numbered 1357" or to "add the number that is in cell 1357 to the number that is 
in cell 2468 and put the answer into cell 1595." The information stored in 
memory may represent practically anything. Letters, numbers, even computer 
instructions can be placed into memory with equal ease. Since the CPU does not 
differentiate between different types of information, it is the software's 
responsibility to give significance to what the memory sees as nothing but a 
series of numbers.

In almost all modern computers, each memory cell is set up to store binary 
numbers in groups of eight bits (called a byte). Each byte is able to represent 
256 different numbers (28 = 256); either from 0 to 255 or −128 to +127. To 
store larger numbers, several consecutive bytes may be used (typically, two, 
four or eight). When negative numbers are required, they are usually stored in 
two's complement notation. Other arrangements are possible, but are usually not 
seen outside of specialized applications or historical contexts. A computer can 
store any kind of information in memory if it can be represented numerically. 
Modern computers have billions or even trillions of bytes of memory.

The CPU contains a special set of memory cells called registers that can be 
read and written to much more rapidly than the main memory area. There are 
typically between two and one hundred registers depending on the type of CPU. 
Registers are used for the most frequently needed data items to avoid having to 
access main memory every time data is needed. As data is constantly being 
worked on, reducing the need to access main memory (which is often slow 
compared to the ALU and control units) greatly increases the computer's speed.

Computer main memory comes in two principal varieties:

random-access memory or RAM
read-only memory or ROM
RAM can be read and written to anytime the CPU commands it, but ROM is 
preloaded with data and software that never changes, therefore the CPU can only 
read from it. ROM is typically used to store the computer's initial start-up 
instructions. In general, the contents of RAM are erased when the power to the 
computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM 
contains a specialized program called the BIOS that orchestrates loading the 
computer's operating system from the hard disk drive into RAM whenever the 
computer is turned on or reset. In embedded computers, which frequently do not 
have disk drives, all of the required software may be stored in ROM. Software 
stored in ROM is often called firmware, because it is notionally more like 
hardware than software. Flash memory blurs the distinction between ROM and RAM, 
as it retains its data when turned off but is also rewritable. It is typically 
much slower than conventional ROM and RAM however, so its use is restricted to 
applications where high speed is unnecessary.[98]

In more sophisticated computers there may be one or more RAM cache memories, 
which are slower than registers but faster than main memory. Generally 
computers with this sort of cache are designed to move frequently needed data 
into the cache automatically, often without the need for any intervention on 
the programmer's part.

Input/output (I/O)
Main article: Input/output

Hard disk drives are common storage devices used with computers.
I/O is the means by which a computer exchanges information with the outside 
world.[99] Devices that provide input or output to the computer are called 
peripherals.[100] On a typical personal computer, peripherals include input 
devices like the keyboard and mouse, and output devices such as the display and 
printer. Hard disk drives, floppy disk drives and optical disc drives serve as 
both input and output devices. Computer networking is another form of I/O. I/O 
devices are often complex computers in their own right, with their own CPU and 
memory. A graphics processing unit might contain fifty or more tiny computers 
that perform the calculations necessary to display 3D graphics.[citation 
needed] Modern desktop computers contain many smaller computers that assist the 
main CPU in performing I/O. A 2016-era flat screen display contains its own 
computer circuitry.

Multitasking
Main article: Computer multitasking
While a computer may be viewed as running one gigantic program stored in its 
main memory, in some systems it is necessary to give the appearance of running 
several programs simultaneously. This is achieved by multitasking i.e. having 
the computer switch rapidly between running each program in turn.[101] One 
means by which this is done is with a special signal called an interrupt, which 
can periodically cause the computer to stop executing instructions where it was 
and do something else instead. By remembering where it was executing prior to 
the interrupt, the computer can return to that task later. If several programs 
are running "at the same time". then the interrupt generator might be causing 
several hundred interrupts per second, causing a program switch each time. 
Since modern computers typically execute instructions several orders of 
magnitude faster than human perception, it may appear that many programs are 
running at the same time even though only one is ever executing in any given 
instant. This method of multitasking is sometimes termed "time-sharing" since 
each program is allocated a "slice" of time in turn.[102]

Before the era of inexpensive computers, the principal use for multitasking was 
to allow many people to share the same computer. Seemingly, multitasking would 
cause a computer that is switching between several programs to run more slowly, 
in direct proportion to the number of programs it is running, but most programs 
spend much of their time waiting for slow input/output devices to complete 
their tasks. If a program is waiting for the user to click on the mouse or 
press a key on the keyboard, then it will not take a "time slice" until the 
event it is waiting for has occurred. This frees up time for other programs to 
execute so that many programs may be run simultaneously without unacceptable 
speed loss.

Multiprocessing
Main article: Multiprocessing

Cray designed many supercomputers that used multiprocessing heavily.
Some computers are designed to distribute their work across several CPUs in a 
multiprocessing configuration, a technique once employed only in large and 
powerful machines such as supercomputers, mainframe computers and servers. 
Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) 
personal and laptop computers are now widely available, and are being 
increasingly used in lower-end markets as a result.

Supercomputers in particular often have highly unique architectures that differ 
significantly from the basic stored-program architecture and from general 
purpose computers.[103] They often feature thousands of CPUs, customized 
high-speed interconnects, and specialized computing hardware. Such designs tend 
to be useful only for specialized tasks due to the large scale of program 
organization required to successfully utilize most of the available resources 
at once. Supercomputers usually see usage in large-scale simulation, graphics 
rendering, and cryptography applications, as well as with other so-called 
"embarrassingly parallel" tasks.

Software
Main article: Computer software
Software refers to parts of the computer which do not have a material form, 
such as programs, data, protocols, etc. Software is that part of a computer 
system that consists of encoded information or computer instructions, in 
contrast to the physical hardware from which the system is built. Computer 
software includes computer programs, libraries and related non-executable data, 
such as online documentation or digital media. It is often divided into system 
software and application software Computer hardware and software require each 
other and neither can be realistically used on its own. When software is stored 
in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC 
compatible computer, it is sometimes called "firmware".

Operating system /System Software	Unix and BSD	UNIX System V, IBM AIX, HP-UX, 
Solaris (SunOS), IRIX, List of BSD operating systems
GNU/Linux	List of Linux distributions, Comparison of Linux distributions
Microsoft Windows	Windows 95, Windows 98, Windows NT, Windows 2000, Windows ME, 
Windows XP, Windows Vista, Windows 7, Windows 8, Windows 8.1, Windows 10
DOS	86-DOS (QDOS), IBM PC DOS, MS-DOS, DR-DOS, FreeDOS
Macintosh operating systems	Classic Mac OS, macOS (previously OS X and Mac OS 
X)
Embedded and real-time	List of embedded operating systems
Experimental	Amoeba, Oberon/Bluebottle, Plan 9 from Bell Labs
Library	Multimedia	DirectX, OpenGL, OpenAL, Vulkan (API)
Programming library	C standard library, Standard Template Library
Data	Protocol	TCP/IP, Kermit, FTP, HTTP, SMTP
File format	HTML, XML, JPEG, MPEG, PNG
User interface	Graphical user interface (WIMP)	Microsoft Windows, GNOME, KDE, 
QNX Photon, CDE, GEM, Aqua
Text-based user interface	Command-line interface, Text user interface
Application Software	Office suite	Word processing, Desktop publishing, 
Presentation program, Database management system, Scheduling & Time management, 
Spreadsheet, Accounting software
Internet Access	Browser, Email client, Web server, Mail transfer agent, Instant 
messaging
Design and manufacturing	Computer-aided design, Computer-aided manufacturing, 
Plant management, Robotic manufacturing, Supply chain management
Graphics	Raster graphics editor, Vector graphics editor, 3D modeler, Animation 
editor, 3D computer graphics, Video editing, Image processing
Audio	Digital audio editor, Audio playback, Mixing, Audio synthesis, Computer 
music
Software engineering	Compiler, Assembler, Interpreter, Debugger, Text editor, 
Integrated development environment, Software performance analysis, Revision 
control, Software configuration management
Educational	Edutainment, Educational game, Serious game, Flight simulator
Games	Strategy, Arcade, Puzzle, Simulation, First-person shooter, Platform, 
Massively multiplayer, Interactive fiction
Misc	Artificial intelligence, Antivirus software, Malware scanner, 
Installer/Package management systems, File manager
Languages
There are thousands of different programming languages—some intended to be 
general purpose, others useful only for highly specialized applications.

Programming languages
Lists of programming languages	Timeline of programming languages, List of 
programming languages by category, Generational list of programming languages, 
List of programming languages, Non-English-based programming languages
Commonly used assembly languages	ARM, MIPS, x86
Commonly used high-level programming languages	Ada, BASIC, C, C++, C#, COBOL, 
Fortran, PL/I, REXX, Java, Lisp, Pascal, Object Pascal
Commonly used scripting languages	Bourne script, JavaScript, Python, Ruby, PHP, 
Perl
Programs
The defining feature of modern computers which distinguishes them from all 
other machines is that they can be programmed. That is to say that some type of 
instructions (the program) can be given to the computer, and it will process 
them. Modern computers based on the von Neumann architecture often have machine 
code in the form of an imperative programming language. In practical terms, a 
computer program may be just a few instructions or extend to many millions of 
instructions, as do the programs for word processors and web browsers for 
example. A typical modern computer can execute billions of instructions per 
second (gigaflops) and rarely makes a mistake over many years of operation. 
Large computer programs consisting of several million instructions may take 
teams of programmers years to write, and due to the complexity of the task 
almost certainly contain errors.

Stored program architecture
Main articles: Computer program and Computer programming

Replica of the Manchester Baby, the world's first electronic stored-program 
computer, at the Museum of Science and Industry in Manchester, England
This section applies to most common RAM machine–based computers.

In most cases, computer instructions are simple: add one number to another, 
move some data from one location to another, send a message to some external 
device, etc. These instructions are read from the computer's memory and are 
generally carried out (executed) in the order they were given. However, there 
are usually specialized instructions to tell the computer to jump ahead or 
backwards to some other place in the program and to carry on executing from 
there. These are called "jump" instructions (or branches). Furthermore, jump 
instructions may be made to happen conditionally so that different sequences of 
instructions may be used depending on the result of some previous calculation 
or some external event. Many computers directly support subroutines by 
providing a type of jump that "remembers" the location it jumped from and 
another instruction to return to the instruction following that jump 
instruction.

Program execution might be likened to reading a book. While a person will 
normally read each word and line in sequence, they may at times jump back to an 
earlier place in the text or skip sections that are not of interest. Similarly, 
a computer may sometimes go back and repeat the instructions in some section of 
the program over and over again until some internal condition is met. This is 
called the flow of control within the program and it is what allows the 
computer to perform tasks repeatedly without human intervention.

Comparatively, a person using a pocket calculator can perform a basic 
arithmetic operation such as adding two numbers with just a few button presses. 
But to add together all of the numbers from 1 to 1,000 would take thousands of 
button presses and a lot of time, with a near certainty of making a mistake. On 
the other hand, a computer may be programmed to do this with just a few simple 
instructions. The following example is written in the MIPS assembly language:

  begin:
  addi $8, $0, 0           # initialize sum to 0
  addi $9, $0, 1           # set first number to add = 1
  loop:
  slti $10, $9, 1000       # check if the number is less than 1000
  beq $10, $0, finish      # if odd number is greater than n then exit
  add $8, $8, $9           # update sum
  addi $9, $9, 1           # get next number
  j loop                   # repeat the summing process
  finish:
  add $2, $8, $0           # put sum in output register
Once told to run this program, the computer will perform the repetitive 
addition task without further human intervention. It will almost never make a 
mistake and a modern PC can complete the task in a fraction of a second.

Machine code
In most computers, individual instructions are stored as machine code with each 
instruction being given a unique number (its operation code or opcode for 
short). The command to add two numbers together would have one opcode; the 
command to multiply them would have a different opcode, and so on. The simplest 
computers are able to perform any of a handful of different instructions; the 
more complex computers have several hundred to choose from, each with a unique 
numerical code. Since the computer's memory is able to store numbers, it can 
also store the instruction codes. This leads to the important fact that entire 
programs (which are just lists of these instructions) can be represented as 
lists of numbers and can themselves be manipulated inside the computer in the 
same way as numeric data. The fundamental concept of storing programs in the 
computer's memory alongside the data they operate on is the crux of the von 
Neumann, or stored program[citation needed], architecture. In some cases, a 
computer might store some or all of its program in memory that is kept separate 
from the data it operates on. This is called the Harvard architecture after the 
Harvard Mark I computer. Modern von Neumann computers display some traits of 
the Harvard architecture in their designs, such as in CPU caches.

While it is possible to write computer programs as long lists of numbers 
(machine language) and while this technique was used with many early 
computers,[104] it is extremely tedious and potentially error-prone to do so in 
practice, especially for complicated programs. Instead, each basic instruction 
can be given a short name that is indicative of its function and easy to 
remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are 
collectively known as a computer's assembly language. Converting programs 
written in assembly language into something the computer can actually 
understand (machine language) is usually done by a computer program called an 
assembler.


A 1970s punched card containing one line from a Fortran program. The card 
reads: "Z(1) = Y + W(1)" and is labeled "PROJ039" for identification purposes.
Programming language
Main article: Programming language
Programming languages provide various ways of specifying programs for computers 
to run. Unlike natural languages, programming languages are designed to permit 
no ambiguity and to be concise. They are purely written languages and are often 
difficult to read aloud. They are generally either translated into machine code 
by a compiler or an assembler before being run, or translated directly at run 
time by an interpreter. Sometimes programs are executed by a hybrid method of 
the two techniques.

Low-level languages
Main article: Low-level programming language
Machine languages and the assembly languages that represent them (collectively 
termed low-level programming languages) tend to be unique to a particular type 
of computer. For instance, an ARM architecture computer (such as may be found 
in a smartphone or a hand-held videogame) cannot understand the machine 
language of an x86 CPU that might be in a PC.[105]

High-level languages
Main article: High-level programming language
Although considerably easier than in machine language, writing long programs in 
assembly language is often difficult and is also error prone. Therefore, most 
practical programs are written in more abstract high-level programming 
languages that are able to express the needs of the programmer more 
conveniently (and thereby help reduce programmer error). High level languages 
are usually "compiled" into machine language (or sometimes into assembly 
language and then into machine language) using another computer program called 
a compiler.[106] High level languages are less related to the workings of the 
target computer than assembly language, and more related to the language and 
structure of the problem(s) to be solved by the final program. It is therefore 
often possible to use different compilers to translate the same high level 
language program into the machine language of many different types of computer. 
This is part of the means by which software like video games may be made 
available for different computer architectures such as personal computers and 
various video game consoles.

Program design

This section does not cite any sources. Please help improve this section by 
adding citations to reliable sources. Unsourced material may be challenged and 
removed.
Find sources: "Computer" – news · newspapers · books · scholar · JSTOR 
(July 2012) (Learn how and when to remove this template message)
Program design of small programs is relatively simple and involves the analysis 
of the problem, collection of inputs, using the programming constructs within 
languages, devising or using established procedures and algorithms, providing 
data for output devices and solutions to the problem as applicable. As problems 
become larger and more complex, features such as subprograms, modules, formal 
documentation, and new paradigms such as object-oriented programming are 
encountered. Large programs involving thousands of line of code and more 
require formal software methodologies. The task of developing large software 
systems presents a significant intellectual challenge. Producing software with 
an acceptably high reliability within a predictable schedule and budget has 
historically been difficult; the academic and professional discipline of 
software engineering concentrates specifically on this challenge.

Bugs
Main article: Software bug

The actual first computer bug, a moth found trapped on a relay of the Harvard 
Mark II computer
Errors in computer programs are called "bugs". They may be benign and not 
affect the usefulness of the program, or have only subtle effects. But in some 
cases, they may cause the program or the entire system to "hang", becoming 
unresponsive to input such as mouse clicks or keystrokes, to completely fail, 
or to crash. Otherwise benign bugs may sometimes be harnessed for malicious 
intent by an unscrupulous user writing an exploit, code designed to take 
advantage of a bug and disrupt a computer's proper execution. Bugs are usually 
not the fault of the computer. Since computers merely execute the instructions 
they are given, bugs are nearly always the result of programmer error or an 
oversight made in the program's design.[107] Admiral Grace Hopper, an American 
computer scientist and developer of the first compiler, is credited for having 
first used the term "bugs" in computing after a dead moth was found shorting a 
relay in the Harvard Mark II computer in September 1947.[108]

Networking and the Internet
Main articles: Computer networking and Internet

Visualization of a portion of the routes on the Internet
Computers have been used to coordinate information between multiple locations 
since the 1950s. The U.S. military's SAGE system was the first large-scale 
example of such a system, which led to a number of special-purpose commercial 
systems such as Sabre.[109] In the 1970s, computer engineers at research 
institutions throughout the United States began to link their computers 
together using telecommunications technology. The effort was funded by ARPA 
(now DARPA), and the computer network that resulted was called the 
ARPANET.[110] The technologies that made the Arpanet possible spread and 
evolved.

In time, the network spread beyond academic and military institutions and 
became known as the Internet. The emergence of networking involved a 
redefinition of the nature and boundaries of the computer. Computer operating 
systems and applications were modified to include the ability to define and 
access the resources of other computers on the network, such as peripheral 
devices, stored information, and the like, as extensions of the resources of an 
individual computer. Initially these facilities were available primarily to 
people working in high-tech environments, but in the 1990s the spread of 
applications like e-mail and the World Wide Web, combined with the development 
of cheap, fast networking technologies like Ethernet and ADSL saw computer 
networking become almost ubiquitous. In fact, the number of computers that are 
networked is growing phenomenally. A very large proportion of personal 
computers regularly connect to the Internet to communicate and receive 
information. "Wireless" networking, often utilizing mobile phone networks, has 
meant networking is becoming increasingly ubiquitous even in mobile computing 
environments.

Unconventional computers
Main article: Human computer
See also: Harvard Computers
A computer does not need to be electronic, nor even have a processor, nor RAM, 
nor even a hard disk. While popular usage of the word "computer" is synonymous 
with a personal electronic computer, the modern[111] definition of a computer 
is literally: "A device that computes, especially a programmable [usually] 
electronic machine that performs high-speed mathematical or logical operations 
or that assembles, stores, correlates, or otherwise processes 
information."[112] Any device which processes information qualifies as a 
computer, especially if the processing is purposeful.[citation needed]

Future
There is active research to make computers out of many promising new types of 
technology, such as optical computers, DNA computers, neural computers, and 
quantum computers. Most computers are universal, and are able to calculate any 
computable function, and are limited only by their memory capacity and 
operating speed. However different designs of computers can give very different 
performance for particular problems; for example quantum computers can 
potentially break some modern encryption algorithms (by quantum factoring) very 
quickly.

Computer architecture paradigms
There are many types of computer architectures:

Quantum computer vs. Chemical computer
Scalar processor vs. Vector processor
Non-Uniform Memory Access (NUMA) computers
Register machine vs. Stack machine
Harvard architecture vs. von Neumann architecture
Cellular architecture
Of all these abstract machines, a quantum computer holds the most promise for 
revolutionizing computing.[113] Logic gates are a common abstraction which can 
apply to most of the above digital or analog paradigms. The ability to store 
and execute lists of instructions called programs makes computers extremely 
versatile, distinguishing them from calculators. The Church–Turing thesis is 
a mathematical statement of this versatility: any computer with a minimum 
capability (being Turing-complete) is, in principle, capable of performing the 
same tasks that any other computer can perform. Therefore, any type of computer 
(netbook, supercomputer, cellular automaton, etc.) is able to perform the same 
computational tasks, given enough time and storage capacity.

Artificial intelligence
A computer will solve problems in exactly the way it is programmed to, without 
regard to efficiency, alternative solutions, possible shortcuts, or possible 
errors in the code. Computer programs that learn and adapt are part of the 
emerging field of artificial intelligence and machine learning. Artificial 
intelligence based products generally fall into two major categories: rule 
based systems and pattern recognition systems. Rule based systems attempt to 
represent the rules used by human experts and tend to be expensive to develop. 
Pattern based systems use data about a problem to generate conclusions. 
Examples of pattern based systems include voice recognition, font recognition, 
translation and the emerging field of on-line marketing.

Professions and organizations
As the use of computers has spread throughout society, there are an increasing 
number of careers involving computers.

Computer-related professions
Hardware-related	Electrical engineering, Electronic engineering, Computer 
engineering, Telecommunications engineering, Optical engineering, 
Nanoengineering
Software-related	Computer science, Computer engineering, Desktop publishing, 
Human–computer interaction, Information technology, Information systems, 
Computational science, Software engineering, Video game industry, Web design
The need for computers to work well together and to be able to exchange 
information has spawned the need for many standards organizations, clubs and 
societies of both a formal and informal nature.

Organizations
Standards groups	ANSI, IEC, IEEE, IETF, ISO, W3C
Professional societies	ACM, AIS, IET, IFIP, BCS
Free/open source software groups	Free Software Foundation, Mozilla Foundation, 
Apache Software Foundation
See also
Glossary of computers
Computability theory
Computer insecurity
Computer security
Glossary of computer hardware terms
History of computer science
List of computer term etymologies
List of fictional computers
List of pioneers in computer science
Pulse computation
TOP500 (list of most powerful computers)
Unconventional computing
References
 Evans 2018, p. 23.
 Smith 2013, p. 6.
 "computer (n.)". Online Etymology Dictionary.
 According to Schmandt-Besserat 1981 , these clay containers contained tokens, 
the total of which were the count of objects being transferred. The containers 
thus served as something of a bill of lading or an accounts book. In order to 
avoid breaking open the containers, first, clay impressions of the tokens were 
placed on the outside of the containers, for the count; the shapes of the 
impressions were abstracted into stylized marks; finally, the abstract marks 
were systematically used as numerals; these numerals were finally formalized as 
numbers. Eventually (Schmandt-Besserat estimates it took 4000 years Archived 30 
January 2012 at the Wayback Machine ) the marks on the outside of the 
containers were all that were needed to convey the count, and the clay 
containers evolved into clay tablets with marks for the count.
 Robson, Eleanor (2008), Mathematics in Ancient Iraq, ISBN 978-0-691-09182-2. 
p. 5: calculi were in use in Iraq for primitive accounting systems as early as 
3200–3000 BCE, with commodity-specific counting representation systems. 
Balanced accounting was in use by 3000–2350 BCE, and a sexagesimal number 
system was in use 2350–2000 BCE.
 The Antikythera Mechanism Research Project Archived 28 April 2008 at the 
Wayback Machine, The Antikythera Mechanism Research Project. Retrieved 1 July 
2007.
 G. Wiet, V. Elisseeff, P. Wolff, J. Naudu (1975). History of Mankind, Vol 3: 
The Great medieval Civilisations, p. 649. George Allen & Unwin Ltd, UNESCO.
 Fuat Sezgin "Catalogue of the Exhibition of the Institute for the History of 
Arabic-Islamic Science (at the Johann Wolfgang Goethe University", Frankfurt, 
Germany) Frankfurt Book Fair 2004, pp. 35 & 38.
 Charette, François (2006). "Archaeology: High tech from Ancient Greece". 
Nature. 444 (7119): 551–552. Bibcode:2006Natur.444..551C. 
doi:10.1038/444551a. PMID 17136077.
 Bedini, Silvio A.; Maddison, Francis R. (1966). "Mechanical Universe: The 
Astrarium of Giovanni de' Dondi". Transactions of the American Philosophical 
Society. 56 (5): 1–69. doi:10.2307/1006002. JSTOR 1006002.
 Price, Derek de S. (1984). "A History of Calculating Machines". IEEE Micro. 4 
(1): 22–52. doi:10.1109/MM.1984.291305.
 Őren, Tuncer (2001). "Advances in Computer and Information Sciences: From 
Abacus to Holonic Agents" (PDF). Turk J Elec Engin. 9 (1): 63–70.
 Donald Routledge Hill (1985). "Al-Biruni's mechanical calendar", Annals of 
Science 42, pp. 139–163.
 "The Writer Automaton, Switzerland". chonday.com. 11 July 2013.
 Ray Girvan, "The revealed grace of the mechanism: computing after Babbage" 
Archived 3 November 2012 at the Wayback Machine, Scientific Computing World, 
May/June 2003
 Halacy, Daniel Stephen (1970). Charles Babbage, Father of the Computer. 
Crowell-Collier Press. ISBN 978-0-02-741370-0.
 "Babbage". Online stuff. Science Museum. 19 January 2007. Retrieved 1 August 
2012.
 "Let's build Babbage's ultimate mechanical computer". opinion. New Scientist. 
23 December 2010. Retrieved 1 August 2012.
 The Modern History of Computing. Stanford Encyclopedia of Philosophy. 2017.
 Zuse, Horst. "Part 4: Konrad Zuse's Z1 and Z3 Computers". The Life and Work of 
Konrad Zuse. EPE Online. Archived from the original on 1 June 2008. Retrieved 
17 June 2008.
 Zuse, Konrad (2010) [1984], The Computer – My Life Translated by McKenna, 
Patricia and Ross, J. Andrew from: Der Computer, mein Lebenswerk (1984), 
Berlin/Heidelberg: Springer-Verlag, ISBN 978-3-642-08151-4
 Salz Trautman, Peggy (20 April 1994). "A Computer Pioneer Rediscovered, 50 
Years On". The New York Times.
 Zuse, Konrad (1993). Der Computer. Mein Lebenswerk (in German) (3rd ed.). 
Berlin: Springer-Verlag. p. 55. ISBN 978-3-540-56292-4.
 "Crash! The Story of IT: Zuse". Archived from the original on 18 September 
2016. Retrieved 1 June 2016.
 Rojas, R. (1998). "How to make Zuse's Z3 a universal computer". IEEE Annals of 
the History of Computing. 20 (3): 51–54. doi:10.1109/85.707574.
 Rojas, Raúl. "How to Make Zuse's Z3 a Universal Computer" (PDF).
 15 January 1941 notice in the Des Moines Register,
 Arthur W. Burks (1989). The First Electronic Computer. ISBN 0472081047.
 Copeland, Jack (2006), Colossus: The Secrets of Bletchley Park's Codebreaking 
Computers, Oxford: Oxford University Press, pp. 101–115, ISBN 
978-0-19-284055-4
 Miller, Joe (10 November 2014). "The woman who cracked Enigma cyphers". BBC 
News. Retrieved 14 October 2018.
 Bearne, Suzanne (24 July 2018). "Meet the female codebreakers of Bletchley 
Park". the Guardian. Retrieved 14 October 2018.
 Bletchley's code-cracking Colossus, BBC News, 2 February 2010, retrieved 19 
October 2012
 "Colossus – The Rebuild Story". The National Museum of Computing. Archived 
from the original on 18 April 2015. Retrieved 7 January 2014.
 Randell, Brian; Fensom, Harry; Milne, Frank A. (15 March 1995), "Obituary: 
Allen Coombs", The Independent, retrieved 18 October 2012
 Fensom, Jim (8 November 2010), "Harry Fensom obituary", The Guardian, 
retrieved 17 October 2012
 John Presper Eckert Jr. and John W. Mauchly, Electronic Numerical Integrator 
and Computer, United States Patent Office, US Patent 3,120,606, filed 26 June 
1947, issued 4 February 1964, and invalidated 19 October 1973 after court 
ruling on Honeywell v. Sperry Rand.
 Evans 2018, p. 39.
 Light 1999, p. 459.
 "Generations of Computer". techiwarehouse.com. Archived from the original on 2 
July 2015. Retrieved 7 January 2014.
 Turing, A. M. (1937). "On Computable Numbers, with an Application to the 
Entscheidungsproblem". Proceedings of the London Mathematical Society. 2. 42 
(1): 230–265. doi:10.1112/plms/s2-42.1.230.
 "von Neumann ... firmly emphasized to me, and to others I am sure, that the 
fundamental conception is owing to Turing—insofar as not anticipated by 
Babbage, Lovelace and others." Letter by Stanley Frankel to Brian Randell, 
1972, quoted in Jack Copeland (2004) The Essential Turing, p22.
 Enticknap, Nicholas (Summer 1998), "Computing's Golden Jubilee", Resurrection 
(20), ISSN 0958-7403, archived from the original on 9 January 2012, retrieved 
19 April 2008
 "Early computers at Manchester University", Resurrection, 1 (4), Summer 1992, 
ISSN 0958-7403, archived from the original on 28 August 2017, retrieved 7 July 
2010
 Early Electronic Computers (1946–51), University of Manchester, archived 
from the original on 5 January 2009, retrieved 16 November 2008
 Napper, R. B. E., Introduction to the Mark 1, The University of Manchester, 
archived from the original on 26 October 2008, retrieved 4 November 2008
 Computer Conservation Society, Our Computer Heritage Pilot Study: Deliveries 
of Ferranti Mark I and Mark I Star computers, archived from the original on 11 
December 2016, retrieved 9 January 2010
 Lavington, Simon. "A brief history of British computers: the first 25 years 
(1948–1973)". British Computer Society. Retrieved 10 January 2010.
 Lee, Thomas H. (2003). The Design of CMOS Radio-Frequency Integrated Circuits 
(PDF). Cambridge University Press. ISBN 9781139643771.
 Puers, Robert; Baldi, Livio; Voorde, Marcel Van de; Nooten, Sebastiaan E. van 
(2017). Nanoelectronics: Materials, Devices, Applications, 2 Volumes. John 
Wiley & Sons. p. 14. ISBN 9783527340538.
 Moskowitz, Sanford L. (2016). Advanced Materials Innovation: Managing Global 
Technology in the 21st century. John Wiley & Sons. pp. 165–167. ISBN 
9780470508923.
 Lavington, Simon (1998), A History of Manchester Computers (2 ed.), Swindon: 
The British Computer Society, pp. 34–35
 Cooke-Yarborough, E. H. (June 1998), "Some early transistor applications in 
the UK", Engineering Science & Education Journal, 7 (3): 100–106, 
doi:10.1049/esej:19980301, ISSN 0963-7346, retrieved 7 June 2009 (subscription 
required)
 Cooke-Yarborough, E.H. (1957). Introduction to Transistor Circuits. Edinburgh: 
Oliver and Boyd. p. 139.
 "1960: Metal Oxide Semiconductor (MOS) Transistor Demonstrated". The Silicon 
Engine: A Timeline of Semiconductors in Computers. Computer History Museum. 
Retrieved 31 August 2019.
 Motoyoshi, M. (2009). "Through-Silicon Via (TSV)" (PDF). Proceedings of the 
IEEE. 97 (1): 43–48. doi:10.1109/JPROC.2008.2007462. ISSN 0018-9219.
 "Transistors Keep Moore's Law Alive". EETimes. 12 December 2018. Retrieved 18 
July 2019.
 "Who Invented the Transistor?". Computer History Museum. 4 December 2013. 
Retrieved 20 July 2019.
 Hittinger, William C. (1973). "Metal-Oxide-Semiconductor Technology". 
Scientific American. 229 (2): 48–59. Bibcode:1973SciAm.229b..48H. 
doi:10.1038/scientificamerican0873-48. ISSN 0036-8733. JSTOR 24923169.
 "Transistors - an overview". ScienceDirect. Retrieved 8 August 2019.
 Malmstadt, Howard V.; Enke, Christie G.; Crouch, Stanley R. (1994). Making the 
Right Connections: Microcomputers and Electronic Instrumentation. American 
Chemical Society. p. 389. ISBN 9780841228610. The relative simplicity and low 
power requirements of MOSFETs have fostered today's microcomputer revolution.
 Fossum, Jerry G.; Trivedi, Vishal P. (2013). Fundamentals of Ultra-Thin-Body 
MOSFETs and FinFETs. Cambridge University Press. p. vii. ISBN 9781107434493.
 "Remarks by Director Iancu at the 2019 International Intellectual Property 
Conference". United States Patent and Trademark Office. 10 June 2019. Retrieved 
20 July 2019.
 "Dawon Kahng". National Inventors Hall of Fame. Retrieved 27 June 2019.
 "Martin Atalla in Inventors Hall of Fame, 2009". Retrieved 21 June 2013.
 "Triumph of the MOS Transistor". YouTube. Computer History Museum. 6 August 
2010. Retrieved 21 July 2019.
 "The Hapless Tale of Geoffrey Dummer" Archived 11 May 2013 at the Wayback 
Machine, (n.d.), (HTML), Electronic Product News, accessed 8 July 2008.
 Kilby, Jack (2000), Nobel lecture (PDF), Stockholm: Nobel Foundation, 
retrieved 15 May 2008
 The Chip that Jack Built, (c. 2008), (HTML), Texas Instruments, Retrieved 29 
May 2008.
 Jack S. Kilby, Miniaturized Electronic Circuits, United States Patent Office, 
US Patent 3,138,743, filed 6 February 1959, issued 23 June 1964.
 Winston, Brian (1998). Media Technology and Society: A History : From the 
Telegraph to the Internet. Routledge. p. 221. ISBN 978-0-415-14230-4.
 Saxena, Arjun N. (2009). Invention of Integrated Circuits: Untold Important 
Facts. World Scientific. p. 140. ISBN 9789812814456.
 "Integrated circuits". NASA. Retrieved 13 August 2019.
 Robert Noyce's Unitary circuit, US patent 2981877, "Semiconductor 
device-and-lead structure", issued 1961-04-25, assigned to Fairchild 
Semiconductor Corporation
 "1959: Practical Monolithic Integrated Circuit Concept Patented". Computer 
History Museum. Retrieved 13 August 2019.
 Lojek, Bo (2007). History of Semiconductor Engineering. Springer Science & 
Business Media. p. 120. ISBN 9783540342588.
 Bassett, Ross Knox (2007). To the Digital Age: Research Labs, Start-up 
Companies, and the Rise of MOS Technology. Johns Hopkins University Press. p. 
46. ISBN 9780801886393.
 Huff, Howard R.; Tsuya, H.; Gösele, U. (1998). Silicon Materials Science and 
Technology: Proceedings of the Eighth International Symposium on Silicon 
Materials Science and Technology. Electrochemical Society. pp. 181–182.
 Kuo, Yue (1 January 2013). "Thin Film Transistor Technology—Past, Present, 
and Future" (PDF). The Electrochemical Society Interface. 22 (1): 55–61. 
doi:10.1149/2.F06131if. ISSN 1064-8208.
 "1960: Metal Oxide Semiconductor (MOS) Transistor Demonstrated". Computer 
History Museum.
 Bassett, Ross Knox (2007). To the Digital Age: Research Labs, Start-up 
Companies, and the Rise of MOS Technology. Johns Hopkins University Press. pp. 
22–25. ISBN 9780801886393.
 "Tortoise of Transistors Wins the Race - CHM Revolution". Computer History 
Museum. Retrieved 22 July 2019.
 "1964 – First Commercial MOS IC Introduced". Computer History Museum.
 "1968: Silicon Gate Technology Developed for ICs". Computer History Museum. 
Retrieved 22 July 2019.
 Kuo, Yue (1 January 2013). "Thin Film Transistor Technology—Past, Present, 
and Future" (PDF). The Electrochemical Society Interface. 22 (1): 55–61. 
doi:10.1149/2.F06131if. ISSN 1064-8208.
 "1971: Microprocessor Integrates CPU Function onto a Single Chip". Computer 
History Museum. Retrieved 22 July 2019.
 Colinge, Jean-Pierre; Greer, James C. (2016). Nanowire Transistors: Physics of 
Devices and Materials in One Dimension. Cambridge University Press. p. 2. ISBN 
9781107052406.
 Intel's First Microprocessor—the Intel 4004, Intel Corp., November 1971, 
archived from the original on 13 May 2008, retrieved 17 May 2008
 The Intel 4004 (1971) die was 12 mm2, composed of 2300 transistors; by 
comparison, the Pentium Pro was 306 mm2, composed of 5.5 million transistors, 
according to Patterson, David; Hennessy, John (1998), Computer Organization and 
Design, San Francisco: Morgan Kaufmann, pp. 27–39, ISBN 978-1-55860-428-5
 Federico Faggin, The Making of the First Microprocessor, IEEE Solid-State 
Circuits Magazine, Winter 2009, IEEE Xplore
 "7 dazzling smartphone improvements with Qualcomm's Snapdragon 835 chip". 3 
January 2017.
 Chartier, David (23 December 2008). "Global notebook shipments finally 
overtake desktops". Ars Technica.
 IDC (25 July 2013). "Growth Accelerates in the Worldwide Mobile Phone and 
Smartphone Markets in the Second Quarter, According to IDC". Archived from the 
original on 26 June 2014.
 Most major 64-bit instruction set architectures are extensions of earlier 
designs. All of the architectures listed in this table, except for Alpha, 
existed in 32-bit forms before their 64-bit incarnations were introduced.
 The control unit's role in interpreting instructions has varied somewhat in 
the past. Although the control unit is solely responsible for instruction 
interpretation in most modern computers, this is not always the case. Some 
computers have instructions that are partially interpreted by the control unit 
with further interpretation performed by another device. For example, EDVAC, 
one of the earliest stored-program computers, used a central control unit that 
only interpreted four instructions. All of the arithmetic-related instructions 
were passed on to its arithmetic unit and further decoded there.
 Instructions often occupy more than one memory address, therefore the program 
counter usually increases by the number of memory locations required to store 
one instruction.
 David J. Eck (2000). The Most Complex Machine: A Survey of Computers and 
Computing. A K Peters, Ltd. p. 54. ISBN 978-1-56881-128-4.
 Erricos John Kontoghiorghes (2006). Handbook of Parallel Computing and 
Statistics. CRC Press. p. 45. ISBN 978-0-8247-4067-2.
 Flash memory also may only be rewritten a limited number of times before 
wearing out, making it less useful for heavy random access usage. (Verma & 
Mielke 1988)
 Donald Eadie (1968). Introduction to the Basic Computer. Prentice-Hall. p. 12.
 Arpad Barna; Dan I. Porat (1976). Introduction to Microcomputers and the 
Microprocessors. Wiley. p. 85. ISBN 978-0-471-05051-3.
 Jerry Peek; Grace Todino; John Strang (2002). Learning the UNIX Operating 
System: A Concise Guide for the New User. O'Reilly. p. 130. ISBN 
978-0-596-00261-9.
 Gillian M. Davis (2002). Noise Reduction in Speech Applications. CRC Press. p. 
111. ISBN 978-0-8493-0949-6.
 However, it is also very common to construct supercomputers out of many pieces 
of cheap commodity hardware; usually individual computers connected by 
networks. These so-called computer clusters can often provide supercomputer 
performance at a much lower cost than customized designs. While custom 
architectures are still used for most of the most powerful supercomputers, 
there has been a proliferation of cluster computers in recent years. (TOP500 
2006)
 Even some later computers were commonly programmed directly in machine code. 
Some minicomputers like the DEC PDP-8 could be programmed directly from a panel 
of switches. However, this method was usually used only as part of the booting 
process. Most modern computers boot entirely automatically by reading a boot 
program from some non-volatile memory.
 However, there is sometimes some form of machine language compatibility 
between different computers. An x86-64 compatible microprocessor like the AMD 
Athlon 64 is able to run most of the same programs that an Intel Core 2 
microprocessor can, as well as programs designed for earlier microprocessors 
like the Intel Pentiums and Intel 80486. This contrasts with very early 
commercial computers, which were often one-of-a-kind and totally incompatible 
with other computers.
 High level languages are also often interpreted rather than compiled. 
Interpreted languages are translated into machine code on the fly, while 
running, by another program called an interpreter.
 It is not universally true that bugs are solely due to programmer oversight. 
Computer hardware may fail or may itself have a fundamental problem that 
produces unexpected results in certain situations. For instance, the Pentium 
FDIV bug caused some Intel microprocessors in the early 1990s to produce 
inaccurate results for certain floating point division operations. This was 
caused by a flaw in the microprocessor design and resulted in a partial recall 
of the affected devices.
 Taylor, Alexander L., III (16 April 1984). "The Wizard Inside the Machine". 
TIME. Retrieved 17 February 2007. (subscription required)
 Agatha C. Hughes (2000). Systems, Experts, and Computers. MIT Press. p. 161. 
ISBN 978-0-262-08285-3. The experience of SAGE helped make possible the first 
truly large-scale commercial real-time network: the SABRE computerized airline 
reservations system ...
 Leiner, Barry M.; Cerf, Vinton G.; Clark, David D.; Kahn, Robert E.; 
Kleinrock, Leonard; Lynch, Daniel C.; Postel, Jon; Roberts, Larry G.; Wolf, 
Stephen (1999). "A Brief History of the Internet". Internet Society. 
arXiv:cs/9901011. Bibcode:1999cs........1011L. Retrieved 20 September 2008.
 According to the Shorter Oxford English Dictionary (6th ed, 2007), the word 
computer dates back to the mid 17th century, when it referred to "A person who 
makes calculations; specifically a person employed for this in an observatory 
etc."
 "Definition of computer". Thefreedictionary.com. Retrieved 29 January 2012.
 II, Joseph D. Dumas (2005). Computer Architecture: Fundamentals and Principles 
of Computer Design. CRC Press. p. 340. ISBN 9780849327490.
Notes
Evans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the 
Internet. New York: Portfolio/Penguin. ISBN 9780735211759.
Fuegi, J.; Francis, J. (2003). "Lovelace & Babbage and the creation of the 1843 
'notes'". IEEE Annals of the History of Computing. 25 (4): 16. 
doi:10.1109/MAHC.2003.1253887.
a Kempf, Karl (1961). "Historical Monograph: Electronic Computers Within the 
Ordnance Corps". Aberdeen Proving Ground (United States Army).
a Phillips, Tony (2000). "The Antikythera Mechanism I". American Mathematical 
Society. Retrieved 5 April 2006.
a Shannon, Claude Elwood (1940). A symbolic analysis of relay and switching 
circuits (Thesis). Massachusetts Institute of Technology. hdl:1721.1/11173.
Digital Equipment Corporation (1972). PDP-11/40 Processor Handbook (PDF). 
Maynard, MA: Digital Equipment Corporation.
Verma, G.; Mielke, N. (1988). "Reliability performance of ETOX based flash 
memories". IEEE International Reliability Physics Symposium.
Swade, Doron D. (February 1993). "Redeeming Charles Babbage's Mechanical 
Computer". Scientific American. 268 (2): 86–91. Bibcode:1993SciAm.268b..86S. 
doi:10.1038/scientificamerican0293-86. JSTOR 24941379.
Meuer, Hans; Strohmaier, Erich; Simon, Horst; Dongarra, Jack (13 November 
2006). "Architectures Share Over Time". TOP500. Archived from the original on 
20 February 2007. Retrieved 27 November 2006.
Lavington, Simon (1998). A History of Manchester Computers (2 ed.). Swindon: 
The British Computer Society. ISBN 978-0-902505-01-8.
Light, Jennifer S. (1999). "When Computers Were Women". Technology and Culture. 
40 (3): 455–483. JSTOR 25147356.
Stokes, Jon (2007). Inside the Machine: An Illustrated Introduction to 
Microprocessors and Computer Architecture. San Francisco: No Starch Press. ISBN 
978-1-59327-104-6.
Zuse, Konrad (1993). The Computer – My life. Berlin: Pringler-Verlag. ISBN 
978-0-387-56453-1.
Felt, Dorr E. (1916). Mechanical arithmetic, or The history of the counting 
machine. Chicago: Washington Institute.
Ifrah, Georges (2001). The Universal History of Computing: From the Abacus to 
the Quantum Computer. New York: John Wiley & Sons. ISBN 978-0-471-39671-0.
Berkeley, Edmund (1949). Giant Brains, or Machines That Think. John Wiley & 
Sons.
Cohen, Bernard (2000). Howard Aiken, Portrait of a computer pioneer. Physics 
Today. 53. Cambridge, Massachusetts: The MIT Press. pp. 74–75. 
Bibcode:2000PhT....53c..74C. doi:10.1063/1.883007. ISBN 978-0-262-53179-5.
Ligonnière, Robert (1987). Préhistoire et Histoire des ordinateurs. Paris: 
Robert Laffont. ISBN 978-2-221-05261-7.
Couffignal, Louis (1933). Les machines à calculer ; leurs principes, leur 
évolution. Paris: Gauthier-Villars.
Essinger, James (2004). Jacquard's Web, How a hand loom led to the birth of the 
information age. Oxford University Press. ISBN 978-0-19-280577-5.
Hyman, Anthony (1985). Charles Babbage: Pioneer of the Computer. Princeton 
University Press. ISBN 978-0-691-02377-9.
Bowden, B. V. (1953). Faster than thought. New York, Toronto, London: Pitman 
publishing corporation.
Moseley, Maboth (1964). Irascible Genius, Charles Babbage, inventor. London: 
Hutchinson.
Collier, Bruce (1970). The little engine that could've: The calculating 
machines of Charles Babbage. Garland Publishing Inc. ISBN 978-0-8240-0043-1.
Randell, Brian (1982). "From Analytical Engine to Electronic Digital Computer: 
The Contributions of Ludgate, Torres, and Bush" (PDF). Archived from the 
original (PDF) on 21 September 2013. Retrieved 29 October 2013.
Smith, Erika E. (2013). "Recognizing a Collective Inheritance through the 
History of Women in Computing". CLCWeb: Comparative Literature and Culture. 15 
(1): 1–9. doi:10.7771/1481-4374.1972.



World War II
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
"The Second World War", "WWII", and "WW2" redirect here. For other uses, see 
The Second World War (disambiguation) and WWII (disambiguation).
World War II
Infobox collage for WWII.PNG
(clockwise from top left)
Chinese forces in the Battle of WanjialingAustralian 25-pounder guns during the 
First Battle of El AlameinGerman Stuka dive bombers on the Eastern Front in 
December 1943US naval force in the Lingayen GulfWilhelm Keitel signing the 
German Instrument of SurrenderSoviet troops in the Battle of Stalingrad
Date	
1 September 1939 – 2 September 1945
(6 years and 1 day)[a]
Location	
Europe, Pacific, Atlantic, South-East Asia, China, Middle East, Mediterranean, 
North Africa, Horn of Africa, Australia, briefly North and South America
Result	
Allied victory
Collapse of Nazi Germany
Fall of the Japanese and Italian Empires
Allied military occupations of Germany, Japan, Austria and foundation of the 
Italian Republic
Beginning of the Nuclear Age
Dissolution of the League of Nations and creation of the United Nations
Emergence of the United States and the Soviet Union as rival superpowers and 
beginning of the Cold War (more ...)
Participants
Allies	Axis
Commanders and leaders
Main Allied leaders
Soviet Union Joseph Stalin
United States Franklin D. Roosevelt
United Kingdom Winston Churchill
Republic of China (1912–1949) Chiang Kai-shek
Main Axis leaders
Nazi Germany Adolf Hitler
Empire of Japan Hirohito
Fascist Italy (1922–1943) Benito Mussolini
Casualties and losses
Military dead:
Over 16,000,000
Civilian dead:
Over 45,000,000
Total dead:
Over 61,000,000
(1937–1945)
...further details
Military dead:
Over 8,000,000
Civilian dead:
Over 4,000,000
Total dead:
Over 12,000,000
(1937–1945)
...further details
vte
Campaigns of World War II
World War II
Alphabetical indices
A B C D E F G H I J K L M
N O P Q R S T U V W X Y Z
0–9
Navigation
CampaignsCountriesEquipment
TimelineOutlineLists
PortalCategoryBibliography
vte
World War II (often abbreviated to WWII or WW2), also known as the Second World 
War, was a global war that lasted from 1939 to 1945. The vast majority of the 
world's countries—including all the great powers—eventually formed two 
opposing military alliances: the Allies and the Axis. A state of total war 
emerged, directly involving more than 100 million people from more than 30 
countries. The major participants threw their entire economic, industrial, and 
scientific capabilities behind the war effort, blurring the distinction between 
civilian and military resources. World War II was the deadliest conflict in 
human history, marked by 70 to 85 million fatalities, most of whom were 
civilians in the Soviet Union and China. It included massacres, genocides 
including the Holocaust, strategic bombing, premeditated death from starvation 
and disease, and the only use of nuclear weapons in war.

Japan, which aimed to dominate Asia and the Pacific, was at war with China by 
1937,[b] though neither side had declared war on the other. World War II is 
generally said to have begun on 1 September 1939, with the invasion of Poland 
by Germany and subsequent declarations of war on Germany by France and the 
United Kingdom. From late 1939 to early 1941, in a series of campaigns and 
treaties, Germany conquered or controlled much of continental Europe, and 
formed the Axis alliance with Italy and Japan. Under the Molotov–Ribbentrop 
Pact of August 1939, Germany and the Soviet Union partitioned and annexed 
territories of their European neighbours, Poland, Finland, Romania and the 
Baltic states. Following the onset of campaigns in North Africa and East 
Africa, and the Fall of France in mid 1940, the war continued primarily between 
the European Axis powers and the British Empire. War in the Balkans, the aerial 
Battle of Britain, the Blitz, and the long Battle of the Atlantic followed. On 
22 June 1941, the European Axis powers launched an invasion of the Soviet 
Union, opening the largest land theatre of war in history. This Eastern Front 
trapped the Axis, most crucially the German Wehrmacht, in a war of attrition. 
In December 1941, Japan launched a surprise attack on the United States as well 
as European colonies in the Pacific. Following an immediate U.S. declaration of 
war against Japan, supported by one from Great Britain, the European Axis 
powers quickly declared war on the U.S. in solidarity with their Japanese ally. 
Rapid Japanese conquests over much of the Western Pacific ensued, perceived by 
many in Asia as liberation from Western dominance and resulting in the support 
of several armies from defeated territories.

The Axis advance in the Pacific halted in 1942 when Japan lost the critical 
Battle of Midway; later, Germany and Italy were defeated in North Africa and 
then, decisively, at Stalingrad in the Soviet Union. Key setbacks in 1943, 
which included a series of German defeats on the Eastern Front, the Allied 
invasions of Sicily and Italy, and Allied victories in the Pacific, cost the 
Axis its initiative and forced it into strategic retreat on all fronts. In 
1944, the Western Allies invaded German-occupied France, while the Soviet Union 
regained its territorial losses and turned toward Germany and its allies. 
During 1944 and 1945 the Japanese suffered major reverses in mainland Asia, in 
Central China, South China and Burma, while the Allies crippled the Japanese 
Navy and captured key Western Pacific islands.

The war in Europe concluded with an invasion of Germany by the Western Allies 
and the Soviet Union, culminating in the capture of Berlin by Soviet troops, 
the suicide of Adolf Hitler and the German unconditional surrender on 8 May 
1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the 
refusal of Japan to surrender under its terms, the United States dropped atomic 
bombs on the Japanese cities of Hiroshima and Nagasaki on 6 and 9 August 
respectively. With an invasion of the Japanese archipelago imminent, the 
possibility of additional atomic bombings, the Soviet entry into the war 
against Japan and its invasion of Manchuria, Japan announced its intention to 
surrender on 15 August 1945, cementing total victory in Asia for the Allies. 
Tribunals were set up by the Allies, and war crimes trials were conducted in 
the wake of the war both against the Germans and against the Japanese.

World War II changed the political alignment and social structure of the globe. 
The United Nations (UN) was established to foster international co-operation 
and prevent future conflicts; the victorious great powers—China, France, the 
Soviet Union, the United Kingdom, and the United States—became the permanent 
members of its Security Council. The Soviet Union and United States emerged as 
rival superpowers, setting the stage for the nearly half-century long Cold War. 
In the wake of European devastation, the influence of its great powers waned, 
triggering the decolonisation of Africa and Asia. Most countries whose 
industries had been damaged moved towards economic recovery and expansion. 
Political integration, especially in Europe, emerged as an effort to end 
pre-war enmities and create a common identity.


Contents
1	Chronology
2	Background
2.1	Europe
2.2	Asia
3	Pre-war events
3.1	Italian invasion of Ethiopia (1935)
3.2	Spanish Civil War (1936–1939)
3.3	Japanese invasion of China (1937)
3.4	Soviet–Japanese border conflicts
3.5	European occupations and agreements
4	Course of the war
4.1	War breaks out in Europe (1939–40)
4.2	Western Europe (1940–41)
4.3	Mediterranean (1940–41)
4.4	Axis attack on the Soviet Union (1941)
4.5	War breaks out in the Pacific (1941)
4.6	Axis advance stalls (1942–43)
4.7	Allies gain momentum (1943–44)
4.8	Allies close in (1944)
4.9	Axis collapse, Allied victory (1944–45)
5	Aftermath
6	Impact
6.1	Casualties and war crimes
6.2	Genocide, concentration camps, and slave labour
6.3	Occupation
6.4	Home fronts and production
6.5	Advances in technology and warfare
7	See also
8	Notes
9	Citations
10	References
11	External links
Chronology
See also: Timeline of World War II
Timelines of World War II
Chronological
Prelude
(in Asiain Europe)
1939194019411942
194319441945 onwards
By topic
Diplomacy
Declarations of war
EngagementsOperations
Battle of Europe air operations
Eastern FrontManhattan Project
United Kingdom home front
Surrender of the Axis armies
vte
The start of the war in Europe is generally held to be 1 September 1939,[2][3] 
beginning with the German invasion of Poland; the United Kingdom and France 
declared war on Germany two days later. The dates for the beginning of war in 
the Pacific include the start of the Second Sino-Japanese War on 7 July 
1937,[4][5] or even the Japanese invasion of Manchuria on 19 September 
1931.[6][7]

Others follow the British historian A.J.P. Taylor, who held that the 
Sino-Japanese War and war in Europe and its colonies occurred simultaneously, 
and the two wars merged in 1941. This article uses the conventional dating. 
Other starting dates sometimes used for World War II include the Italian 
invasion of Abyssinia on 3 October 1935.[8] The British historian Antony Beevor 
views the beginning of World War II as the Battles of Khalkhin Gol fought 
between Japan and the forces of Mongolia and the Soviet Union from May to 
September 1939.[9]

The exact date of the war's end is also not universally agreed upon. It was 
generally accepted at the time that the war ended with the armistice of 14 
August 1945 (V-J Day), rather than the formal surrender of Japan, which was on 
2 September 1945 that officially ended the war in Asia. A peace treaty with 
Japan was signed in 1951.[10] A treaty regarding Germany's future allowed the 
reunification of East and West Germany to take place in 1990 and resolved most 
post-World War II issues.[11] No formal peace treaty between Japan and the 
Soviet Union was ever signed.[12]

Background
Main article: Causes of World War II
Europe
World War I had radically altered the political European map, with the defeat 
of the Central Powers—including Austria-Hungary, Germany, Bulgaria and the 
Ottoman Empire—and the 1917 Bolshevik seizure of power in Russia, which 
eventually led to the founding of the Soviet Union. Meanwhile, the victorious 
Allies of World War I, such as France, Belgium, Italy, Romania and Greece, 
gained territory, and new nation-states were created out of the collapse of 
Austria-Hungary and the Ottoman and Russian Empires.


The League of Nations assembly, held in Geneva, Switzerland, 1930
To prevent a future world war, the League of Nations was created during the 
1919 Paris Peace Conference. The organisation's primary goals were to prevent 
armed conflict through collective security, military and naval disarmament, and 
settling international disputes through peaceful negotiations and arbitration.

Despite strong pacifist sentiment after World War I,[13] its aftermath still 
caused irredentist and revanchist nationalism in several European states. These 
sentiments were especially marked in Germany because of the significant 
territorial, colonial, and financial losses incurred by the Treaty of 
Versailles. Under the treaty, Germany lost around 13 percent of its home 
territory and all its overseas possessions, while German annexation of other 
states was prohibited, reparations were imposed, and limits were placed on the 
size and capability of the country's armed forces.[14]

The German Empire was dissolved in the German Revolution of 1918–1919, and a 
democratic government, later known as the Weimar Republic, was created. The 
interwar period saw strife between supporters of the new republic and hardline 
opponents on both the right and left. Italy, as an Entente ally, had made some 
post-war territorial gains; however, Italian nationalists were angered that the 
promises made by the United Kingdom and France to secure Italian entrance into 
the war were not fulfilled in the peace settlement. From 1922 to 1925, the 
Fascist movement led by Benito Mussolini seized power in Italy with a 
nationalist, totalitarian, and class collaborationist agenda that abolished 
representative democracy, repressed socialist, left-wing and liberal forces, 
and pursued an aggressive expansionist foreign policy aimed at making Italy a 
world power, promising the creation of a "New Roman Empire".[15]


Adolf Hitler at a German National Socialist political rally in Nuremberg, 
August 1933
Adolf Hitler, after an unsuccessful attempt to overthrow the German government 
in 1923, eventually became the Chancellor of Germany in 1933. He abolished 
democracy, espousing a radical, racially motivated revision of the world order, 
and soon began a massive rearmament campaign.[16] Meanwhile, France, to secure 
its alliance, allowed Italy a free hand in Ethiopia, which Italy desired as a 
colonial possession. The situation was aggravated in early 1935 when the 
Territory of the Saar Basin was legally reunited with Germany and Hitler 
repudiated the Treaty of Versailles, accelerated his rearmament programme, and 
introduced conscription.[17]

The United Kingdom, France and Italy formed the Stresa Front in April 1935 in 
order to contain Germany, a key step towards military globalisation; however, 
that June, the United Kingdom made an independent naval agreement with Germany, 
easing prior restrictions. The Soviet Union, concerned by Germany's goals of 
capturing vast areas of Eastern Europe, drafted a treaty of mutual assistance 
with France. Before taking effect, though, the Franco-Soviet pact was required 
to go through the bureaucracy of the League of Nations, which rendered it 
essentially toothless.[18] The United States, concerned with events in Europe 
and Asia, passed the Neutrality Act in August of the same year.[19]

Hitler defied the Versailles and Locarno treaties by remilitarising the 
Rhineland in March 1936, encountering little opposition due to appeasement.[20] 
In October 1936, Germany and Italy formed the Rome–Berlin Axis. A month 
later, Germany and Japan signed the Anti-Comintern Pact, which Italy joined the 
following year.[21]

Asia
The Kuomintang (KMT) party in China launched a unification campaign against 
regional warlords and nominally unified China in the mid-1920s, but was soon 
embroiled in a civil war against its former Chinese Communist Party allies[22] 
and new regional warlords. In 1931, an increasingly militaristic Empire of 
Japan, which had long sought influence in China[23] as the first step of what 
its government saw as the country's right to rule Asia, staged the Mukden 
Incident as a pretext to invade Manchuria and establish the puppet state of 
Manchukuo.[24]

China appealed to the League of Nations to stop the Japanese invasion of 
Manchuria. Japan withdrew from the League of Nations after being condemned for 
its incursion into Manchuria. The two nations then fought several battles, in 
Shanghai, Rehe and Hebei, until the Tanggu Truce was signed in 1933. 
Thereafter, Chinese volunteer forces continued the resistance to Japanese 
aggression in Manchuria, and Chahar and Suiyuan.[25] After the 1936 Xi'an 
Incident, the Kuomintang and communist forces agreed on a ceasefire to present 
a united front to oppose Japan.[26]

Pre-war events
Italian invasion of Ethiopia (1935)
Main article: Second Italo-Ethiopian War

Benito Mussolini inspecting troops during the Italo-Ethiopian War, 1935
The Second Italo–Ethiopian War was a brief colonial war that began in October 
1935 and ended in May 1936. The war began with the invasion of the Ethiopian 
Empire (also known as Abyssinia) by the armed forces of the Kingdom of Italy 
(Regno d'Italia), which was launched from Italian Somaliland and Eritrea.[27] 
The war resulted in the military occupation of Ethiopia and its annexation into 
the newly created colony of Italian East Africa (Africa Orientale Italiana, or 
AOI); in addition it exposed the weakness of the League of Nations as a force 
to preserve peace. Both Italy and Ethiopia were member nations, but the League 
did little when the former clearly violated Article X of the League's 
Covenant.[28] The United Kingdom and France supported imposing sanctions on 
Italy for the invasion, but they were not fully enforced and failed to end the 
Italian invasion.[29] Italy subsequently dropped its objections to Germany's 
goal of absorbing Austria.[30]


Spanish Civil War (1936–1939)
Main article: Spanish Civil War

The bombing of Guernica in 1937, during the Spanish Civil War, sparked fears 
abroad in Europe that the next war would be based on bombing of cities with 
very high civilian casualties.
When civil war broke out in Spain, Hitler and Mussolini lent military support 
to the Nationalist rebels, led by General Francisco Franco. Italy supported the 
Nationalists to a greater extent than the Nazis did: altogether Mussolini sent 
to Spain more than 70,000 ground troops and 6,000 aviation personnel, as well 
as about 720 aircraft.[31] The Soviet Union supported the existing government, 
the Spanish Republic. More than 30,000 foreign volunteers, known as the 
International Brigades, also fought against the Nationalists. Both Germany and 
the Soviet Union used this proxy war as an opportunity to test in combat their 
most advanced weapons and tactics. The Nationalists won the civil war in April 
1939; Franco, now dictator, remained officially neutral during World War II but 
generally favoured the Axis.[32] His greatest collaboration with Germany was 
the sending of volunteers to fight on the Eastern Front.[33]

Japanese invasion of China (1937)
Main article: Second Sino-Japanese War

Japanese Imperial Army soldiers during the Battle of Shanghai, 1937
In July 1937, Japan captured the former Chinese imperial capital of Peking 
after instigating the Marco Polo Bridge Incident, which culminated in the 
Japanese campaign to invade all of China.[34] The Soviets quickly signed a 
non-aggression pact with China to lend materiel support, effectively ending 
China's prior co-operation with Germany. From September to November, the 
Japanese attacked Taiyuan, engaged the Kuomintang Army around Xinkou,[35] and 
fought Communist forces in Pingxingguan.[36][37] Generalissimo Chiang Kai-shek 
deployed his best army to defend Shanghai, but, after three months of fighting, 
Shanghai fell. The Japanese continued to push the Chinese forces back, 
capturing the capital Nanking in December 1937. After the fall of Nanking, tens 
of thousands if not hundreds of thousands of Chinese civilians and disarmed 
combatants were murdered by the Japanese.[38][39]

In March 1938, Nationalist Chinese forces won their first major victory at 
Taierzhuang but then the city of Xuzhou was taken by Japanese in May.[40] In 
June 1938, Chinese forces stalled the Japanese advance by flooding the Yellow 
River; this manoeuvre bought time for the Chinese to prepare their defences at 
Wuhan, but the city was taken by October.[41] Japanese military victories did 
not bring about the collapse of Chinese resistance that Japan had hoped to 
achieve; instead the Chinese government relocated inland to Chongqing and 
continued the war.[42][43]

Soviet–Japanese border conflicts
Main article: Soviet–Japanese border conflicts

Red Army artillery unit during the Battle of Lake Khasan, 1938
In the mid-to-late 1930s, Japanese forces in Manchukuo had sporadic border 
clashes with the Soviet Union and Mongolia. The Japanese doctrine of 
Hokushin-ron, which emphasised Japan's expansion northward, was favoured by the 
Imperial Army during this time. With the Japanese defeat at Khalkin Gol in 
1939, the ongoing Second Sino-Japanese War[44] and ally Nazi Germany pursuing 
neutrality with the Soviets, this policy would prove difficult to maintain. 
Japan and the Soviet Union eventually signed a Neutrality Pact in April 1941, 
and Japan adopted the doctrine of Nanshin-ron, promoted by the Navy, which took 
its focus southward, eventually leading to its war with the United States and 
the Western Allies.[45][46]

European occupations and agreements

Chamberlain, Daladier, Hitler, Mussolini, and Ciano pictured just before 
signing the Munich Agreement, 29 September 1938
In Europe, Germany and Italy were becoming more aggressive. In March 1938, 
Germany annexed Austria, again provoking little response from other European 
powers.[47] Encouraged, Hitler began pressing German claims on the Sudetenland, 
an area of Czechoslovakia with a predominantly ethnic German population. Soon 
the United Kingdom and France followed the appeasement policy of British Prime 
Minister Neville Chamberlain and conceded this territory to Germany in the 
Munich Agreement, which was made against the wishes of the Czechoslovak 
government, in exchange for a promise of no further territorial demands.[48] 
Soon afterwards, Germany and Italy forced Czechoslovakia to cede additional 
territory to Hungary, and Poland annexed Czechoslovakia's Zaolzie region.[49]

Although all of Germany's stated demands had been satisfied by the agreement, 
privately Hitler was furious that British interference had prevented him from 
seizing all of Czechoslovakia in one operation. In subsequent speeches Hitler 
attacked British and Jewish "war-mongers" and in January 1939 secretly ordered 
a major build-up of the German navy to challenge British naval supremacy. In 
March 1939, Germany invaded the remainder of Czechoslovakia and subsequently 
split it into the German Protectorate of Bohemia and Moravia and a pro-German 
client state, the Slovak Republic.[50] Hitler also delivered 20 March 1939 
ultimatum to Lithuania, forcing the concession of the Klaipėda Region.[51]


German Foreign Minister Joachim von Ribbentrop (right) and the Soviet leader 
Joseph Stalin, after signing the Molotov–Ribbentrop Pact, 23 August 1939
Greatly alarmed and with Hitler making further demands on the Free City of 
Danzig, the United Kingdom and France guaranteed their support for Polish 
independence; when Italy conquered Albania in April 1939, the same guarantee 
was extended to Romania and Greece.[52] Shortly after the Franco-British pledge 
to Poland, Germany and Italy formalised their own alliance with the Pact of 
Steel.[53] Hitler accused the United Kingdom and Poland of trying to "encircle" 
Germany and renounced the Anglo-German Naval Agreement and the German–Polish 
Non-Aggression Pact.[54]

The situation reached a general crisis in late August as German troops 
continued to mobilise against the Polish border. On 23 August, when tripartite 
negotiations about a military alliance between France, the United Kingdom and 
Soviet Union stalled,[55] the Soviet Union signed a non-aggression pact with 
Germany.[56] This pact had a secret protocol that defined German and Soviet 
"spheres of influence" (western Poland and Lithuania for Germany; eastern 
Poland, Finland, Estonia, Latvia and Bessarabia for the Soviet Union), and 
raised the question of continuing Polish independence.[57] The pact neutralised 
the possibility of Soviet opposition to a campaign against Poland and assured 
that Germany would not have to face the prospect of a two-front war, as it had 
in World War I. Immediately after that, Hitler ordered the attack to proceed on 
26 August, but upon hearing that the United Kingdom had concluded a formal 
mutual assistance pact with Poland, and that Italy would maintain neutrality, 
he decided to delay it.[58]

In response to British requests for direct negotiations to avoid war, Germany 
made demands on Poland, which only served as a pretext to worsen relations.[59] 
On 29 August, Hitler demanded that a Polish plenipotentiary immediately travel 
to Berlin to negotiate the handover of Danzig, and to allow a plebiscite in the 
Polish Corridor in which the German minority would vote on secession.[59] The 
Poles refused to comply with the German demands, and on the night of 30–31 
August in a stormy meeting with the British ambassador Neville Henderson, 
Ribbentrop declared that Germany considered its claims rejected.[60]

Course of the war
Further information: Diplomatic history of World War II
War breaks out in Europe (1939–40)

Soldiers of the German Wehrmacht tearing down the border crossing into Poland, 
1 September 1939
On 1 September 1939, Germany invaded Poland after having staged several false 
flag border incidents as a pretext to initiate the attack.[61] The Battle of 
Westerplatte is often described as the first battle of the war.[62] The United 
Kingdom responded with an ultimatum to Germany to cease military operations, 
and on 3 September, after the ultimatum was ignored, France and Britain, along 
with their empires, declared war on Germany. As part of this, Australia, New 
Zealand, South Africa and Canada also joined the war. The alliance provided no 
direct military support to Poland, outside of a cautious French probe into the 
Saarland.[63] The Western Allies also began a naval blockade of Germany, which 
aimed to damage the country's economy and war effort.[64] Germany responded by 
ordering U-boat warfare against Allied merchant and warships, which would later 
escalate into the Battle of the Atlantic.[65]


Soldiers of the Polish Army during the defence of Poland, September 1939
On 8 September, German troops reached the suburbs of Warsaw. The Polish counter 
offensive to the west halted the German advance for several days, but it was 
outflanked and encircled by the Wehrmacht. Remnants of the Polish army broke 
through to besieged Warsaw. On 17 September 1939, after signing a cease-fire 
with Japan, the Soviets invaded Eastern Poland[66] under a pretext that the 
Polish state had ostensibly ceased to exist.[67] On 27 September, the Warsaw 
garrison surrendered to the Germans, and the last large operational unit of the 
Polish Army surrendered on 6 October. Despite the military defeat, Poland never 
surrendered; instead it formed the Polish government-in-exile and a clandestine 
state apparatus remained in occupied Poland.[68] A significant part of Polish 
military personnel evacuated to Romania and the Baltic countries; many of them 
would fight against the Axis in other theatres of the war.[69]

Germany annexed the western and occupied the central part of Poland, and the 
Soviet Union annexed its eastern part; small shares of Polish territory were 
transferred to Lithuania and Slovakia. On 6 October, Hitler made a public peace 
overture to the United Kingdom and France, but said that the future of Poland 
was to be determined exclusively by Germany and the Soviet Union. The proposal 
was rejected,[60] and Hitler ordered an immediate offensive against France,[70] 
which would be postponed until the spring of 1940 due to bad 
weather.[71][72][73]


Finnish machine gun nest aimed at Soviet Red Army positions during the Winter 
War, February 1940
The Soviet Union forced the Baltic countries—Estonia, Latvia and Lithuania, 
the states that were in the Soviet "sphere of influence" under the 
Molotov-Ribbentrop pact—to sign "mutual assistance pacts" that stipulated 
stationing Soviet troops in these countries. Soon after, significant Soviet 
military contingents were moved there.[74][75][76] Finland refused to sign a 
similar pact and rejected ceding part of its territory to the Soviet Union. The 
Soviet Union invaded Finland in November 1939,[77] and the Soviet Union was 
expelled from the League of Nations.[78] Despite overwhelming numerical 
superiority, Soviet military success was modest, and the Finno-Soviet war ended 
in March 1940 with minimal Finnish concessions.[79]

In June 1940, the Soviet Union forcibly annexed Estonia, Latvia and 
Lithuania,[75] and the disputed Romanian regions of Bessarabia, Northern 
Bukovina and Hertza. Meanwhile, Nazi-Soviet political rapprochement and 
economic co-operation[80][81] gradually stalled,[82][83] and both states began 
preparations for war.[84]

Western Europe (1940–41)
Main article: Western Front (World War II)

German advance into Belgium and Northern France, 10 May-4 June 1940, swept past 
the Maginot Line (shown in dark red)
In April 1940, Germany invaded Denmark and Norway to protect shipments of iron 
ore from Sweden, which the Allies were attempting to cut off.[85] Denmark 
capitulated after a few hours, and Norway was conquered within two months[86] 
despite Allied support. British discontent over the Norwegian campaign led to 
the appointment of Winston Churchill as Prime Minister on 10 May 1940.[87]

On the same day, Germany launched an offensive against France. To circumvent 
the strong Maginot Line fortifications on the Franco-German border, Germany 
directed its attack at the neutral nations of Belgium, the Netherlands, and 
Luxembourg.[88] The Germans carried out a flanking manoeuvre through the 
Ardennes region,[89] which was mistakenly perceived by Allies as an 
impenetrable natural barrier against armoured vehicles.[90][91] By successfully 
implementing new blitzkrieg tactics, the Wehrmacht rapidly advanced to the 
Channel and cut off the Allied forces in Belgium, trapping the bulk of the 
Allied armies in a cauldron on the Franco-Belgian border near Lille. The United 
Kingdom was able to evacuate a significant number of Allied troops from the 
continent by early June, although abandoning almost all their equipment.[92]

On 10 June, Italy invaded France, declaring war on both France and the United 
Kingdom.[93] The Germans turned south against the weakened French army, and 
Paris fell to them on 14 June. Eight days later France signed an armistice with 
Germany; it was divided into German and Italian occupation zones,[94] and an 
unoccupied rump state under the Vichy Regime, which, though officially neutral, 
was generally aligned with Germany. France kept its fleet, which the United 
Kingdom attacked on 3 July in an attempt to prevent its seizure by Germany.[95]


London seen from St. Paul's Cathedral after the German Blitz, 29 December 1940
The Battle of Britain[96] began in early July with Luftwaffe attacks on 
shipping and harbours.[97] The United Kingdom rejected Hitler's 
ultimatum,[which?][98] and the German air superiority campaign started in 
August but failed to defeat RAF Fighter Command. Due to this the proposed 
German invasion of Britain was postponed indefinitely on 17 September. The 
German strategic bombing offensive intensified with night attacks on London and 
other cities in the Blitz, but failed to significantly disrupt the British war 
effort[97] and largely ended in May 1941.[99]

Using newly captured French ports, the German Navy enjoyed success against an 
over-extended Royal Navy, using U-boats against British shipping in the 
Atlantic.[100] The British Home Fleet scored a significant victory on 27 May 
1941 by sinking the German battleship Bismarck.[101]

In November 1939, the United States was taking measures to assist China and the 
Western Allies, and amended the Neutrality Act to allow "cash and carry" 
purchases by the Allies.[102] In 1940, following the German capture of Paris, 
the size of the United States Navy was significantly increased. In September 
the United States further agreed to a trade of American destroyers for British 
bases.[103] Still, a large majority of the American public continued to oppose 
any direct military intervention in the conflict well into 1941.[104] In 
December 1940 Roosevelt accused Hitler of planning world conquest and ruled out 
any negotiations as useless, calling for the United States to become an 
"arsenal of democracy" and promoting Lend-Lease programmes of aid to support 
the British war effort.[98] The United States started strategic planning to 
prepare for a full-scale offensive against Germany.[105]

At the end of September 1940, the Tripartite Pact formally united Japan, Italy 
and Germany as the Axis Powers. The Tripartite Pact stipulated that any 
country, with the exception of the Soviet Union, which attacked any Axis Power 
would be forced to go to war against all three.[106] The Axis expanded in 
November 1940 when Hungary, Slovakia and Romania joined.[107] Romania and 
Hungary would make major contributions to the Axis war against the Soviet 
Union, in Romania's case partially to recapture territory ceded to the Soviet 
Union.[108]

Mediterranean (1940–41)
Main article: Mediterranean and Middle East theatre of World War II

Soldiers of the British Commonwealth forces from the Australian Army's 9th 
Division during the Siege of Tobruk; North African Campaign, August 1941
In early June 1940 the Italian Regia aeronautica attacked and besieged Malta, a 
British possession. In late summer through early autumn Italy conquered British 
Somaliland and made an incursion into British-held Egypt. In October Italy 
attacked Greece, but the attack was repulsed with heavy Italian casualties; the 
campaign ended within days with minor territorial changes.[109] Germany started 
preparation for an invasion of the Balkans to assist Italy, to prevent the 
British from gaining a foothold there, which would be a potential threat for 
Romanian oil fields, and to strike against the British dominance of the 
Mediterranean.[110]

In December 1940, British Empire forces began counter-offensives against 
Italian forces in Egypt and Italian East Africa.[111] The offensives were 
highly successful; by early February 1941 Italy had lost control of eastern 
Libya, and large numbers of Italian troops had been taken prisoner. The Italian 
Navy also suffered significant defeats, with the Royal Navy putting three 
Italian battleships out of commission by a carrier attack at Taranto and 
neutralising several more warships at the Battle of Cape Matapan.[112]


German panzers of the Afrika Korps advancing across the North African desert, 
1941
Italian defeats prompted Germany to deploy an expeditionary force to North 
Africa, and at the end of March 1941 Rommel's Afrika Korps launched an 
offensive which drove back the Commonwealth forces.[113] In under a month, Axis 
forces advanced to western Egypt and besieged the port of Tobruk.[114]

By late March 1941 Bulgaria and Yugoslavia signed the Tripartite Pact; however, 
the Yugoslav government was overthrown two days later by pro-British 
nationalists. Germany responded with simultaneous invasions of both Yugoslavia 
and Greece, commencing on 6 April 1941; both nations were forced to surrender 
within the month.[115] The airborne invasion of the Greek island of Crete at 
the end of May completed the German conquest of the Balkans.[116] Although the 
Axis victory was swift, bitter and large-scale partisan warfare subsequently 
broke out against the Axis occupation of Yugoslavia, which continued until the 
end of the war.[117]

In the Middle East, in May Commonwealth forces quashed an uprising in Iraq 
which had been supported by German aircraft from bases within Vichy-controlled 
Syria.[118] Between June and July they invaded and occupied the French 
possessions Syria and Lebanon, with the assistance of the Free French. [119]

Axis attack on the Soviet Union (1941)
Main article: Eastern Front (World War II)

European theatre of World War II animation map, 1939–1945 – Red: Western 
Allies and Soviet Union after 1941; Green: Soviet Union before 1941; Blue: Axis 
powers
With the situation in Europe and Asia relatively stable, Germany, Japan, and 
the Soviet Union made preparations. With the Soviets wary of mounting tensions 
with Germany and the Japanese planning to take advantage of the European War by 
seizing resource-rich European possessions in Southeast Asia, the two powers 
signed the Soviet–Japanese Neutrality Pact in April 1941.[120] By contrast, 
the Germans were steadily making preparations for an attack on the Soviet 
Union, massing forces on the Soviet border.[121]

Hitler believed that the United Kingdom's refusal to end the war was based on 
the hope that the United States and the Soviet Union would enter the war 
against Germany sooner or later.[122] He therefore decided to try to strengthen 
Germany's relations with the Soviets, or failing that to attack and eliminate 
them as a factor. In November 1940, negotiations took place to determine if the 
Soviet Union would join the Tripartite Pact. The Soviets showed some interest, 
but asked for concessions from Finland, Bulgaria, Turkey, and Japan that 
Germany considered unacceptable. On 18 December 1940, Hitler issued the 
directive to prepare for an invasion of the Soviet Union.[123]


German soldiers during the invasion of the Soviet Union by the Axis powers, 
1941
On 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet 
Union in Operation Barbarossa, with Germany accusing the Soviets of plotting 
against them. They were joined shortly by Finland and Hungary.[124] The primary 
targets of this surprise offensive[125] were the Baltic region, Moscow and 
Ukraine, with the ultimate goal of ending the 1941 campaign near the 
Arkhangelsk-Astrakhan line, from the Caspian to the White Seas. Hitler's 
objectives were to eliminate the Soviet Union as a military power, exterminate 
Communism, generate Lebensraum ("living space")[126] by dispossessing the 
native population[127] and guarantee access to the strategic resources needed 
to defeat Germany's remaining rivals.[128]

Although the Red Army was preparing for strategic counter-offensives before the 
war,[129] Barbarossa forced the Soviet supreme command to adopt a strategic 
defence. During the summer, the Axis made significant gains into Soviet 
territory, inflicting immense losses in both personnel and materiel. By 
mid-August, however, the German Army High Command decided to suspend the 
offensive of a considerably depleted Army Group Centre, and to divert the 2nd 
Panzer Group to reinforce troops advancing towards central Ukraine and 
Leningrad.[130] The Kiev offensive was overwhelmingly successful, resulting in 
encirclement and elimination of four Soviet armies, and made possible further 
advance into Crimea and industrially developed Eastern Ukraine (the First 
Battle of Kharkov).[131]


Soviet civilians leaving destroyed houses after a German bombardment during the 
Battle of Leningrad, 10 December 1942
The diversion of three quarters of the Axis troops and the majority of their 
air forces from France and the central Mediterranean to the Eastern Front[132] 
prompted the United Kingdom to reconsider its grand strategy.[133] In July, the 
UK and the Soviet Union formed a military alliance against Germany[134] The 
British and Soviets invaded neutral Iran to secure the Persian Corridor and 
Iran's oil fields.[135] In August, the United Kingdom and the United States 
jointly issued the Atlantic Charter, which outlined British and American goals 
for the war, even though America had yet to officially join.[136]

By October Axis operational objectives in Ukraine and the Baltic region were 
achieved, with only the sieges of Leningrad[137] and Sevastopol 
continuing.[138] A major offensive against Moscow was renewed; after two months 
of fierce battles in increasingly harsh weather the German army almost reached 
the outer suburbs of Moscow, where the exhausted troops[139] were forced to 
suspend their offensive.[140] Large territorial gains were made by Axis forces, 
but their campaign had failed to achieve its main objectives: two key cities 
remained in Soviet hands, the Soviet capability to resist was not broken, and 
the Soviet Union retained a considerable part of its military potential. The 
blitzkrieg phase of the war in Europe had ended.[141]

By early December, freshly mobilised reserves[142] allowed the Soviets to 
achieve numerical parity with Axis troops.[143] This, as well as intelligence 
data which established that a minimal number of Soviet troops in the East would 
be sufficient to deter any attack by the Japanese Kwantung Army,[144] allowed 
the Soviets to begin a massive counter-offensive that started on 5 December all 
along the front and pushed German troops 100–250 kilometres (62–155 mi) 
west.[145]

War breaks out in the Pacific (1941)
Main article: Pacific War
In 1939, the United States had renounced its trade treaty with Japan, and 
beginning with an aviation gasoline ban in July 1940, Japan became subject to 
increasing economic pressure.[98] During this time, Japan launched its first 
attack against Changsha, a strategically important Chinese city, but was 
repulsed by late September.[146] Despite several offensives by both sides, the 
war between China and Japan was stalemated by 1940. To increase pressure on 
China by blocking supply routes, and to better position Japanese forces in the 
event of a war with the Western powers, Japan invaded and occupied northern 
Indochina.[147] Afterwards, the United States embargoed iron, steel and 
mechanical parts against Japan.[148]

Chinese nationalist forces launched a large-scale counter-offensive in early 
1940. In August, Chinese communists launched an offensive in Central China; in 
retaliation, Japan instituted harsh measures in occupied areas to reduce human 
and material resources for the communists.[149] Continued antipathy between 
Chinese communist and nationalist forces culminated in armed clashes in January 
1941, effectively ending their co-operation.[150] In March, the Japanese 11th 
army attacked the headquarters of the Chinese 19th army but was repulsed during 
Battle of Shanggao.[151] In September, Japan attempted to take the city of 
Changsha again and clashed with Chinese nationalist forces.[152]


Mitsubishi A6M2 "Zero" fighters on the Imperial Japanese Navy aircraft carrier 
Shōkaku, just before the attack on Pearl Harbor
German successes in Europe encouraged Japan to increase pressure on European 
governments in Southeast Asia. The Dutch government agreed to provide Japan 
some oil supplies from the Dutch East Indies, but negotiations for additional 
access to their resources ended in failure in June 1941.[153] In July 1941 
Japan sent troops to southern Indochina, thus threatening British and Dutch 
possessions in the Far East. The United States, United Kingdom and other 
Western governments reacted to this move with a freeze on Japanese assets and a 
total oil embargo.[154][155] At the same time, Japan was planning an invasion 
of the Soviet Far East, intending to capitalise off the German invasion in the 
west, but abandoned the operation after the sanctions.[156]

Since early 1941 the United States and Japan had been engaged in negotiations 
in an attempt to improve their strained relations and end the war in China. 
During these negotiations Japan advanced a number of proposals which were 
dismissed by the Americans as inadequate.[157] At the same time the United 
States, the United Kingdom, and the Netherlands engaged in secret discussions 
for the joint defence of their territories, in the event of a Japanese attack 
against any of them.[158] Roosevelt reinforced the Philippines (an American 
protectorate scheduled for independence in 1946) and warned Japan that the 
United States would react to Japanese attacks against any "neighboring 
countries".[158]


The USS Arizona was a total loss in the Japanese surprise air attack on the 
American Pacific Fleet at Pearl Harbor, Sunday 7 December 1941.
Frustrated at the lack of progress and feeling the pinch of the 
American-British-Dutch sanctions, Japan prepared for war. On 20 November a new 
government under Hideki Tojo presented an interim proposal as its final offer. 
It called for the end of American aid to China and for lifting the embargo on 
the supply of oil and other resources to Japan. In exchange, Japan promised not 
to launch any attacks in Southeast Asia and to withdraw its forces from 
southern Indochina.[157] The American counter-proposal of 26 November required 
that Japan evacuate all of China without conditions and conclude non-aggression 
pacts with all Pacific powers.[159] That meant Japan was essentially forced to 
choose between abandoning its ambitions in China, or seizing the natural 
resources it needed in the Dutch East Indies by force;[160][161] the Japanese 
military did not consider the former an option, and many officers considered 
the oil embargo an unspoken declaration of war.[162]

Japan planned to rapidly seize European colonies in Asia to create a large 
defensive perimeter stretching into the Central Pacific. The Japanese would 
then be free to exploit the resources of Southeast Asia while exhausting the 
over-stretched Allies by fighting a defensive war.[163][164] To prevent 
American intervention while securing the perimeter, it was further planned to 
neutralise the United States Pacific Fleet and the American military presence 
in the Philippines from the outset.[165] On 7 December 1941 (8 December in 
Asian time zones), Japan attacked British and American holdings with 
near-simultaneous offensives against Southeast Asia and the Central 
Pacific.[166] These included an attack on the American fleets at Pearl Harbor 
and the Philippines, landings in Thailand and Malaya,[166] and the Battle of 
Hong Kong.[167]

These attacks led the United States, United Kingdom, China, Australia and 
several other states to formally declare war on Japan, whereas the Soviet 
Union, being heavily involved in large-scale hostilities with European Axis 
countries, maintained its neutrality agreement with Japan.[168] Germany, 
followed by the other Axis states, declared war on the United States[169] in 
solidarity with Japan, citing as justification the American attacks on German 
war vessels that had been ordered by Roosevelt.[124][170]

Axis advance stalls (1942–43)

US President Franklin D. Roosevelt and British PM Winston Churchill seated at 
the Casablanca Conference, January 1943
On 1 January 1942, the Allied Big Four[171]—the Soviet Union, China, the 
United Kingdom and the United States—and 22 smaller or exiled governments 
issued the Declaration by United Nations, thereby affirming the Atlantic 
Charter,[172] and agreeing not to sign a separate peace with the Axis 
powers.[173]

During 1942, Allied officials debated on the appropriate grand strategy to 
pursue. All agreed that defeating Germany was the primary objective. The 
Americans favoured a straightforward, large-scale attack on Germany through 
France. The Soviets were also demanding a second front. The British, on the 
other hand, argued that military operations should target peripheral areas to 
wear out German strength, leading to increasing demoralisation, and bolster 
resistance forces. Germany itself would be subject to a heavy bombing campaign. 
An offensive against Germany would then be launched primarily by Allied armour 
without using large-scale armies.[174] Eventually, the British persuaded the 
Americans that a landing in France was infeasible in 1942 and they should 
instead focus on driving the Axis out of North Africa.[175]

At the Casablanca Conference in early 1943, the Allies reiterated the 
statements issued in the 1942 Declaration, and demanded the unconditional 
surrender of their enemies. The British and Americans agreed to continue to 
press the initiative in the Mediterranean by invading Sicily to fully secure 
the Mediterranean supply routes.[176] Although the British argued for further 
operations in the Balkans to bring Turkey into the war, in May 1943, the 
Americans extracted a British commitment to limit Allied operations in the 
Mediterranean to an invasion of the Italian mainland and to invade France in 
1944.[177]

Pacific (1942–43)

Map of Japanese military advances through mid-1942
By the end of April 1942, Japan and its ally Thailand had almost fully 
conquered Burma, Malaya, the Dutch East Indies, Singapore, and Rabaul, 
inflicting severe losses on Allied troops and taking a large number of 
prisoners.[178] Despite stubborn resistance by Filipino and US forces, the 
Philippine Commonwealth was eventually captured in May 1942, forcing its 
government into exile.[179] On 16 April, in Burma, 7,000 British soldiers were 
encircled by the Japanese 33rd Division during the Battle of Yenangyaung and 
rescued by the Chinese 38th Division.[180] Japanese forces also achieved naval 
victories in the South China Sea, Java Sea and Indian Ocean,[181] and bombed 
the Allied naval base at Darwin, Australia. In January 1942, the only Allied 
success against Japan was a Chinese victory at Changsha.[182] These easy 
victories over unprepared US and European opponents left Japan overconfident, 
as well as overextended.[183]

In early May 1942, Japan initiated operations to capture Port Moresby by 
amphibious assault and thus sever communications and supply lines between the 
United States and Australia. The planned invasion was thwarted when an Allied 
task force, centred on two American fleet carriers, fought Japanese naval 
forces to a draw in the Battle of the Coral Sea.[184] Japan's next plan, 
motivated by the earlier Doolittle Raid, was to seize Midway Atoll and lure 
American carriers into battle to be eliminated; as a diversion, Japan would 
also send forces to occupy the Aleutian Islands in Alaska.[185] In mid-May, 
Japan started the Zhejiang-Jiangxi Campaign in China, with the goal of 
inflicting retribution on the Chinese who aided the surviving American airmen 
in the Doolittle Raid by destroying air bases and fighting against the Chinese 
23rd and 32nd Army Groups.[186][187] In early June, Japan put its operations 
into action, but the Americans, having broken Japanese naval codes in late May, 
were fully aware of the plans and order of battle, and used this knowledge to 
achieve a decisive victory at Midway over the Imperial Japanese Navy.[188]


US Marines during the Guadalcanal Campaign, in the Pacific theatre, 1942
With its capacity for aggressive action greatly diminished as a result of the 
Midway battle, Japan chose to focus on a belated attempt to capture Port 
Moresby by an overland campaign in the Territory of Papua.[189] The Americans 
planned a counter-attack against Japanese positions in the southern Solomon 
Islands, primarily Guadalcanal, as a first step towards capturing Rabaul, the 
main Japanese base in Southeast Asia.[190]

Both plans started in July, but by mid-September, the Battle for Guadalcanal 
took priority for the Japanese, and troops in New Guinea were ordered to 
withdraw from the Port Moresby area to the northern part of the island, where 
they faced Australian and United States troops in the Battle of Buna-Gona.[191] 
Guadalcanal soon became a focal point for both sides with heavy commitments of 
troops and ships in the battle for Guadalcanal. By the start of 1943, the 
Japanese were defeated on the island and withdrew their troops.[192] In Burma, 
Commonwealth forces mounted two operations. The first, an offensive into the 
Arakan region in late 1942, went disastrously, forcing a retreat back to India 
by May 1943.[193] The second was the insertion of irregular forces behind 
Japanese front-lines in February which, by the end of April, had achieved mixed 
results.[194]

Eastern Front (1942–43)

Red Army soldiers on the counterattack during the Battle of Stalingrad, 
February 1943
Despite considerable losses, in early 1942 Germany and its allies stopped a 
major Soviet offensive in central and southern Russia, keeping most territorial 
gains they had achieved during the previous year.[195] In May the Germans 
defeated Soviet offensives in the Kerch Peninsula and at Kharkov,[196] and then 
launched their main summer offensive against southern Russia in June 1942, to 
seize the oil fields of the Caucasus and occupy Kuban steppe, while maintaining 
positions on the northern and central areas of the front. The Germans split 
Army Group South into two groups: Army Group A advanced to the lower Don River 
and struck south-east to the Caucasus, while Army Group B headed towards the 
Volga River. The Soviets decided to make their stand at Stalingrad on the 
Volga.[197]

By mid-November, the Germans had nearly taken Stalingrad in bitter street 
fighting. The Soviets began their second winter counter-offensive, starting 
with an encirclement of German forces at Stalingrad,[198] and an assault on the 
Rzhev salient near Moscow, though the latter failed disastrously.[199] By early 
February 1943, the German Army had taken tremendous losses; German troops at 
Stalingrad had been defeated,[200] and the front-line had been pushed back 
beyond its position before the summer offensive. In mid-February, after the 
Soviet push had tapered off, the Germans launched another attack on Kharkov, 
creating a salient in their front line around the Soviet city of Kursk.[201]

Western Europe/Atlantic and Mediterranean (1942–43)

American 8th Air Force Boeing B-17 Flying Fortress bombing raid on the 
Focke-Wulf factory in Germany, 9 October 1943
Exploiting poor American naval command decisions, the German navy ravaged 
Allied shipping off the American Atlantic coast.[202] By November 1941, 
Commonwealth forces had launched a counter-offensive, Operation Crusader, in 
North Africa, and reclaimed all the gains the Germans and Italians had 
made.[203] In North Africa, the Germans launched an offensive in January, 
pushing the British back to positions at the Gazala Line by early 
February,[204] followed by a temporary lull in combat which Germany used to 
prepare for their upcoming offensives.[205] Concerns the Japanese might use 
bases in Vichy-held Madagascar caused the British to invade the island in early 
May 1942.[206] An Axis offensive in Libya forced an Allied retreat deep inside 
Egypt until Axis forces were stopped at El Alamein.[207] On the Continent, 
raids of Allied commandos on strategic targets, culminating in the disastrous 
Dieppe Raid,[208] demonstrated the Western Allies' inability to launch an 
invasion of continental Europe without much better preparation, equipment, and 
operational security.[209][page needed]

In August 1942, the Allies succeeded in repelling a second attack against El 
Alamein[210] and, at a high cost, managed to deliver desperately needed 
supplies to the besieged Malta.[211] A few months later, the Allies commenced 
an attack of their own in Egypt, dislodging the Axis forces and beginning a 
drive west across Libya.[212] This attack was followed up shortly after by 
Anglo-American landings in French North Africa, which resulted in the region 
joining the Allies.[213] Hitler responded to the French colony's defection by 
ordering the occupation of Vichy France;[213] although Vichy forces did not 
resist this violation of the armistice, they managed to scuttle their fleet to 
prevent its capture by German forces.[213][214] The Axis forces in Africa 
withdrew into Tunisia, which was conquered by the Allies in May 1943.[213][215]

In June 1943 the British and Americans began a strategic bombing campaign 
against Germany with a goal to disrupt the war economy, reduce morale, and 
"de-house" the civilian population.[216] The firebombing of Hamburg was among 
the first attacks in this campaign, inflicting significant casualties and 
considerable losses on infrastructure of this important industrial centre.[217]

Allies gain momentum (1943–44)

U.S. Navy SBD-5 scout plane flying patrol over USS Washington and USS Lexington 
during the Gilbert and Marshall Islands campaign, 1943
After the Guadalcanal Campaign, the Allies initiated several operations against 
Japan in the Pacific. In May 1943, Canadian and US forces were sent to 
eliminate Japanese forces from the Aleutians.[218] Soon after, the United 
States, with support from Australian and New Zealand forces, began major 
operations to isolate Rabaul by capturing surrounding islands, and breach the 
Japanese Central Pacific perimeter at the Gilbert and Marshall Islands.[219] By 
the end of March 1944, the Allies had completed both of these objectives, and 
had also neutralised the major Japanese base at Truk in the Caroline Islands. 
In April, the Allies launched an operation to retake Western New Guinea.[220]

In the Soviet Union, both the Germans and the Soviets spent the spring and 
early summer of 1943 preparing for large offensives in central Russia. On 4 
July 1943, Germany attacked Soviet forces around the Kursk Bulge. Within a 
week, German forces had exhausted themselves against the Soviets' deeply 
echeloned and well-constructed defences,[221] and for the first time in the war 
Hitler cancelled the operation before it had achieved tactical or operational 
success.[222] This decision was partially affected by the Western Allies' 
invasion of Sicily launched on 9 July, which, combined with previous Italian 
failures, resulted in the ousting and arrest of Mussolini later that 
month.[223]


Red Army troops in a counter-offensive on German positions at the Battle of 
Kursk, July 1943
On 12 July 1943, the Soviets launched their own counter-offensives, thereby 
dispelling any chance of German victory or even stalemate in the east. The 
Soviet victory at Kursk marked the end of German superiority,[224] giving the 
Soviet Union the initiative on the Eastern Front.[225][226] The Germans tried 
to stabilise their eastern front along the hastily fortified Panther–Wotan 
line, but the Soviets broke through it at Smolensk and by the Lower Dnieper 
Offensives.[227]

On 3 September 1943, the Western Allies invaded the Italian mainland, following 
Italy's armistice with the Allies.[228] Germany with the help of fascists 
responded by disarming Italian forces that were in many places without superior 
orders, seizing military control of Italian areas,[229] and creating a series 
of defensive lines.[230] German special forces then rescued Mussolini, who then 
soon established a new client state in German-occupied Italy named the Italian 
Social Republic,[231] causing an Italian civil war. The Western Allies fought 
through several lines until reaching the main German defensive line in 
mid-November.[232]

German operations in the Atlantic also suffered. By May 1943, as Allied 
counter-measures became increasingly effective, the resulting sizeable German 
submarine losses forced a temporary halt of the German Atlantic naval 
campaign.[233] In November 1943, Franklin D. Roosevelt and Winston Churchill 
met with Chiang Kai-shek in Cairo and then with Joseph Stalin in Tehran.[234] 
The former conference determined the post-war return of Japanese territory[235] 
and the military planning for the Burma Campaign,[236] while the latter 
included agreement that the Western Allies would invade Europe in 1944 and that 
the Soviet Union would declare war on Japan within three months of Germany's 
defeat.[237]


Ruins of the Benedictine monastery, during the Battle of Monte Cassino, Italian 
Campaign, May 1944
From November 1943, during the seven-week Battle of Changde, the Chinese forced 
Japan to fight a costly war of attrition, while awaiting Allied 
relief.[238][239][240] In January 1944, the Allies launched a series of attacks 
in Italy against the line at Monte Cassino and tried to outflank it with 
landings at Anzio.[241]

On 27 January 1944, Soviet troops launched a major offensive that expelled 
German forces from the Leningrad region, thereby ending the most lethal siege 
in history.[242] The following Soviet offensive was halted on the pre-war 
Estonian border by the German Army Group North aided by Estonians hoping to 
re-establish national independence. This delay slowed subsequent Soviet 
operations in the Baltic Sea region.[243] By late May 1944, the Soviets had 
liberated Crimea, largely expelled Axis forces from Ukraine, and made 
incursions into Romania, which were repulsed by the Axis troops.[244] The 
Allied offensives in Italy had succeeded and, at the expense of allowing 
several German divisions to retreat, on 4 June Rome was captured.[245]

The Allies had mixed success in mainland Asia. In March 1944, the Japanese 
launched the first of two invasions, an operation against British positions in 
Assam, India,[246] and soon besieged Commonwealth positions at Imphal and 
Kohima.[247] In May 1944, British forces mounted a counter-offensive that drove 
Japanese troops back to Burma by July,[247] and Chinese forces that had invaded 
northern Burma in late 1943 besieged Japanese troops in Myitkyina.[248] The 
second Japanese invasion of China aimed to destroy China's main fighting 
forces, secure railways between Japanese-held territory and capture Allied 
airfields.[249] By June, the Japanese had conquered the province of Henan and 
begun a new attack on Changsha in Hunan province.[250]

Allies close in (1944)

American troops approaching Omaha Beach during the invasion of Normandy on 
D-Day, 6 June 1944
On 6 June 1944 (known as D-Day), after three years of Soviet pressure,[251] the 
Western Allies invaded northern France. After reassigning several Allied 
divisions from Italy, they also attacked southern France.[252] These landings 
were successful, and led to the defeat of the German Army units in France. 
Paris was liberated on 25 August by the local resistance assisted by the Free 
French Forces, both led by General Charles de Gaulle,[253] and the Western 
Allies continued to push back German forces in western Europe during the latter 
part of the year. An attempt to advance into northern Germany spearheaded by a 
major airborne operation in the Netherlands failed.[254] After that, the 
Western Allies slowly pushed into Germany, but failed to cross the Ruhr river 
in a large offensive. In Italy, Allied advance also slowed due to the last 
major German defensive line.[255]


German SS soldiers from the Dirlewanger Brigade, tasked with suppressing the 
Warsaw Uprising against Nazi occupation, August 1944
On 22 June, the Soviets launched a strategic offensive in Belarus ("Operation 
Bagration") that destroyed the German Army Group Centre almost completely.[256] 
Soon after that, another Soviet strategic offensive forced German troops from 
Western Ukraine and Eastern Poland. The Soviets formed the Polish Committee of 
National Liberation to control territory in Poland and combat the Polish Armia 
Krajowa; The Soviet Red Army remained in the Praga district on the other side 
of the Vistula and watched passively as the Germans quelled the Warsaw Uprising 
initiated by the Armia Krajowa.[257] The national uprising in Slovakia was also 
quelled by the Germans.[citation needed] The Soviet Red Army's strategic 
offensive in eastern Romania cut off and destroyed the considerable German 
troops there and triggered a successful coup d'état in Romania and in 
Bulgaria, followed by those countries' shift to the Allied side.[258]

In September 1944, Soviet troops advanced into Yugoslavia and forced the rapid 
withdrawal of German Army Groups E and F in Greece, Albania and Yugoslavia to 
rescue them from being cut off.[259] By this point, the Communist-led Partisans 
under Marshal Josip Broz Tito, who had led an increasingly successful guerrilla 
campaign against the occupation since 1941, controlled much of the territory of 
Yugoslavia and engaged in delaying efforts against German forces further south. 
In northern Serbia, the Soviet Red Army, with limited support from Bulgarian 
forces, assisted the Partisans in a joint liberation of the capital city of 
Belgrade on 20 October. A few days later, the Soviets launched a massive 
assault against German-occupied Hungary that lasted until the fall of Budapest 
in February 1945.[260] Unlike impressive Soviet victories in the Balkans, 
bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus 
denied the Soviets occupation of Finland and led to a Soviet-Finnish armistice 
on relatively mild conditions,[261] although Finland was forced to fight their 
former ally Germany.[262][broken footnote]


General Douglas MacArthur returns to the Philippines during the Battle of 
Leyte, 20 October 1944
By the start of July 1944, Commonwealth forces in Southeast Asia had repelled 
the Japanese sieges in Assam, pushing the Japanese back to the Chindwin 
River[263] while the Chinese captured Myitkyina. In September 1944, Chinese 
forces captured Mount Song and reopened the Burma Road.[264] In China, the 
Japanese had more successes, having finally captured Changsha in mid-June and 
the city of Hengyang by early August.[265] Soon after, they invaded the 
province of Guangxi, winning major engagements against Chinese forces at Guilin 
and Liuzhou by the end of November[266] and successfully linking up their 
forces in China and Indochina by mid-December.[267]

In the Pacific, US forces continued to press back the Japanese perimeter. In 
mid-June 1944, they began their offensive against the Mariana and Palau 
islands, and decisively defeated Japanese forces in the Battle of the 
Philippine Sea. These defeats led to the resignation of the Japanese Prime 
Minister, Hideki Tojo, and provided the United States with air bases to launch 
intensive heavy bomber attacks on the Japanese home islands. In late October, 
American forces invaded the Filipino island of Leyte; soon after, Allied naval 
forces scored another large victory in the Battle of Leyte Gulf, one of the 
largest naval battles in history.[268]

Axis collapse, Allied victory (1944–45)

Yalta Conference held in February 1945, with Winston Churchill, Franklin D. 
Roosevelt and Joseph Stalin
On 16 December 1944, Germany made a last attempt on the Western Front by using 
most of its remaining reserves to launch a massive counter-offensive in the 
Ardennes and along the French–German border to split the Western Allies, 
encircle large portions of Western Allied troops and capture their primary 
supply port at Antwerp to prompt a political settlement.[269] By January, the 
offensive had been repulsed with no strategic objectives fulfilled.[269] In 
Italy, the Western Allies remained stalemated at the German defensive line. In 
mid-January 1945, the Soviets and Poles attacked in Poland, pushing from the 
Vistula to the Oder river in Germany, and overran East Prussia.[270] On 4 
February Soviet, British and US leaders met for the Yalta Conference. They 
agreed on the occupation of post-war Germany, and on when the Soviet Union 
would join the war against Japan.[271]

In February, the Soviets entered Silesia and Pomerania, while Western Allies 
entered western Germany and closed to the Rhine river. By March, the Western 
Allies crossed the Rhine north and south of the Ruhr, encircling the German 
Army Group B.[272] In early March, in an attempt to protect its last oil 
reserves in Hungary and to retake Budapest, Germany launched its last major 
offensive against Soviet troops near Lake Balaton. In two weeks, the offensive 
had been repulsed, the Soviets advanced to Vienna, and captured the city. In 
early April, Soviet troops captured Königsberg, while the Western Allies 
finally pushed forward in Italy and swept across western Germany capturing 
Hamburg and Nuremberg. American and Soviet forces met at the Elbe river on 25 
April, leaving several unoccupied pockets in southern Germany and around 
Berlin.


The German Reichstag after its capture by the Allied forces, 3 June 1945.
Soviet and Polish forces stormed and captured Berlin in late April. In Italy, 
German forces surrendered on 29 April. On 30 April, the Reichstag was captured, 
signalling the military defeat of Nazi Germany,[273] Berlin garrison 
surrendered on 2 May.

Several changes in leadership occurred during this period. On 12 April, 
President Roosevelt died and was succeeded by Harry S. Truman. Benito Mussolini 
was killed by Italian partisans on 28 April.[274] Two days later, Hitler 
committed suicide in besieged Berlin, and he was succeeded by Grand Admiral 
Karl Dönitz.[275] Total and unconditional surrender in Europe was signed on 7 
and 8 May, to be effective by the end of 8 May.[276] German Army Group Centre 
resisted in Prague until 11 May.[277]

In the Pacific theatre, American forces accompanied by the forces of the 
Philippine Commonwealth advanced in the Philippines, clearing Leyte by the end 
of April 1945. They landed on Luzon in January 1945 and recaptured Manila in 
March. Fighting continued on Luzon, Mindanao, and other islands of the 
Philippines until the end of the war.[278] Meanwhile, the United States Army 
Air Forces launched a massive firebombing campaign of strategic cities in Japan 
in an effort to destroy Japanese war industry and civilian morale. A 
devastating bombing raid on Tokyo of 9–10 March was the deadliest 
conventional bombing raid in history.[279]


Atomic bombing of Nagasaki on 9 August 1945.
In May 1945, Australian troops landed in Borneo, over-running the oilfields 
there. British, American, and Chinese forces defeated the Japanese in northern 
Burma in March, and the British pushed on to reach Rangoon by 3 May.[280] 
Chinese forces started a counterattack in the Battle of West Hunan that 
occurred between 6 April and 7 June 1945. American naval and amphibious forces 
also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of 
June.[281] At the same time, American submarines cut off Japanese imports, 
drastically reducing Japan's ability to supply its overseas forces.[282]

On 11 July, Allied leaders met in Potsdam, Germany. They confirmed earlier 
agreements about Germany,[283] and the American, British and Chinese 
governments reiterated the demand for unconditional surrender of Japan, 
specifically stating that "the alternative for Japan is prompt and utter 
destruction".[284] During this conference, the United Kingdom held its general 
election, and Clement Attlee replaced Churchill as Prime Minister.[285]

The call for unconditional surrender was rejected by the Japanese government, 
which believed it would be capable of negotiating for more favourable surrender 
terms.[286] In early August, the United States dropped atomic bombs on the 
Japanese cities of Hiroshima and Nagasaki. Between the two bombings, the 
Soviets, pursuant to the Yalta agreement, invaded Japanese-held Manchuria and 
quickly defeated the Kwantung Army, which was the largest Japanese fighting 
force,[287] thereby persuading previously adamant Imperial Army leaders to 
accept surrender terms.[288] The Red Army also captured the southern part of 
Sakhalin Island and the Kuril Islands. On 15 August 1945, Japan surrendered, 
with the surrender documents finally signed at Tokyo Bay on the deck of the 
American battleship USS Missouri on 2 September 1945, ending the war.[289]

Aftermath
Main articles: Aftermath of World War II and Consequences of Nazism

Ruins of Warsaw in January 1945, after the deliberate destruction of the city 
by the occupying German forces
The Allies established occupation administrations in Austria and Germany. The 
former became a neutral state, non-aligned with any political bloc. The latter 
was divided into western and eastern occupation zones controlled by the Western 
Allies and the Soviet Union. A denazification programme in Germany led to the 
prosecution of Nazi war criminals in the Nuremberg trials and the removal of 
ex-Nazis from power, although this policy moved towards amnesty and 
re-integration of ex-Nazis into West German society.[290]

Germany lost a quarter of its pre-war (1937) territory. Among the eastern 
territories, Silesia, Neumark and most of Pomerania were taken over by 
Poland,[291] and East Prussia was divided between Poland and the Soviet Union, 
followed by the expulsion to Germany of the nine million Germans from these 
provinces,[292][293] as well as three million Germans from the Sudetenland in 
Czechoslovakia. By the 1950s, one-fifth of West Germans were refugees from the 
east. The Soviet Union also took over the Polish provinces east of the Curzon 
line,[294] from which 2 million Poles were expelled;[293][295] north-east 
Romania,[296][297] parts of eastern Finland,[298] and the three Baltic states 
were incorporated into the Soviet Union.[299][300]


Defendants at the Nuremberg trials, where the Allied forces prosecuted 
prominent members of the political, military, judicial and economic leadership 
of Nazi Germany for crimes against humanity
In an effort to maintain world peace,[301] the Allies formed the United 
Nations, which officially came into existence on 24 October 1945,[302] and 
adopted the Universal Declaration of Human Rights in 1948 as a common standard 
for all member nations.[303] The great powers that were the victors of the 
war—France, China, the United Kingdom, the Soviet Union and the United 
States—became the permanent members of the UN's Security Council.[304] The 
five permanent members remain so to the present, although there have been two 
seat changes, between the Republic of China and the People's Republic of China 
in 1971, and between the Soviet Union and its successor state, the Russian 
Federation, following the dissolution of the Soviet Union in 1991. The alliance 
between the Western Allies and the Soviet Union had begun to deteriorate even 
before the war was over.[305]


Post-war border changes in Central Europe and creation of the Eastern Bloc
Germany had been de facto divided, and two independent states, the Federal 
Republic of Germany (West Germany) and the German Democratic Republic (East 
Germany),[306] were created within the borders of Allied and Soviet occupation 
zones. The rest of Europe was also divided into Western and Soviet spheres of 
influence.[307] Most eastern and central European countries fell into the 
Soviet sphere, which led to establishment of Communist-led regimes, with full 
or partial support of the Soviet occupation authorities. As a result, East 
Germany,[308] Poland, Hungary, Romania, Czechoslovakia, and Albania[309] became 
Soviet satellite states. Communist Yugoslavia conducted a fully independent 
policy, causing tension with the Soviet Union.[310]

Post-war division of the world was formalised by two international military 
alliances, the United States-led NATO and the Soviet-led Warsaw Pact.[311] The 
long period of political tensions and military competition between them, the 
Cold War, would be accompanied by an unprecedented arms race and proxy 
wars.[312]

In Asia, the United States led the occupation of Japan and administrated 
Japan's former islands in the Western Pacific, while the Soviets annexed 
Sakhalin and the Kuril Islands.[313] Korea, formerly under Japanese rule, was 
divided and occupied by the Soviet Union in the North and the United States in 
the South between 1945 and 1948. Separate republics emerged on both sides of 
the 38th parallel in 1948, each claiming to be the legitimate government for 
all of Korea, which led ultimately to the Korean War.[314]


David Ben-Gurion proclaiming the Israeli Declaration of Independence at the 
Independence Hall, 14 May 1948
In China, nationalist and communist forces resumed the civil war in June 1946. 
Communist forces were victorious and established the People's Republic of China 
on the mainland, while nationalist forces retreated to Taiwan in 1949.[315] In 
the Middle East, the Arab rejection of the United Nations Partition Plan for 
Palestine and the creation of Israel marked the escalation of the 
Arab–Israeli conflict. While European powers attempted to retain some or all 
of their colonial empires, their losses of prestige and resources during the 
war rendered this unsuccessful, leading to decolonisation.[316][317]

The global economy suffered heavily from the war, although participating 
nations were affected differently. The United States emerged much richer than 
any other nation, leading to a baby boom, and by 1950 its gross domestic 
product per person was much higher than that of any of the other powers, and it 
dominated the world economy.[318] The UK and US pursued a policy of industrial 
disarmament in Western Germany in the years 1945–1948.[319] Because of 
international trade interdependencies this led to European economic stagnation 
and delayed European recovery for several years.[320][321]

Recovery began with the mid-1948 currency reform in Western Germany, and was 
sped up by the liberalisation of European economic policy that the Marshall 
Plan (1948–1951) both directly and indirectly caused.[322][323] The post-1948 
West German recovery has been called the German economic miracle.[324] Italy 
also experienced an economic boom[325] and the French economy rebounded.[326] 
By contrast, the United Kingdom was in a state of economic ruin,[327] and 
although receiving a quarter of the total Marshall Plan assistance, more than 
any other European country,[328] it continued in relative economic decline for 
decades.[329]

The Soviet Union, despite enormous human and material losses, also experienced 
rapid increase in production in the immediate post-war era.[330] Japan 
experienced incredibly rapid economic growth, becoming one of the most powerful 
economies in the world by the 1980s.[331] China returned to its pre-war 
industrial production by 1952.[332]

Impact
Main article: Historiography of World War II
Casualties and war crimes
Main articles: World War II casualties and List of war crimes § 1939–1945: 
World War II

World War II deaths
Estimates for the total number of casualties in the war vary, because many 
deaths went unrecorded.[333] Most suggest that some 60 million people died in 
the war, including about 20 million military personnel and 40 million 
civilians.[334][335][336] Many of the civilians died because of deliberate 
genocide, massacres, mass bombings, disease, and starvation.[citation needed]

The Soviet Union alone lost around 27 million people during the war,[337] 
including 8.7 million military and 19 million civilian deaths.[338] A quarter 
of the people in the Soviet Union were wounded or killed.[339] Germany 
sustained 5.3 million military losses, mostly on the Eastern Front and during 
the final battles in Germany.[340]

An estimated 11[341] to 17 million[342] civilians died as a direct or as an 
indirect result of Nazi racist policies, including mass killing of around 6 
million Jews, along with Roma, homosexuals, at least 1.9 million ethnic 
Poles[343][344] and millions of other Slavs (including Russians, Ukrainians and 
Belarusians), and other ethnic and minority groups.[345][342] Between 1941 and 
1945, more than 200,000 ethnic Serbs, along with gypsies and Jews, were 
persecuted and murdered by the Axis-aligned Croatian Ustaše in 
Yugoslavia.[346] Also, more than 100,000 Poles were massacred by the Ukrainian 
Insurgent Army in the Volhynia massacres, between 1943 and 1945.[347] At the 
same time about 10,000–15,000 Ukrainians were killed by the Polish Home Army 
and other Polish units, in reprisal attacks.[348]


Chinese civilians being buried alive by soldiers of the Imperial Japanese Army, 
during the Nanking Massacre, December 1937
In Asia and the Pacific, between 3 million and more than 10 million civilians, 
mostly Chinese (estimated at 7.5 million[349]), were killed by the Japanese 
occupation forces.[350] The most infamous Japanese atrocity was the Nanking 
Massacre, in which fifty to three hundred thousand Chinese civilians were raped 
and murdered.[351] Mitsuyoshi Himeta reported that 2.7 million casualties 
occurred during the Sankō Sakusen. General Yasuji Okamura implemented the 
policy in Heipei and Shantung.[352]

Axis forces employed biological and chemical weapons. The Imperial Japanese 
Army used a variety of such weapons during its invasion and occupation of China 
(see Unit 731)[353][354] and in early conflicts against the Soviets.[355] Both 
the Germans and the Japanese tested such weapons against civilians,[356] and 
sometimes on prisoners of war.[357]

The Soviet Union was responsible for the Katyn massacre of 22,000 Polish 
officers,[358] and the imprisonment or execution of thousands of political 
prisoners by the NKVD, along with mass civilian deportations to Siberia, in the 
Baltic states and eastern Poland annexed by the Red Army.[359]

The mass bombing of cities in Europe and Asia has often been called a war 
crime, although no positive or specific customary international humanitarian 
law with respect to aerial warfare existed before or during World War II.[360] 
The USAAF firebombed a total of 67 Japanese cities, killing 393,000 civilians 
and destroying 65% of built-up areas.[361]

Genocide, concentration camps, and slave labour
Main articles: Genocide, The Holocaust, Nazi concentration camps, Extermination 
camp, Forced labour under German rule during World War II, Kidnapping of 
children by Nazi Germany, and Nazi human experimentation

Schutzstaffel (SS) female camp guards removing prisoners' bodies from lorries 
and carrying them to a mass grave, inside the German Bergen-Belsen 
concentration camp, 1945
Nazi Germany was responsible for the Holocaust (killing approximately 6 million 
Jews), as well as for killing 2.7 million ethnic Poles[362] and 4 million 
others who were deemed "unworthy of life" (including the disabled and mentally 
ill, Soviet prisoners of war, Romani, homosexuals, Freemasons, and Jehovah's 
Witnesses) as part of a programme of deliberate extermination, in effect 
becoming a "genocidal state".[363] Soviet POWs were kept in especially 
unbearable conditions, and 3.6 million Soviet POWs out of 5.7 died in Nazi 
camps during the war.[364][365] In addition to concentration camps, death camps 
were created in Nazi Germany to exterminate people at an industrial scale. Nazi 
Germany extensively used forced labourers; about 12 million Europeans from 
German occupied countries were abducted and used as a slave work force in 
German industry, agriculture and war economy.[366]

The Soviet Gulag became a de facto system of deadly camps during 1942–43, 
when wartime privation and hunger caused numerous deaths of inmates,[367] 
including foreign citizens of Poland and other countries occupied in 1939–40 
by the Soviet Union, as well as Axis POWs.[368] By the end of the war, most 
Soviet POWs liberated from Nazi camps and many repatriated civilians were 
detained in special filtration camps where they were subjected to NKVD 
evaluation, and 226,127 were sent to the Gulag as real or perceived Nazi 
collaborators.[369]


Prisoner identity photograph taken by the German SS of a Polish girl deported 
to Auschwitz. Approximately 230,000 children were held prisoner, and used in 
forced labour and medical experiments.
Japanese prisoner-of-war camps, many of which were used as labour camps, also 
had high death rates. The International Military Tribunal for the Far East 
found the death rate of Western prisoners was 27.1 per cent (for American POWs, 
37 per cent),[370] seven times that of POWs under the Germans and 
Italians.[371] While 37,583 prisoners from the UK, 28,500 from the Netherlands, 
and 14,473 from the United States were released after the surrender of Japan, 
the number of Chinese released was only 56.[372]

At least five million Chinese civilians from northern China and Manchukuo were 
enslaved between 1935 and 1941 by the East Asia Development Board, or Kōain, 
for work in mines and war industries. After 1942, the number reached 10 
million.[373] In Java, between 4 and 10 million rōmusha (Japanese: "manual 
labourers"), were forced to work by the Japanese military. About 270,000 of 
these Javanese labourers were sent to other Japanese-held areas in South East 
Asia, and only 52,000 were repatriated to Java.[374]

Occupation
Main articles: German-occupied Europe, Resistance during World War II, 
Collaboration with the Axis Powers, and Nazi plunder

Polish civilians wearing blindfolds photographed just before their execution by 
German soldiers in Palmiry forest, 1940
In Europe, occupation came under two forms. In Western, Northern, and Central 
Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of 
Czechoslovakia) Germany established economic policies through which it 
collected roughly 69.5 billion reichmarks (27.8 billion US dollars) by the end 
of the war; this figure does not include the sizeable plunder of industrial 
products, military equipment, raw materials and other goods.[375] Thus, the 
income from occupied nations was over 40 per cent of the income Germany 
collected from taxation, a figure which increased to nearly 40 per cent of 
total German income as the war went on.[376]


Soviet partisans hanged by the German army. The Russian Academy of Sciences 
reported in 1995 civilian victims in the Soviet Union at German hands totalled 
13.7 million dead, twenty percent of the 68 million persons in the occupied 
Soviet Union.
In the East, the intended gains of Lebensraum were never attained as 
fluctuating front-lines and Soviet scorched earth policies denied resources to 
the German invaders.[377] Unlike in the West, the Nazi racial policy encouraged 
extreme brutality against what it considered to be the "inferior people" of 
Slavic descent; most German advances were thus followed by mass 
executions.[378] Although resistance groups formed in most occupied 
territories, they did not significantly hamper German operations in either the 
East[379] or the West[380] until late 1943.

In Asia, Japan termed nations under its occupation as being part of the Greater 
East Asia Co-Prosperity Sphere, essentially a Japanese hegemony which it 
claimed was for purposes of liberating colonised peoples.[381] Although 
Japanese forces were originally welcomed as liberators from European domination 
in some territories, their excessive brutality turned local public opinion 
against them within weeks.[382] During Japan's initial conquest it captured 
4,000,000 barrels (640,000 m3) of oil (~5.5×105 tonnes) left behind by 
retreating Allied forces, and by 1943 was able to get production in the Dutch 
East Indies up to 50 million barrels (~6.8×106 t), 76 per cent of its 1940 
output rate.[382]

Home fronts and production
Main articles: Military production during World War II and Home front during 
World War II

Allies to Axis GDP ratio between 1938 and 1945.
In Europe, before the outbreak of the war, the Allies had significant 
advantages in both population and economics. In 1938 the Western Allies (United 
Kingdom, France, Poland and the British Dominions) had a 30 per cent larger 
population and a 30 per cent higher gross domestic product than the European 
Axis powers (Germany and Italy); if colonies are included, the Allies had more 
than a 5:1 advantage in population and a nearly 2:1 advantage in GDP.[383] In 
Asia at the same time, China had roughly six times the population of Japan, but 
only an 89 per cent higher GDP; this is reduced to three times the population 
and only a 38 per cent higher GDP if Japanese colonies are included.[383]

The United States produced about two-thirds of all the munitions used by the 
Allies in WWII, including warships, transports, warplanes, artillery, tanks, 
trucks, and ammunition.[384] Though the Allies' economic and population 
advantages were largely mitigated during the initial rapid blitzkrieg attacks 
of Germany and Japan, they became the decisive factor by 1942, after the United 
States and Soviet Union joined the Allies, as the war largely settled into one 
of attrition.[385] While the Allies' ability to out-produce the Axis is often 
attributed[by whom?] to the Allies having more access to natural resources, 
other factors, such as Germany and Japan's reluctance to employ women in the 
labour force,[386] Allied strategic bombing,[387] and Germany's late shift to a 
war economy[388] contributed significantly. Additionally, neither Germany nor 
Japan planned to fight a protracted war, and had not equipped themselves to do 
so.[389] To improve their production, Germany and Japan used millions of slave 
labourers;[390] Germany used about 12 million people, mostly from Eastern 
Europe,[366] while Japan used more than 18 million people in Far East 
Asia.[373][374]

Advances in technology and warfare
Main article: Technology during World War II

B-29 Superfortress strategic bombers on the Boeing assembly line in Wichita, 
Kansas, 1944
Aircraft were used for reconnaissance, as fighters, bombers, and 
ground-support, and each role was advanced considerably. Innovation included 
airlift (the capability to quickly move limited high-priority supplies, 
equipment, and personnel);[391] and of strategic bombing (the bombing of enemy 
industrial and population centres to destroy the enemy's ability to wage 
war).[392] Anti-aircraft weaponry also advanced, including defences such as 
radar and surface-to-air artillery. The use of the jet aircraft was pioneered 
and, though late introduction meant it had little impact, it led to jets 
becoming standard in air forces worldwide.[393] Although guided missiles were 
being developed, they were not advanced enough to reliably target aircraft 
until some years after the war.

Advances were made in nearly every aspect of naval warfare, most notably with 
aircraft carriers and submarines. Although aeronautical warfare had relatively 
little success at the start of the war, actions at Taranto, Pearl Harbor, and 
the Coral Sea established the carrier as the dominant capital ship in place of 
the battleship.[394][395][396] In the Atlantic, escort carriers proved to be a 
vital part of Allied convoys, increasing the effective protection radius and 
helping to close the Mid-Atlantic gap.[397] Carriers were also more economical 
than battleships because of the relatively low cost of aircraft[398] and their 
not requiring to be as heavily armoured.[399] Submarines, which had proved to 
be an effective weapon during the First World War,[400] were anticipated by all 
sides to be important in the second. The British focused development on 
anti-submarine weaponry and tactics, such as sonar and convoys, while Germany 
focused on improving its offensive capability, with designs such as the Type 
VII submarine and wolfpack tactics.[401] Gradually, improving Allied 
technologies such as the Leigh light, hedgehog, squid, and homing torpedoes 
proved victorious over the German submarines.[citation needed]


A V-2 rocket launched from a fixed site in Peenemünde, 21 June 1943
Land warfare changed from the static front lines of trench warfare of World War 
I, which had relied on improved artillery that outmatched the speed of both 
infantry and cavalry, to increased mobility and combined arms. The tank, which 
had been used predominantly for infantry support in the First World War, had 
evolved into the primary weapon.[402] In the late 1930s, tank design was 
considerably more advanced than it had been during World War I,[403] and 
advances continued throughout the war with increases in speed, armour and 
firepower.[citation needed] At the start of the war, most commanders thought 
enemy tanks should be met by tanks with superior specifications.[404] This idea 
was challenged by the poor performance of the relatively light early tank guns 
against armour, and German doctrine of avoiding tank-versus-tank combat. This, 
along with Germany's use of combined arms, were among the key elements of their 
highly successful blitzkrieg tactics across Poland and France.[402] Many means 
of destroying tanks, including indirect artillery, anti-tank guns (both towed 
and self-propelled), mines, short-ranged infantry antitank weapons, and other 
tanks were used.[404] Even with large-scale mechanisation, infantry remained 
the backbone of all forces,[405] and throughout the war, most infantry were 
equipped similarly to World War I.[406] The portable machine gun spread, a 
notable example being the German MG34, and various submachine guns which were 
suited to close combat in urban and jungle settings.[406] The assault rifle, a 
late war development incorporating many features of the rifle and submachine 
gun, became the standard postwar infantry weapon for most armed forces.[407]


Nuclear Gadget being raised to the top of the detonation "shot tower", at 
Alamogordo Bombing Range; Trinity nuclear test, New Mexico, July 1945
Most major belligerents attempted to solve the problems of complexity and 
security involved in using large codebooks for cryptography by designing 
ciphering machines, the most well known being the German Enigma machine.[408] 
Development of SIGINT (signals intelligence) and cryptanalysis enabled the 
countering process of decryption. Notable examples were the Allied decryption 
of Japanese naval codes[409] and British Ultra, a pioneering method for 
decoding Enigma benefiting from information given to the United Kingdom by the 
Polish Cipher Bureau, which had been decoding early versions of Enigma before 
the war.[410] Another aspect of military intelligence was the use of deception, 
which the Allies used to great effect, such as in operations Mincemeat and 
Bodyguard.[409][411]

Other technological and engineering feats achieved during, or as a result of, 
the war include the world's first programmable computers (Z3, Colossus, and 
ENIAC), guided missiles and modern rockets, the Manhattan Project's development 
of nuclear weapons, operations research and the development of artificial 
harbours and oil pipelines under the English Channel.[citation needed] 
Penicillin was first mass-produced and used during the war (see Stabilization 
and mass production of penicillin).[412]

See also
	World War II portal
	War portal
Index of World War II articles
Lists of World War II topics
Outline of World War II
Notes
 While various other dates have been proposed as the date on which World War II 
began or ended, this is the time span most frequently cited.
 Although open hostilities existed between Japan and China from 1937, neither 
belligerent formally declared war on the other until after the attack on Pearl 
Harbor in 1941.[1]
Citations
 "China's Declaration of War Against Japan, Also Against Germany and Italy". 
Contemporary China. 1 (15). 15 December 1941. Archived from the original on 25 
March 2018. Retrieved 6 June 2019.
 Weinberg 2005, p. 6.
 Wells, Anne Sharp (2014) Historical Dictionary of World War II: The War 
against Germany and Italy. Rowman & Littlefield Publishing. p. 7.
 Ferris, John; Mawdsley, Evan (2015). The Cambridge History of the Second World 
War, Volume I: Fighting the War. Cambridge: Cambridge University Press.
 Förster & Gessler 2005, p. 64.
 Ghuhl, Wernar (2007) Imperial Japan's World War Two Transaction Publishers pp. 
7, 30
 Polmar, Norman; Thomas B. Allen (1991) World War II: America at war, 
1941–1945 ISBN 978-0-394-58530-7
 Ben-Horin 1943, p. 169; Taylor 1979, p. 124; Yisreelit, Hevrah Mizrahit 
(1965). Asian and African Studies, p. 191.
For 1941 see Taylor 1961, p. vii; Kellogg, William O (2003). American History 
the Easy Way. Barron's Educational Series. p. 236 ISBN 0-7641-1973-7.
There is also the viewpoint that both World War I and World War II are part of 
the same "European Civil War" or "Second Thirty Years War": Canfora 2006, p. 
155; Prins 2002, p. 11.
 Beevor 2012, p. 10.
 Masaya 1990, p. 4.
 "History of German-American Relations » 1989–1994 – Reunification » 
"Two-plus-Four-Treaty": Treaty on the Final Settlement with Respect to Germany, 
September 12, 1990". usa.usembassy.de. Archived from the original on 7 May 
2012. Retrieved 6 May 2012.
 Why Japan and Russia never signed a WWII peace treaty Archived 4 June 2018 at 
the Wayback Machine. Asia Times.
 Ingram 2006, pp. 76–78.
 Kantowicz 1999, p. 149.
 Shaw 2000, p. 35.
 Brody 1999, p. 4.
 Zalampas 1989, p. 62.
 Mandelbaum 1988, p. 96; Record 2005, p. 50.
 Schmitz 2000, p. 124.
 Adamthwaite 1992, p. 52.
 Shirer 1990, pp. 298–99.
 Preston 1998, p. 104.
 Myers & Peattie 1987, p. 458.
 Smith & Steadman 2004, p. 28.
 Coogan 1993: "Although some Chinese troops in the Northeast managed to retreat 
south, others were trapped by the advancing Japanese Army and were faced with 
the choice of resistance in defiance of orders, or surrender. A few commanders 
submitted, receiving high office in the puppet government, but others took up 
arms against the invader. The forces they commanded were the first of the 
volunteer armies."
 Busky 2002, p. 10.
 Andrea L. Stanton; Edward Ramsamy; Peter J. Seybolt (2012). Cultural Sociology 
of the Middle East, Asia, and Africa: An Encyclopedia. p. 308. ISBN 
978-1-4129-8176-7. Archived from the original on 18 August 2018. Retrieved 6 
April 2014.
 Barker 1971, pp. 131–32.
 Shirer 1990, p. 289.
 Kitson 2001, p. 231.
 Neulen 2000, p. 25.
 Payne 2008, p. 271.
 Payne 2008, p. 146.
 Eastman 1986, pp. 547–51.
 Hsu & Chang 1971, pp. 195–200.
 Tucker, Spencer C. (2009). A Global Chronology of Conflict: From the Ancient 
World to the Modern Middle East [6 volumes]: From the Ancient World to the 
Modern Middle East. ABC-CLIO. ISBN 978-1-85109-672-5. Archived from the 
original on 18 August 2018. Retrieved 27 August 2017 – via Google Books.
 Yang Kuisong, "On the reconstruction of the facts of the Battle of 
Pingxingguan"
 Levene, Mark and Roberts, Penny. The Massacre in History. 1999, pp. 223–24
 Totten, Samuel. Dictionary of Genocide. 2008, 298–99.
 Hsu & Chang 1971, pp. 221–30.
 Eastman 1986, p. 566.
 Taylor 2009, pp. 150–52.
 Sella 1983, pp. 651–87.
 Beevor 2012, p. 342.
 Goldman, Stuart D. (28 August 2012). "The Forgotten Soviet-Japanese War of 
1939". The Diplomat. Archived from the original on 29 June 2015. Retrieved 26 
June 2015.
 Timothy Neeno. "Nomonhan: The Second Russo-Japanese War". 
MilitaryHistoryOnline.com. Archived from the original on 24 November 2005. 
Retrieved 26 June 2015.
 Collier & Pedley 2000, p. 144.
 Kershaw 2001, pp. 121–22.
 Kershaw 2001, p. 157.
 Davies 2006, pp. 143–44 (2008 ed.).
 Shirer 1990, pp. 461–62.
 Lowe & Marzari 2002, p. 330.
 Dear & Foot 2001, p. 234.
 Shirer 1990, p. 471.
 Watson, Derek (2000). "Molotov's Apprenticeship in Foreign Policy: The Triple 
Alliance Negotiations in 1939". Europe-Asia Studies. 52 (4): 695–722. 
doi:10.1080/713663077. JSTOR 153322.
 Shore 2003, p. 108.
 Dear & Foot 2001, p. 608.
 "The German Campaign In Poland (1939)". Archived from the original on 24 May 
2014. Retrieved 29 October 2014.
 "The Danzig Crisis". ww2db.com. Archived from the original on 5 May 2016. 
Retrieved 29 April 2016.
 "Major international events of 1939, with explanation". Ibiblio.org. Archived 
from the original on 10 March 2013. Retrieved 9 May 2013.
 Evans 2008, pp. 1–2.
 David T. Zabecki (1 May 2015). World War II in Europe: An Encyclopedia. 
Routledge. p. 1663. ISBN 978-1-135-81242-3. The earliest fighting started at 
0445 hours when marines from the battleship Schleswig-Holstein attempted to 
storm a small Polish fort in Danzig, the Westerplate
 Keegan 1997, p. 35.
Cienciala 2010, p. 128, observes that, while it is true that Poland was far 
away, making it difficult for the French and British to provide support, "[f]ew 
Western historians of World War II ... know that the British had committed to 
bomb Germany if it attacked Poland, but did not do so except for one raid on 
the base of Wilhelmshafen. The French, who committed to attack Germany in the 
west, had no intention of doing so."
 Beevor 2012, p. 32; Dear & Foot 2001, pp. 248–49; Roskill 1954, p. 64.
 James Bjorkman, New Hope for Allied Shipping Archived 18 December 2018 at the 
Wayback Machine, Retrieved 17 December 2018.
 Zaloga 2002, pp. 80, 83.
 Ginsburgs, George (1958). "A Case Study in the Soviet Use of International 
Law: Eastern Poland in 1939". The American Journal of International Law. 52 
(1): 69–84. doi:10.2307/2195670. JSTOR 2195670.
 Hempel 2005, p. 24.
 Zaloga 2002, pp. 88–89.
 Nuremberg Documents C-62/GB86, a directive from Hitler in October 1939 which 
concludes: "The attack [on France] is to be launched this Autumn if conditions 
are at all possible."
 Liddell Hart 1977, pp. 39–40.
 Bullock 1990, pp. 563–64, 566, 568–69, 574–75 (1983 ed.).
 Blitzkrieg: From the Rise of Hitler to the Fall of Dunkirk, L Deighton, 
Jonathan Cape, 1993, pp. 186–87. Deighton states that "the offensive was 
postponed twenty-nine times before it finally took place."
 Smith et al. 2002, p. 24.
 Bilinsky 1999, p. 9.
 Murray & Millett 2001, pp. 55–56.
 Spring 1986, pp. 207–26.
 Carl van Dyke. The Soviet Invasion of Finland. Frank Cass Publishers, 
Portland, OR. ISBN 0-7146-4753-5, p. 71.
 Hanhimäki 1997, p. 12.
 Ferguson 2006, pp. 367, 376, 379, 417.
 Snyder 2010, p. 118ff.
 Koch 1983, pp. 912–14, 917–20.
 Roberts 2006, p. 56.
 Roberts 2006, p. 59.
 Murray & Millett 2001, pp. 57–63.
 Commager 2004, p. 9.
 Reynolds 2006, p. 76.
 Evans 2008, pp. 122–23.
 Keegan 1997, pp. 59–60.
 Regan 2004, p. 152.
 Liddell Hart 1977, p. 48.
 Keegan 1997, pp. 66–67.
 Overy & Wheatcroft 1999, p. 207.
 Umbreit 1991, p. 311.
 Brown 2004, p. 198.
 Keegan 1997, p. 72.
 Murray 1983, The Battle of Britain.
 "Major international events of 1940, with explanation". Ibiblio.org. Archived 
from the original on 25 May 2013.
 Dear & Foot 2001, pp. 108–09.
 Goldstein 2004, p. 35
 Steury 1987, p. 209; Zetterling & Tamelander 2009, p. 282.
 Overy & Wheatcroft 1999, pp. 328–30.
 Maingot 1994, p. 52.
 Cantril 1940, p. 390.
 Skinner Watson, Mark. "Coordination With Britain". US Army in WWII – Chief 
of Staff: Prewar Plans and Operations. Archived from the original on 30 April 
2013. Retrieved 13 May 2013.
 Bilhartz & Elliott 2007, p. 179.
 Dear & Foot 2001, p. 877.
 Dear & Foot 2001, pp. 745–46.
 Clogg 2002, p. 118.
 Evans 2008, pp. 146, 152; US Army 1986, pp. 4–6
 Jowett 2001, pp. 9–10.
 Jackson 2006, p. 106.
 Laurier 2001, pp. 7–8.
 Murray & Millett 2001, pp. 263–76.
 Gilbert 1989, pp. 174–75.
 Gilbert 1989, pp. 184–87.
 Gilbert 1989, pp. 208, 575, 604.
 Watson 2003, p. 80.
 Morrisey, Will (24 January 2019), "What Churchill and De Gaulle learned from 
the Great War", Winston Churchill, Routledge, pp. 119–126, 
doi:10.4324/9780429027642-6, ISBN 9780429027642
 Garver 1988, p. 114.
 Weinberg 2005, p. 195.
 Murray 1983, p. 69.
 Shirer 1990, pp. 810–12.
 Klooz, Marle; Wiley, Evelyn (1944), Events leading up to World War II – 
Chronological History, 78th Congress, 2d Session – House Document N. 541, 
Director: Humphrey, Richard A., Washington: US Government Printing Office, pp. 
267–312 (1941), archived from the original on 14 December 2013, retrieved 9 
May 2013.
 Sella 1978.
 Kershaw 2007, pp. 66–69.
 Steinberg 1995.
 Hauner 1978.
 Roberts 1995.
 Wilt 1981.
 Erickson 2003, pp. 114–37.
 Glantz 2001, p. 9.
 Farrell 1993.
 Keeble 1990, p. 29.
 Bueno de Mesquita et al. 2003, p. 425.
 Beevor 2012, p. 220.
 Kleinfeld 1983.
 Jukes 2001, p. 113.
 Glantz 2001, p. 26: "By 1 November [the Wehrmacht] had lost fully 20% of its 
committed strength (686,000 men), up to 2/3 of its ½-million motor vehicles, 
and 65 percent of its tanks. The German Army High Command (OKH) rated its 136 
divisions as equivalent to 83 full-strength divisions."
 Reinhardt 1992, p. 227.
 Milward 1964.
 Rotundo 1986.
 Glantz 2001, p. 26.
 Deighton, Len (1993). Blood, Tears and Folly. London: Pimlico. p. 479. ISBN 
978-0-7126-6226-0.
 Beevor 1998, pp. 41–42; Evans 2008, pp. 213–14, notes that "Zhukov had 
pushed the Germans back where they had launched Operation Typhoon two months 
before. ... Only Stalin's decision to attack all along the front instead of 
concentrating his forces in an all-out assault against the retreating German 
Army Group Centre prevented the disaster from being even worse."
 Jowett & Andrew 2002, p. 14.
 Overy & Wheatcroft 1999, p. 289.
 Morison 2002, p. 60.
 Joes 2004, p. 224.
 Fairbank & Goldman 2006, p. 320.
 Hsu & Chang 1971, p. 30.
 Hsu & Chang 1971, p. 33.
 "Japanese Policy and Strategy 1931 – July 1941". US Army in WWII – 
Strategy and Command: The First Two Years. pp. 45–66. Archived from the 
original on 6 January 2013. Retrieved 15 May 2013.
 Anderson 1975, p. 201.
 Evans & Peattie 2012, p. 456.
 Coox, Alvin (1985). Nomonhan: Japan against Russia, 1939. Stanford, CA: 
Stanford University Press. pp. 1046–49. ISBN 978-0-8047-1835-6.
 "The decision for War". US Army in WWII – Strategy and Command: The First 
Two Years. pp. 113–27. Archived from the original on 25 May 2013. Retrieved 
15 May 2013.
 "The Showdown With Japan Aug–Dec 1941". US Army in WWII – Strategic 
Planning for Coalition Warfare. pp. 63–96. Archived from the original on 9 
November 2012. Retrieved 15 May 2013.
 The United States Replies Archived 29 April 2013 at the Wayback Machine. 
Investigation of the Pearl Harbor attack.
 Painter 2012, p. 26: "The United States cut off oil exports to Japan in the 
summer of 1941, forcing Japanese leaders to choose between going to war to 
seize the oil fields of the Netherlands East Indies or giving in to U.S. 
pressure."
 Wood 2007, p. 9, listing various military and diplomatic developments, 
observes that "the threat to Japan was not purely economic."
 Lightbody 2004, p. 125.
 Weinberg 2005, p. 310
 Dower 1986, p. 5, calls attention to the fact that "the Allied struggle 
against Japan exposed the racist underpinnings of the European and American 
colonial structure. Japan did not invade independent countries in southern 
Asia. It invaded colonial outposts which the Westerners had dominated for 
generations, taking absolutely for granted their racial and cultural 
superiority over their Asian subjects." Dower goes on to note that, before the 
horrors of Japanese occupation made themselves felt, many Asians responded 
favourably to the victories of the Imperial Japanese forces.
 Wood 2007, pp. 11–12.
 Wohlstetter 1962, pp. 341–43.
 Keegan, John (1989) The Second World War. New York: Viking. pp.256-257. ISBN 
9780399504341
 Dunn 1998, p. 157. According to May 1955, p. 155, Churchill stated: "Russian 
declaration of war on Japan would be greatly to our advantage, provided, but 
only provided, that Russians are confident that will not impair their Western 
Front."
 Adolf Hitler's Declaration of War against the United States in Wikisource.
 Klooz, Marle; Wiley, Evelyn (1944), Events leading up to World War II – 
Chronological History, 78th Congress, 2d Session – House Document N. 541, 
Director: Humphrey, Richard A., Washington: US Government Printing Office, p. 
310 (1941), archived from the original on 14 December 2013, retrieved 9 May 
2013.
 Bosworth & Maiolo 2015, pp. 313–14.
 Mingst & Karns 2007, p. 22.
 Shirer 1990, p. 904.
 "The First Full Dress Debate over Strategic Deployment. Dec 1941 – Jan 
1942". US Army in WWII – Strategic Planning for Coalition Warfare. pp. 
97–119. Archived from the original on 9 November 2012. Retrieved 16 May 2013.
 "The Elimination of the Alternatives. Jul–Aug 1942". US Army in WWII – 
Strategic Planning for Coalition Warfare. pp. 266–92. Archived from the 
original on 30 April 2013. Retrieved 16 May 2013.
 "Casablanca – Beginning of an Era: January 1943". US Army in WWII – 
Strategic Planning for Coalition Warfare. pp. 18–42. Archived from the 
original on 25 May 2013. Retrieved 16 May 2013.
 "The Trident Conference – New Patterns: May 1943". US Army in WWII – 
Strategic Planning for Coalition Warfare. pp. 126–45. Archived from the 
original on 25 May 2013. Retrieved 16 May 2013.
 Beevor 2012, pp. 247–67, 345.
 Lewis 1953, p. 529 (Table 11).
 Slim 1956, pp. 71–74.
 Grove 1995, p. 362.
 Ch'i 1992, p. 158.
 Perez 1998, p. 145.
 Maddox 1992, pp. 111–12.
 Salecker 2001, p. 186.
 Schoppa 2011, p. 28.
 Chevrier & Chomiczewski & Garrigue 2004 Archived 18 August 2018 at the Wayback 
Machine, p. 19.
 Ropp 2000, p. 368.
 Weinberg 2005, p. 339.
 Gilbert, Adrian (2003). The Encyclopedia of Warfare: From Earliest Times to 
the Present Day. Globe Pequot. p. 259. ISBN 978-1-59228-027-8. Archived from 
the original on 19 July 2019. Retrieved 26 June 2019.
 Swain 2001, p. 197.
 Hane 2001, p. 340.
 Marston 2005, p. 111.
 Brayley 2002, p. 9.
 Glantz 2001, p. 31.
 Read 2004, p. 764.
 Davies 2006, p. 100 (2008 ed.).
 Beevor 1998, pp. 239–65.
 Black 2003, p. 119.
 Beevor 1998, pp. 383–91.
 Erickson 2001, p. 142.
 Milner 1990, p. 52.
 Beevor 2012, pp. 224–28.
 Molinari 2007, p. 91.
 Mitcham 2007, p. 31.
 Beevor 2012, pp. 380–81.
 Rich 1992, p. 178.
 Gordon 2004, p. 129.
 Neillands 2005.
 Keegan 1997, p. 277.
 Smith 2002.
 Thomas & Andrew 1998, p. 8.
 Ross 1997, p. 38.
 Bonner & Bonner 2001, p. 24.
 Collier 2003, p. 11.
 "The Civilians" Archived 5 November 2013 at the Wayback Machine United States 
Strategic Bombing Survey Summary Report (European War)
 Overy 1995, pp. 119–20.
 Thompson & Randall 2008, p. 164.
 Kennedy 2001, p. 610.
 Rottman 2002, p. 228.
 Glantz 1986; Glantz 1989, pp. 149–59.
 Kershaw 2001, p. 592.
 O'Reilly 2001, p. 32.
 Bellamy 2007, p. 595.
 O'Reilly 2001, p. 35.
 Healy 1992, p. 90.
 Glantz 2001, pp. 50–55.
 Kolko 1990, p. 45
 Mazower 2008, p. 362.
 Hart, Hart & Hughes 2000, p. 151.
 Blinkhorn 2006, p. 52.
 Read & Fisher 2002, p. 129.
 Padfield 1998, pp. 335–36.
 Kolko 1990, pp. 211, 235, 267–68.
 Iriye 1981, p. 154.
 Mitter 2014, p. 286.
 Polley 2000, p. 148.
 Beevor 2012, pp. 268–74.
 Ch'i 1992, p. 161.
 Hsu & Chang 1971, pp. 412–16, Map 38
 Weinberg 2005, pp. 660–61.
 Glantz 2002, pp. 327–66.
 Glantz 2002, pp. 367–414.
 Chubarov 2001, p. 122.
 Holland 2008, pp. 169–84; Beevor 2012, pp. 568–73.
The weeks after the fall of Rome saw a dramatic upswing in German atrocities in 
Italy (Mazower 2008, pp. 500–02). The period featured massacres with victims 
in the hundreds at Civitella (de Grazia & Paggi 1991; Belco 2010), Fosse 
Ardeatine (Portelli 2003), and Sant'Anna di Stazzema (Gordon 2012, pp. 
10–11), and is capped with the Marzabotto massacre.
 Lightbody 2004, p. 224.
 Zeiler 2004, p. 60.
 Beevor 2012, pp. 555–60.
 Ch'i 1992, p. 163.
 Coble 2003, p. 85.
 Rees 2008, pp. 406–07: "Stalin always believed that Britain and America were 
delaying the second front so that the Soviet Union would bear the brunt of the 
war."
 Weinberg 2005, p. 695.
 Badsey 1990, p. 91.
 Dear & Foot 2001, p. 562.
 Forrest, Evans & Gibbons 2012, p. 191
 Zaloga 1996, p. 7: "It was the most calamitous defeat of all the German armed 
forces in World War II."
 Berend 1996, p. 8.
 "Armistice Negotiations and Soviet Occupation". US Library of Congress. 
Archived from the original on 30 April 2011. Retrieved 14 November 2009. The 
coup speeded the Red Army's advance, and the Soviet Union later awarded Michael 
the Order of Victory for his personal courage in overthrowing Antonescu and 
putting an end to Romania's war against the Allies. Western historians 
uniformly point out that the Communists played only a supporting role in the 
coup; postwar Romanian historians, however, ascribe to the Communists the 
decisive role in Antonescu's overthrow
 Evans 2008, p. 653.
 Wiest & Barbier 2002, pp. 65–66.
 Wiktor, Christian L (1998). Multilateral Treaty Calendar – 1648–1995. 
Kluwer Law International. p. 426. ISBN 978-90-411-0584-4.
 Schire 1990, p. 1085.
 Marston 2005, p. 120.
 Article about War of Resistance "Archived copy". Archived from the original on 
3 March 2016. Retrieved 16 March 2013.
 Jowett & Andrew 2002, p. 8.
 Howard 2004, p. 140.
 Drea 2003, p. 54.
 Cook & Bewes 1997, p. 305.
 Parker 2004, pp. xiii–xiv, 6–8, 68–70, 329–30
 Glantz 2001, p. 85.
 Beevor 2012, pp. 709–22.
 Buchanan 2006, p. 21.
 Shepardson 1998.
 O'Reilly 2001, p. 244.
 Kershaw 2001, p. 823.
 Evans 2008, p. 737.
 Glantz 1998, p. 24.
 Chant, Christopher (1986). The Encyclopedia of Codenames of World War II. 
Routledge & Kegan Paul. p. 118. ISBN 978-0-7102-0718-0.
 Long, Tony (9 March 2011). "March 9, 1945: Burning the Heart Out of the 
Enemy". Wired. Wired Magazine. Archived from the original on 23 March 2017. 
Retrieved 22 June 2018. 1945: In the single deadliest air raid of World War II, 
330 American B-29s rain incendiary bombs on Tokyo, touching off a firestorm 
that kills upwards of 100,000 people, burns a quarter of the city to the 
ground, and leaves a million homeless.
 Drea 2003, p. 57.
 Jowett & Andrew 2002, p. 6.
 Poirier, Michel Thomas (20 October 1999). "Results of the German and American 
Submarine Campaigns of World War II". U.S. Navy. Archived from the original on 
9 April 2008. Retrieved 13 April 2008.
 Williams 2006, p. 90.
 Miscamble 2007, p. 201.
 Miscamble 2007, pp. 203–04.
 Ward Wilson. "The Winning Weapon? Rethinking Nuclear Weapons in Light of 
Hiroshima". International Security, Vol. 31, No. 4 (Spring 2007), pp. 162–79.
 Glantz 2005.
 Pape 1993 " The principal cause of Japan's surrender was the ability of the 
United States to increase the military vulnerability of Japan's home islands, 
persuading Japanese leaders that defense of the homeland was highly unlikely to 
succeed. The key military factor causing this effect was the sea blockade, 
which crippled Japan's ability to produce and equip the forces necessary to 
execute its strategy. The most important factor accounting for the timing of 
surrender was the Soviet attack against Manchuria, largely because it persuaded 
previously adamant Army leaders that the homeland could not be defended.".
 Beevor 2012, p. 776.
 Frei 2002, pp. 41–66.
 Eberhardt, Piotr (2015). "The Oder-Neisse Line as Poland's western border: As 
postulated and made a reality". Geographia Polonica. 88 (1): 77–105. 
doi:10.7163/GPol.0007. Archived from the original on 3 May 2018. Retrieved 3 
May 2018.
 Eberhardt, Piotr (2006). Political Migrations in Poland 1939–1948 (PDF). 
Warsaw: Didactica. ISBN 978-1-5361-1035-7. Archived from the original (PDF) on 
26 June 2015.
 Eberhardt, Piotr (2011). Political Migrations On Polish Territories 
(1939-1950) (PDF). Warsaw: Polish Academy of Sciences. ISBN 978-83-61590-46-0. 
Archived (PDF) from the original on 20 May 2014. Retrieved 3 May 2018.
 Eberhardt, Piotr (2012). "The Curzon line as the eastern boundary of Poland. 
The origins and the political background". Geographia Polonica. 85 (1): 5–21. 
doi:10.7163/GPol.2012.1.1. Archived from the original on 3 May 2018. Retrieved 
3 May 2018.
 Roberts 2006, p. 43.
 Roberts 2006, p. 55.
 Shirer 1990, p. 794.
 Kennedy-Pipe 1995.
 Wettig 2008, pp. 20–21.
 Senn 2007, p. ?.
 Yoder 1997, p. 39.
 "History of the UN". United Nations. Archived from the original on 18 February 
2010. Retrieved 25 January 2010.
 Waltz 2002.
The UDHR is viewable here [1] Archived 3 July 2017 at the Wayback Machine.
 The UN Security Council, archived from the original on 20 June 2012, retrieved 
15 May 2012
 Kantowicz 2000, p. 6.
 Wettig 2008, pp. 96–100.
 Trachtenberg 1999, p. 33.
 Applebaum 2012.
 Naimark 2010.
 Swain 1992.
 Borstelmann 2005, p. 318.
 Leffler & Westad 2010.
 Weinberg 2005, p. 911.
 Stueck 2010, p. 71.
 Lynch 2010, pp. 12–13.
 Roberts 1997, p. 589.
 Darwin 2007, pp. 441–43, 464–68.
 Dear & Foot 2001, p. 1006; Harrison 1998, pp. 34–55.
 Balabkins 1964, p. 207.
 Petrov 1967, p. 263.
 Balabkins 1964, pp. 208, 209.
 DeLong & Eichengreen 1993, pp. 190, 191
 Balabkins 1964, p. 212.
 Wolf 1993, pp. 29, 30, 32
 Bull & Newell 2005, pp. 20, 21
 Ritchie 1992, p. 23.
 Minford 1993, p. 117.
 Schain 2001.
 Emadi-Coffin 2002, p. 64.
 Smith 1993, p. 32.
 Neary 1992, p. 49.
 Genzberger, Christine (1994). China Business: The Portable Encyclopedia for 
Doing Business with China. Petaluma, CA: World Trade Press. p. 4. ISBN 
978-0-9631864-3-0.
 Quick Reference Handbook Set, Basic Knowledge and Modern Technology (revised) 
by Edward H. Litchfield, Ph.D 1984 page 195
 O'Brien, Prof. Joseph V. "World War II: Combatants and Casualties 
(1937–1945)". Obee's History Page. John Jay College of Criminal Justice. 
Archived from the original on 25 December 2010. Retrieved 28 December 2013.
 White, Matthew. "Source List and Detailed Death Tolls for the Twentieth 
Century Hemoclysm". Historical Atlas of the Twentieth Century. Matthew White's 
Homepage. Archived from the original on 7 March 2011. Retrieved 20 April 2007.
 "World War II Fatalities". secondworldwar.co.uk. Archived from the original on 
22 September 2008. Retrieved 20 April 2007.
 Hosking 2006, p. 242
 Ellman & Maksudov 1994.
 Smith 1994, p. 204.
 Herf 2003.
 Florida Center for Instructional Technology (2005). "Victims". A Teacher's 
Guide to the Holocaust. University of South Florida. Archived from the original 
on 16 May 2016. Retrieved 2 February 2008.
 Niewyk & Nicosia 2000, pp. 45–52.
 Snyder, Timothy (16 July 2009). "Holocaust: The Ignored Reality". The New York 
Review of Books. Archived from the original on 10 October 2017. Retrieved 27 
August 2017.
 "Polish Victims". www.ushmm.org. Archived from the original on 7 May 2016. 
Retrieved 27 August 2017.
 "Non-Jewish Holocaust Victims : The 5,000,000 others". BBC. April 2006. 
Archived from the original on 3 March 2013. Retrieved 4 August 2013.
 Evans 2008, pp. 158–60, 234–36.
 Massacre, Volhynia. "The Effects of the Volhynian Massacres". Volhynia 
Massacre. Archived from the original on 21 June 2018. Retrieved 9 July 2018.
 "Od rzezi wołyńskiej do akcji Wisła. Konflikt polsko-ukraiński 
1943–1947". dzieje.pl (in Polish). Archived from the original on 24 June 
2018. Retrieved 10 March 2018.
 Dear & Foot 2001, p. 290.
 Rummell, R.J. "Statistics". Freedom, Democide, War. The University of Hawaii 
System. Archived from the original on 23 March 2010. Retrieved 25 January 2010.
 Chang 1997, p. 102.
 Bix 2000, p. ?.
 Gold, Hal (1996). Unit 731 testimony. Tuttle. pp. 75–77. ISBN 
978-0-8048-3565-7.
 Tucker & Roberts 2004, p. 320.
 Harris 2002, p. 74.
 Lee 2002, p. 69.
 "Japan tested chemical weapons on Aussie POW: new evidence". The Japan Times 
Online. 27 July 2004. Archived from the original on 29 May 2012. Retrieved 25 
January 2010.
 Kużniar-Plota, Małgorzata (30 November 2004). "Decision to commence 
investigation into Katyn Massacre". Departmental Commission for the Prosecution 
of Crimes against the Polish Nation. Retrieved 4 August 2011.
 Robert Gellately (2007). Lenin, Stalin, and Hitler: The Age of Social 
Catastrophe. Knopf, ISBN 1-4000-4005-1 p. 391
 Terror from the Sky: The Bombing of German Cities in World War II. Berghahn 
Books. 2010. p. 167. ISBN 978-1-84545-844-7.
 John Dower (2007). "Lessons from Iwo Jima". Perspectives. 45 (6): 54–56. 
Archived from the original on 17 January 2011. Retrieved 12 January 2014.
 Institute of National Remembrance, Polska 1939–1945 Straty osobowe i ofiary 
represji pod dwiema okupacjami. Materski and Szarota. page 9 "Total Polish 
population losses under German occupation are currently calculated at about 2 
770 000".
 (2006). The World Must Know: The History of the Holocaust as Told in the 
United States Holocaust Memorial Museum (2nd ed.). Washington, DC: United 
States Holocaust Memorial Museum. ISBN 978-0-8018-8358-3.
 Herbert 1994, p. 222
 Overy 2004, pp. 568–69.
 Marek, Michael (27 October 2005). "Final Compensation Pending for Former Nazi 
Forced Laborers". dw-world.de. Deutsche Welle. Archived from the original on 2 
May 2006. Retrieved 19 January 2010.
 J. Arch Getty, Gábor T. Rittersporn and Viktor N. Zemskov. Victims of the 
Soviet Penal System in the Pre-War Years: A First Approach on the Basisof 
Archival Evidence. The American Historical Review, Vol. 98, No. 4 (Oct. 1993), 
pp. 1017–49
 Applebaum 2003, pp. 389–96.
 Zemskov V.N. On repatriation of Soviet citizens. Istoriya SSSR., 1990, No.4, 
(in Russian). See also [2] Archived 14 October 2011 at the Wayback Machine 
(online version), and Bacon 1992; Ellman 2002.
 "Japanese Atrocities in the Philippines". American Experience: the Bataan 
Rescue. PBS Online. Archived from the original on 27 July 2003. Retrieved 18 
January 2010.
 Tanaka 1996, pp. 2–3.
 Bix 2000, p. 360.
 Ju, Zhifen (June 2002). "Japan's atrocities of conscripting and abusing north 
China draughtees after the outbreak of the Pacific war". Joint Study of the 
Sino-Japanese War: Minutes of the June 2002 Conference. Harvard University 
Faculty of Arts and Sciences. Archived from the original on 21 May 2012. 
Retrieved 28 December 2013.
 "Indonesia: World War II and the Struggle For Independence, 1942–50; The 
Japanese Occupation, 1942–45". Library of Congress. 1992. Archived from the 
original on 30 October 2004. Retrieved 9 February 2007.
 Liberman 1996, p. 42.
 Milward 1992, p. 138.
 Milward 1992, p. 148.
 Barber & Harrison 2006, p. 232.
 Hill 2005, p. 5.
 Christofferson & Christofferson 2006, p. 156
 Radtke 1997, p. 107.
 Rahn 2001, p. 266.
 Harrison 1998, p. 3.
 Compare: Wilson, Mark R. (2016). Destructive Creation: American Business and 
the Winning of World War II. American Business, Politics, and Society (reprint 
ed.). Philadelphia: University of Pennsylvania Press. p. 2. ISBN 9780812293548. 
Retrieved 19 December 2019. By producing nearly two thirds of the munitions 
used by Allied forces - including huge numbers of aircraft, ships, tanks, 
trucks, rifles, artillery shells , and bombs - American industry became what 
President Franklin D. Roosevelt once called 'the arsenal of democracy' [...].
 Harrison 1998, p. 2.
 Bernstein 1991, p. 267.
 Griffith, Charles (1999). The Quest: Haywood Hansell and American Strategic 
Bombing in World War II. Diane Publishing. p. 203. ISBN 978-1-58566-069-8.
 Overy 1994, p. 26.
 BBSU 1998, p. 84; Lindberg & Todd 2001, p. 126..
 Unidas, Naciones (2005). World Economic And Social Survey 2004: International 
Migration. United Nations Pubns. p. 23. ISBN 978-92-1-109147-2.
 Tucker & Roberts 2004, p. 76.
 Levine 1992, p. 227.
 Klavans, Di Benedetto & Prudom 1997; Ward 2010, pp. 247–51.
 Tucker & Roberts 2004, p. 163.
 Bishop, Chris; Chant, Chris (2004). Aircraft Carriers: The World's Greatest 
Naval Vessels and Their Aircraft. Wigston, Leics: Silverdale Books. p. 7. ISBN 
978-1-84509-079-1.
 Chenoweth, H. Avery; Nihart, Brooke (2005). Semper Fi: The Definitive 
Illustrated History of the U.S. Marines. New York: Main Street. p. 180. ISBN 
978-1-4027-3099-3.
 Sumner & Baker 2001, p. 25.
 Hearn 2007, p. 14.
 Gardiner & Brown 2004, p. 52.
 Burcher & Rydill 1995, p. 15.
 Burcher & Rydill 1995, p. 16.
 Tucker & Roberts 2004, p. 125.
 Dupuy, Trevor Nevitt (1982). The Evolution of Weapons and Warfare. Jane's 
Information Group. p. 231. ISBN 978-0-7106-0123-0.
 Tucker & Roberts 2004, p. 108.
 Tucker & Roberts 2004, p. 734.
 Cowley & Parker 2001, p. 221.
 Sprague, Oliver; Griffiths, Hugh (2006). "The AK-47: the worlds favourite 
killing machine" (PDF). controlarms.org. p. 1. Archived from the original on 28 
December 2018. Retrieved 14 November 2009.
 Ratcliff 2006, p. 11.
 Schoenherr, Steven (2007). "Code Breaking in World War II". History Department 
at the University of San Diego. Archived from the original on 9 May 2008. 
Retrieved 15 November 2009.
 Macintyre, Ben (10 December 2010). "Bravery of thousands of Poles was vital in 
securing victory". The Times. London. p. 27.
 Rowe, Neil C.; Rothstein, Hy. "Deception for Defense of Information Systems: 
Analogies from Conventional Warfare". Departments of Computer Science and 
Defense Analysis U.S. Naval Postgraduate School. Air University. Archived from 
the original on 23 November 2010. Retrieved 15 November 2009.
 "Discovery and Development of Penicillin: International Historic Chemical 
Landmark". Washington, D.C.: American Chemical Society. Archived from the 
original on 28 June 2019. Retrieved 15 July 2019.
References
Main article: Bibliography of World War II
Adamthwaite, Anthony P. (1992). The Making of the Second World War. New York: 
Routledge. ISBN 978-0-415-90716-3.
Anderson, Irvine H., Jr. (1975). "The 1941 De Facto Embargo on Oil to Japan: A 
Bureaucratic Reflex". The Pacific Historical Review. 44 (2): 201–31. 
doi:10.2307/3638003. JSTOR 3638003.
Applebaum, Anne (2003). Gulag: A History of the Soviet Camps. London: Allen 
Lane. ISBN 978-0-7139-9322-6.
——— (2012). Iron Curtain: The Crushing of Eastern Europe 1944–56. 
London: Allen Lane. ISBN 978-0-7139-9868-9.
Bacon, Edwin (1992). "Glasnost' and the Gulag: New Information on Soviet Forced 
Labour around World War II". Soviet Studies. 44 (6): 1069–86. 
doi:10.1080/09668139208412066. JSTOR 152330.
Badsey, Stephen (1990). Normandy 1944: Allied Landings and Breakout. Oxford: 
Osprey Publishing. ISBN 978-0-85045-921-0.
Balabkins, Nicholas (1964). Germany Under Direct Controls: Economic Aspects of 
Industrial Disarmament 1945–1948. New Brunswick, NJ: Rutgers University 
Press. ISBN 978-0-8135-0449-0.
Barber, John; Harrison, Mark (2006). "Patriotic War, 1941–1945". In Ronald 
Grigor Suny, ed.,' The Cambridge History of Russia, Volume III: The Twentieth 
Century pp. 217–42. Cambridge: Cambridge University Press. ISBN 
978-0-521-81144-6.
Barker, A.J. (1971). The Rape of Ethiopia 1936. New York: Ballantine Books. 
ISBN 978-0-345-02462-6.
Barrett, David P.; Shyu, Lawrence N. (2001). China in the Anti-Japanese War, 
1937–1945: Politics, Culture and Society. New York: Peter Lang. ISBN 
978-0-8204-4556-4.
Beevor, Antony (1998). Stalingrad. New York: Viking. ISBN 978-0-670-87095-0.
——— (2012). The Second World War. London: Weidenfeld & Nicolson. ISBN 
978-0-297-84497-6.
Belco, Victoria (2010). War, Massacre, and Recovery in Central Italy: 
1943–1948. Toronto: University of Toronto Press. ISBN 978-0-8020-9314-1.
Bellamy, Chris T. (2007). Absolute War: Soviet Russia in the Second World War. 
New York: Alfred A. Knopf. ISBN 978-0-375-41086-4.
Ben-Horin, Eliahu (1943). The Middle East: Crossroads of History. New York: 
W.W. Norton.
Berend, Ivan T. (1996). Central and Eastern Europe, 1944–1993: Detour from 
the Periphery to the Periphery. Cambridge: Cambridge University Press. ISBN 
978-0-521-55066-6.
Bernstein, Gail Lee (1991). Recreating Japanese Women, 1600–1945. Berkeley & 
Los Angeles: University of California Press. ISBN 978-0-520-07017-2.
Bilhartz, Terry D.; Elliott, Alan C. (2007). Currents in American History: A 
Brief History of the United States. Armonk, NY: M.E. Sharpe. ISBN 
978-0-7656-1821-4.
Bilinsky, Yaroslav (1999). Endgame in NATO's Enlargement: The Baltic States and 
Ukraine. Westport, CT: Greenwood Publishing Group. ISBN 978-0-275-96363-7.
Bix, Herbert P. (2000). Hirohito and the Making of Modern Japan. New York: 
HarperCollins. ISBN 978-0-06-019314-0.
Black, Jeremy (2003). World War Two: A Military History. Abingdon & New York: 
Routledge. ISBN 978-0-415-30534-1.
Blinkhorn, Martin (2006) [1984]. Mussolini and Fascist Italy (3rd ed.). 
Abingdon & New York: Routledge. ISBN 978-0-415-26206-4.
Bonner, Kit; Bonner, Carolyn (2001). Warship Boneyards. Osceola, WI: MBI 
Publishing Company. ISBN 978-0-7603-0870-7.
Borstelmann, Thomas (2005). "The United States, the Cold War, and the colour 
line". In Melvyn P. Leffler and David S. Painter, eds., Origins of the Cold 
War: An International History (pp. 317–32) (2nd ed.). Abingdon & New York: 
Routledge. ISBN 978-0-415-34109-7.
Bosworth, Richard; Maiolo, Joseph (2015). The Cambridge History of the Second 
World War Volume 2: Politics and Ideology. The Cambridge History of the Second 
World War (3 vol). Cambridge: Cambridge University Press. pp. 313–14.
Brayley, Martin J. (2002). The British Army 1939–45, Volume 3: The Far East. 
Oxford: Osprey Publishing. ISBN 978-1-84176-238-8.
British Bombing Survey Unit (1998). The Strategic Air War Against Germany, 
1939–1945. London & Portland, OR: Frank Cass Publishers. ISBN 
978-0-7146-4722-7.
Brody, J. Kenneth (1999). The Avoidable War: Pierre Laval and the Politics of 
Reality, 1935–1936. New Brunswick, NJ: Transaction Publishers. ISBN 
978-0-7658-0622-2.
Brown, David (2004). The Road to Oran: Anglo-French Naval Relations, September 
1939 – July 1940. London & New York: Frank Cass. ISBN 978-0-7146-5461-4.
Buchanan, Tom (2006). Europe's Troubled Peace, 1945–2000. Oxford & Malden, 
MA: Blackwell Publishing. ISBN 978-0-631-22162-3.
Budiansky, Stephen (2001). Battle of Wits: The Complete Story of Codebreaking 
in World War II. London: Penguin Books. ISBN 978-0-14-028105-7.
Bueno de Mesquita, Bruce; Smith, Alastair; Siverson, Randolph M.; Morrow, James 
D. (2003). The Logic of Political Survival. Cambridge, MA: MIT Press. ISBN 
978-0-262-02546-1.
Bull, Martin J.; Newell, James L. (2005). Italian Politics: Adjustment Under 
Duress. Polity. ISBN 978-0-7456-1298-0.
Bullock, Alan (1990). Hitler: A Study in Tyranny. London: Penguin Books. ISBN 
978-0-14-013564-0.
Burcher, Roy; Rydill, Louis (1995). Concepts in Submarine Design. Journal of 
Applied Mechanics. 62. Cambridge: Cambridge University Press. p. 268. 
Bibcode:1995JAM....62R.268B. doi:10.1115/1.2895927. ISBN 978-0-521-55926-3.
Busky, Donald F. (2002). Communism in History and Theory: Asia, Africa, and the 
Americas. Westport, CT: Praeger Publishers. ISBN 978-0-275-97733-7.
Canfora, Luciano (2006) [2004]. Democracy in Europe: A History. Oxford & Malden 
MA: Blackwell Publishing. ISBN 978-1-4051-1131-7.
Cantril, Hadley (1940). "America Faces the War: A Study in Public Opinion". 
Public Opinion Quarterly. 4 (3): 387–407. doi:10.1086/265420. JSTOR 2745078.
Chang, Iris (1997). The Rape of Nanking: The Forgotten Holocaust of World War 
II. New York: Basic Books. ISBN 978-0-465-06835-7.
Christofferson, Thomas R.; Christofferson, Michael S. (2006). France During 
World War II: From Defeat to Liberation. New York: Fordham University Press. 
ISBN 978-0-8232-2562-0.
Chubarov, Alexander (2001). Russia's Bitter Path to Modernity: A History of the 
Soviet and Post-Soviet Eras. London & New York: Continuum. ISBN 
978-0-8264-1350-5.
Ch'i, Hsi-Sheng (1992). "The Military Dimension, 1942–1945". In James C. 
Hsiung and Steven I. Levine, eds., China's Bitter Victory: War with Japan, 
1937–45 pp. 157–84. Armonk, NY: M.E. Sharpe. ISBN 978-1-56324-246-5.
Cienciala, Anna M. (2010). "Another look at the Poles and Poland during World 
War II". The Polish Review. 55 (1): 123–43. JSTOR 25779864.
Clogg, Richard (2002). A Concise History of Greece (2nd ed.). Cambridge: 
Cambridge University Press. ISBN 978-0-521-80872-9.
Coble, Parks M. (2003). Chinese Capitalists in Japan's New Order: The Occupied 
Lower Yangzi, 1937–1945. Berkeley & Los Angeles: University of California 
Press. ISBN 978-0-520-23268-6.
Collier, Paul (2003). The Second World War (4): The Mediterranean 1940–1945. 
Oxford: Osprey Publishing. ISBN 978-1-84176-539-6.
Collier, Martin; Pedley, Philip (2000). Germany 1919–45. Oxford: Heinemann. 
ISBN 978-0-435-32721-7.
Commager, Henry Steele (2004). The Story of the Second World War. Brassey's. 
ISBN 978-1-57488-741-9.
Coogan, Anthony (1993). "The Volunteer Armies of Northeast China". History 
Today. 43. Retrieved 6 May 2012.
Cook, Chris; Bewes, Diccon (1997). What Happened Where: A Guide to Places and 
Events in Twentieth-Century History. London: UCL Press. ISBN 978-1-85728-532-1.
Cowley, Robert; Parker, Geoffrey, eds. (2001). The Reader's Companion to 
Military History. Boston: Houghton Mifflin Company. ISBN 978-0-618-12742-9.
Darwin, John (2007). After Tamerlane: The Rise & Fall of Global Empires 
1400–2000. London: Penguin Books. ISBN 978-0-14-101022-9.
Davidson, Eugene (1999). The Death and Life of Germany: An Account of the 
American Occupation. University of Missouri Press. ISBN 978-0-8262-1249-8.
Davies, Norman (2006). Europe at War 1939–1945: No Simple Victory. London: 
Macmillan. ix+544 pages. ISBN 978-0-333-69285-1. OCLC 70401618.
Dear, I.C.B.; Foot, M.R.D., eds. (2001) [1995]. The Oxford Companion to World 
War II. Oxford: Oxford University Press. ISBN 978-0-19-860446-4.
DeLong, J. Bradford; Eichengreen, Barry (1993). "The Marshall Plan: History's 
Most Successful Structural Adjustment Program". In Rudiger Dornbusch, Wilhelm 
Nölling and Richard Layard, eds., Postwar Economic Reconstruction and Lessons 
for the East Today (pp. 189–230). Cambridge, MA: MIT Press. ISBN 
978-0-262-04136-2.
Dower, John W. (1986). War Without Mercy: Race and Power in the Pacific War. 
New York: Pantheon Books. ISBN 978-0-394-50030-0.
Drea, Edward J. (2003). In the Service of the Emperor: Essays on the Imperial 
Japanese Army. Lincoln, NE: University of Nebraska Press. ISBN 
978-0-8032-6638-4.
de Grazia, Victoria; Paggi, Leonardo (Autumn 1991). "Story of an Ordinary 
Massacre: Civitella della Chiana, 29 June, 1944". Cardozo Studies in Law and 
Literature. 3 (2): 153–69. doi:10.1525/lal.1991.3.2.02a00030. JSTOR 743479.
Dunn, Dennis J. (1998). Caught Between Roosevelt & Stalin: America's 
Ambassadors to Moscow. Lexington, KY: University Press of Kentucky. ISBN 
978-0-8131-2023-2.
Eastman, Lloyd E. (1986). "Nationalist China during the Sino-Japanese War 
1937–1945". In John K. Fairbank and Denis Twitchett, eds., The Cambridge 
History of China, Volume 13: Republican China 1912–1949, Part 2. Cambridge: 
Cambridge University Press. ISBN 978-0-521-24338-4.
Ellman, Michael (2002). "Soviet Repression Statistics: Some Comments" (PDF). 
Europe-Asia Studies. 54 (7): 1151–1172. doi:10.1080/0966813022000017177. 
JSTOR 826310. Archived from the original (PDF) on 22 November 2012. Copy
———; Maksudov, S. (1994). "Soviet Deaths in the Great Patriotic War: A 
Note" (PDF). Europe-Asia Studies. 46 (4): 671–80. 
doi:10.1080/09668139408412190. JSTOR 152934. PMID 12288331.
Emadi-Coffin, Barbara (2002). Rethinking International Organization: 
Deregulation and Global Governance. London & New York: Routledge. ISBN 
978-0-415-19540-9.
Erickson, John (2001). "Moskalenko". In Shukman, Harold (ed.). Stalin's 
Generals. London: Phoenix Press. pp. 137–54. ISBN 978-1-84212-513-7.
——— (2003). The Road to Stalingrad. London: Cassell Military. ISBN 
978-0-304-36541-8.
Evans, David C.; Peattie, Mark R. (2012) [1997]. Kaigun: Strategy, Tactics, and 
Technology in the Imperial Japanese Navy. Annapolis, MD: Naval Institute Press. 
ISBN 978-1-59114-244-7.
Evans, Richard J. (2008). The Third Reich at War. London: Allen Lane. ISBN 
978-0-7139-9742-2.
Fairbank, John King; Goldman, Merle (2006) [1994]. China: A New History (2nd 
ed.). Cambridge: Harvard University Press. ISBN 978-0-674-01828-0.
Farrell, Brian P. (1993). "Yes, Prime Minister: Barbarossa, Whipcord, and the 
Basis of British Grand Strategy, Autumn 1941". Journal of Military History. 57 
(4): 599–625. doi:10.2307/2944096. JSTOR 2944096.
Ferguson, Niall (2006). The War of the World: Twentieth-Century Conflict and 
the Descent of the West. Penguin. ISBN 978-0-14-311239-6.
Fitzgerald, Stephanie (2011). Children of the Holocaust. Mankato, MN: Compass 
Point Books. ISBN 978-0-7565-4390-7.
Forrest, Glen; Evans, Anthony; Gibbons, David (2012). The Illustrated Timeline 
of Military History. New York: The Rosen Publishing Group. ISBN 
978-1-4488-4794-5.
Förster, Stig; Gessler, Myriam (2005). "The Ultimate Horror: Reflections on 
Total War and Genocide". In Roger Chickering, Stig Förster and Bernd Greiner, 
eds., A World at Total War: Global Conflict and the Politics of Destruction, 
1937–1945 (pp. 53–68). Cambridge: Cambridge University Press. ISBN 
978-0-521-83432-2.
Frei, Norbert (2002). Adenauer's Germany and the Nazi Past: The Politics of 
Amnesty and Integration. New York: Columbia University Press. ISBN 
978-0-231-11882-8.
Gardiner, Robert; Brown, David K., eds. (2004). The Eclipse of the Big Gun: The 
Warship 1906–1945. London: Conway Maritime Press. ISBN 978-0-85177-953-9.
Garthoff, Raymond L. (1969). "The Soviet Manchurian Campaign, August 1945". 
Military Affairs. 33 (2): 312–36. doi:10.2307/1983926. JSTOR 1983926.
Garver, John W. (1988). Chinese-Soviet Relations, 1937–1945: The Diplomacy of 
Chinese Nationalism. New York: Oxford University Press. ISBN 978-0-19-505432-3.
Gilbert, Martin (2001). "Final Solution". In Dear, Ian; Foot, Richard D. 
(eds.). The Oxford Companion to World War II. Oxford: Oxford University Press. 
pp. 285–92. ISBN 978-0-19-280670-3.
Gilbert, Martin (1989). Second World War. London: Weidenfeld and Nicolson. ISBN 
978-0-297-79616-9.
Glantz, David M. (1986). "Soviet Defensive Tactics at Kursk, July 1943". CSI 
Report No. 11. Combined Arms Research Library. OCLC 278029256. Archived from 
the original on 6 March 2008. Retrieved 15 July 2013.
——— (1989). Soviet Military Deception in the Second World War. Abingdon & 
New York: Frank Cass. ISBN 978-0-7146-3347-3.
——— (1998). When Titans Clashed: How the Red Army Stopped Hitler. 
Lawrence, KS: University Press of Kansas. ISBN 978-0-7006-0899-7.
——— (2001). "The Soviet-German War 1941–45 Myths and Realities: A 
Survey Essay" (PDF). Archived from the original (PDF) on 9 July 2011.
——— (2002). The Battle for Leningrad: 1941–1944. Lawrence, KS: 
University Press of Kansas. ISBN 978-0-7006-1208-6.
——— (2005). "August Storm: The Soviet Strategic Offensive in Manchuria". 
Leavenworth Papers. Combined Arms Research Library. OCLC 78918907. Archived 
from the original on 2 March 2008. Retrieved 15 July 2013.
Goldstein, Margaret J. (2004). World War II: Europe. Minneapolis: Lerner 
Publications. ISBN 978-0-8225-0139-8.
Gordon, Andrew (2004). "The greatest military armada ever launched". In Jane 
Penrose, ed., The D-Day Companion. Oxford: Osprey Publishing. pp. 127–144. 
ISBN 978-1-84176-779-6.
Gordon, Robert S.C. (2012). The Holocaust in Italian Culture, 1944–2010. 
Stanford, CA: Stanford University Press. ISBN 978-0-8047-6346-2.
Grove, Eric J. (1995). "A Service Vindicated, 1939–1946". In J.R. Hill, ed., 
The Oxford Illustrated History of the Royal Navy. Oxford: Oxford University 
Press. pp. 348–80. ISBN 978-0-19-211675-8.
Hane, Mikiso (2001). Modern Japan: A Historical Survey (3rd ed.). Boulder, CO: 
Westview Press. ISBN 978-0-8133-3756-2.
Hanhimäki, Jussi M. (1997). Containing Coexistence: America, Russia, and the 
"Finnish Solution". Kent, OH: Kent State University Press. ISBN 
978-0-87338-558-9.
Harris, Sheldon H. (2002). Factories of Death: Japanese Biological Warfare, 
1932–1945, and the American Cover-up (2nd ed.). London & New York: Routledge. 
ISBN 978-0-415-93214-1.
Harrison, Mark (1998). "The economics of World War II: an overview". In Mark 
Harrison, ed., The Economics of World War II: Six Great Powers in International 
Comparison. Cambridge: Cambridge University Press. pp. 1–42. ISBN 
978-0-521-62046-8.
Hart, Stephen; Hart, Russell; Hughes, Matthew (2000). The German Soldier in 
World War II. Osceola, WI: MBI Publishing Company. ISBN 978-1-86227-073-2.
Hauner, Milan (1978). "Did Hitler Want a World Dominion?". Journal of 
Contemporary History. 13 (1): 15–32. doi:10.1177/002200947801300102. JSTOR 
260090.
Healy, Mark (1992). Kursk 1943: The Tide Turns in the East. Oxford: Osprey 
Publishing. ISBN 978-1-85532-211-0.
Hearn, Chester G. (2007). Carriers in Combat: The Air War at Sea. 
Mechanicsburg, PA: Stackpole Books. ISBN 978-0-8117-3398-4.
Hedgepeth, Sonja; Saidel, Rochelle (2010). Sexual Violence against Jewish Women 
During the Holocaust. Lebanon, NH: University Press of New England. ISBN 
978-1-58465-904-4.
Hempel, Andrew (2005). Poland in World War II: An Illustrated Military History. 
New York: Hippocrene Books. ISBN 978-0-7818-1004-3.
Herbert, Ulrich (1994). "Labor as spoils of conquest, 1933–1945". In David F. 
Crew, ed., Nazism and German Society, 1933–1945. London & New York: 
Routledge. pp. 219–73. ISBN 978-0-415-08239-6.
Herf, Jeffrey (2003). "The Nazi Extermination Camps and the Ally to the East. 
Could the Red Army and Air Force Have Stopped or Slowed the Final Solution?". 
Kritika: Explorations in Russian and Eurasian History. 4 (4): 913–30. 
doi:10.1353/kri.2003.0059.
Hill, Alexander (2005). The War Behind The Eastern Front: The Soviet Partisan 
Movement In North-West Russia 1941–1944. London & New York: Frank Cass. ISBN 
978-0-7146-5711-0.
Holland, James (2008). Italy's Sorrow: A Year of War 1944–45. London: 
HarperPress. ISBN 978-0-00-717645-8.
Hosking, Geoffrey A. (2006). Rulers and Victims: The Russians in the Soviet 
Union. Cambridge: Harvard University Press. ISBN 978-0-674-02178-5.
Howard, Joshua H. (2004). Workers at War: Labor in China's Arsenals, 
1937–1953. Stanford, CA: Stanford University Press. ISBN 978-0-8047-4896-4.
Hsu, Long-hsuen; Chang, Ming-kai (1971). History of The Sino-Japanese War 
(1937–1945) 2nd Ed. Chung Wu Publishers. ASIN B00005W210.
Ingram, Norman (2006). "Pacifism". In Lawrence D. Kritzman and Brian J. Reilly, 
eds., The Columbia History Of Twentieth-Century French Thought. New York: 
Columbia University Press. pp. 76–78. ISBN 978-0-231-10791-4.
Iriye, Akira (1981). Power and Culture: The Japanese-American War, 1941–1945. 
Cambridge, MA: Harvard University Press. ISBN 978-0-674-69580-1.
Jackson, Ashley (2006). The British Empire and the Second World War. London & 
New York: Hambledon Continuum. ISBN 978-1-85285-417-1.
Joes, Anthony James (2004). Resisting Rebellion: The History And Politics of 
Counterinsurgency. Lexington: University Press of Kentucky. ISBN 
978-0-8131-2339-4.
Jowett, Philip S. (2001). The Italian Army 1940–45, Volume 2: Africa 
1940–43. Oxford: Osprey Publishing. ISBN 978-1-85532-865-5.
———; Andrew, Stephen (2002). The Japanese Army, 1931–45. Oxford: Osprey 
Publishing. ISBN 978-1-84176-353-8.
Jukes, Geoffrey (2001). "Kuznetzov". In Harold Shukman, ed., Stalin's Generals 
(pp. 109–16). London: Phoenix Press. ISBN 978-1-84212-513-7.
Kantowicz, Edward R. (1999). The Rage of Nations. Grand Rapids, MI: William B. 
Eerdmans Publishing Company. ISBN 978-0-8028-4455-2.
——— (2000). Coming Apart, Coming Together. Grand Rapids, MI: William B. 
Eerdmans Publishing Company. ISBN 978-0-8028-4456-9.
Keeble, Curtis (1990). "The historical perspective". In Alex Pravda and Peter 
J. Duncan, eds., Soviet-British Relations Since the 1970s. Cambridge: Cambridge 
University Press. ISBN 978-0-521-37494-1.
Keegan, John (1997). The Second World War. London: Pimlico. ISBN 
978-0-7126-7348-8.
Kennedy, David M. (2001). Freedom from Fear: The American People in Depression 
and War, 1929–1945. Oxford University Press. ISBN 978-0-19-514403-1.
Kennedy-Pipe, Caroline (1995). Stalin's Cold War: Soviet Strategies in Europe, 
1943–56. Manchester: Manchester University Press. ISBN 978-0-7190-4201-0.
Kershaw, Ian (2001). Hitler, 1936–1945: Nemesis. New York: W.W. Norton]. ISBN 
978-0-393-04994-7.
——— (2007). Fateful Choices: Ten Decisions That Changed the World, 
1940–1941. London: Allen Lane. ISBN 978-0-7139-9712-5.
Kitson, Alison (2001). Germany 1858–1990: Hope, Terror, and Revival. Oxford: 
Oxford University Press. ISBN 978-0-19-913417-5.
Klavans, Richard A.; Di Benedetto, C. Anthony; Prudom, Melanie J. (1997). 
"Understanding Competitive Interactions: The U.S. Commercial Aircraft Market". 
Journal of Managerial Issues. 9 (1): 13–361. JSTOR 40604127.
Kleinfeld, Gerald R. (1983). "Hitler's Strike for Tikhvin". Military Affairs. 
47 (3): 122–128. doi:10.2307/1988082. JSTOR 1988082.
Koch, H.W. (1983). "Hitler's 'Programme' and the Genesis of Operation 
'Barbarossa'". The Historical Journal. 26 (4): 891–920. 
doi:10.1017/S0018246X00012747. JSTOR 2639289.
Kolko, Gabriel (1990) [1968]. The Politics of War: The World and United States 
Foreign Policy, 1943–1945. New York: Random House. ISBN 978-0-679-72757-6.
Laurier, Jim (2001). Tobruk 1941: Rommel's Opening Move. Oxford: Osprey 
Publishing. ISBN 978-1-84176-092-6.
Lee, En-han (2002). "The Nanking Massacre Reassessed: A Study of the 
Sino-Japanese Controversy over the Factual Number of Massacred Victims". In 
Robert Sabella, Fei Fei Li and David Liu, eds., Nanking 1937: Memory and 
Healing (pp. 47–74). Armonk, NY: M.E. Sharpe. ISBN 978-0-7656-0816-1.
Leffler, Melvyn P.; Westad, Odd Arne, eds. (2010). The Cambridge History of the 
Cold War (3 volumes). Cambridge: Cambridge University Press. ISBN 
978-0-521-83938-9.
Levine, Alan J. (1992). The Strategic Bombing of Germany, 1940–1945. 
Westport, CT: Praeger. ISBN 978-0-275-94319-6.
Lewis, Morton (1953). "Japanese Plans and American Defenses". In Greenfield, 
Kent Roberts (ed.). The Fall of the Philippines. Washington, DC: US Government 
Printing Office. LCCN 53-63678.
Liberman, Peter (1996). Does Conquest Pay?: The Exploitation of Occupied 
Industrial Societies. Princeton, NJ: Princeton University Press. ISBN 
978-0-691-02986-3.
Liddell Hart, Basil (1977). History of the Second World War (4th ed.). London: 
Pan. ISBN 978-0-330-23770-3.
Lightbody, Bradley (2004). The Second World War: Ambitions to Nemesis. London & 
New York: Routledge. ISBN 978-0-415-22404-8.
Lindberg, Michael; Todd, Daniel (2001). Brown-, Green- and Blue-Water Fleets: 
the Influence of Geography on Naval Warfare, 1861 to the Present. Westport, CT: 
Praeger. ISBN 978-0-275-96486-3.
Lowe, C.J.; Marzari, F. (2002). Italian Foreign Policy 1870–1940. London: 
Routledge. ISBN 978-0-415-26681-9.
Lynch, Michael (2010). The Chinese Civil War 1945–49. Oxford: Osprey 
Publishing. ISBN 978-1-84176-671-3.
Macksey, Kenneth (1997) [1979]. Rommel: Battles and Campaigns. Cambridge, MA: 
Da Capo Press. ISBN 978-0-306-80786-2.
Maddox, Robert James (1992). The United States and World War II. Boulder, CO: 
Westview Press. ISBN 978-0-8133-0437-3.
Maingot, Anthony P. (1994). The United States and the Caribbean: Challenges of 
an Asymmetrical Relationship. Boulder, CO: Westview Press. ISBN 
978-0-8133-2241-4.
Mandelbaum, Michael (1988). The Fate of Nations: The Search for National 
Security in the Nineteenth and Twentieth Centuries. Cambridge University Press. 
p. 96. ISBN 978-0-521-35790-6.
Marston, Daniel (2005). The Pacific War Companion: From Pearl Harbor to 
Hiroshima. Oxford: Osprey Publishing. ISBN 978-1-84176-882-3.
Masaya, Shiraishi (1990). Japanese Relations with Vietnam, 1951–1987. Ithaca, 
NY: SEAP Publications. ISBN 978-0-87727-122-2.
May, Ernest R. (1955). "The United States, the Soviet Union, and the Far 
Eastern War, 1941–1945". Pacific Historical Review. 24 (2): 153–74. 
doi:10.2307/3634575. JSTOR 3634575.
Mazower, Mark (2008). Hitler's Empire: Nazi Rule in Occupied Europe. London: 
Allen Lane. ISBN 978-1-59420-188-2.
Milner, Marc (1990). "The Battle of the Atlantic". In John Gooch, ed., Decisive 
Campaigns of the Second World War (pp. 45–66). Abingdon: Frank Cass. ISBN 
978-0-7146-3369-5.
Milward, A.S. (1964). "The End of the Blitzkrieg". The Economic History Review. 
16 (3): 499–518. JSTOR 2592851.
——— (1992) [1977]. War, Economy, and Society, 1939–1945. Berkeley, CA: 
University of California Press. ISBN 978-0-520-03942-1.
Minford, Patrick (1993). "Reconstruction and the UK Postwar Welfare State: 
False Start and New Beginning". In Rudiger Dornbusch, Wilhelm Nölling and 
Richard Layard, eds., Postwar Economic Reconstruction and Lessons for the East 
Today (pp. 115–38). Cambridge, MA: MIT Press. ISBN 978-0-262-04136-2.
Mingst, Karen A.; Karns, Margaret P. (2007). United Nations in the Twenty-First 
Century (3rd ed.). Boulder, CO: Westview Press. ISBN 978-0-8133-4346-4.
Miscamble, Wilson D. (2007). From Roosevelt to Truman: Potsdam, Hiroshima, and 
the Cold War. New York: Cambridge University Press. ISBN 978-0-521-86244-8.
Mitcham, Samuel W. (2007) [1982]. Rommel's Desert War: The Life and Death of 
the Afrika Korps. Mechanicsburg, PA: Stackpole Books. ISBN 978-0-8117-3413-4.
Mitter, Rana (2014). Forgotten Ally: China's World War II, 1937–1945. Mariner 
Books. ISBN 978-0-544-33450-2.
Molinari, Andrea (2007). Desert Raiders: Axis and Allied Special Forces 
1940–43. Oxford: Osprey Publishing. ISBN 978-1-84603-006-2.
Morison, Samuel Eliot (2002). History of United States Naval Operations in 
World War II, Volume 14: Victory in the Pacific, 1945. Champaign, IL: 
University of Illinois Press. ISBN 978-0-252-07065-5.
Murray, Williamson (1983). Strategy for Defeat: The Luftwaffe, 1933–1945. 
Maxwell Air Force Base, AL: Air University Press. ISBN 978-1-4294-9235-5.
———; Millett, Allan Reed (2001). A War to Be Won: Fighting the Second 
World War. Cambridge, MA: Harvard University Press. ISBN 978-0-674-00680-5.
Myers, Ramon; Peattie, Mark (1987). The Japanese Colonial Empire, 1895–1945. 
Princeton, NJ: Princeton University Press. ISBN 978-0-691-10222-1.
Naimark, Norman (2010). "The Sovietization of Eastern Europe, 1944–1953". In 
Melvyn P. Leffler and Odd Arne Westad, eds., The Cambridge History of the Cold 
War, Volume I: Origins (pp. 175–97). Cambridge: Cambridge University Press. 
ISBN 978-0-521-83719-4.
Neary, Ian (1992). "Japan". In Martin Harrop, ed., Power and Policy in Liberal 
Democracies (pp. 49–70). Cambridge: Cambridge University Press. ISBN 
978-0-521-34579-8.
Neillands, Robin (2005). The Dieppe Raid: The Story of the Disastrous 1942 
Expedition. Bloomington, IN: Indiana University Press. ISBN 978-0-253-34781-7.
Neulen, Hans Werner (2000). In the skies of Europe – Air Forces allied to the 
Luftwaffe 1939–1945. Ramsbury, Marlborough, UK: The Crowood Press. ISBN 
1-86126-799-1.
Niewyk, Donald L.; Nicosia, Francis (2000). The Columbia Guide to the 
Holocaust. New York: Columbia University Press. ISBN 978-0-231-11200-0.
Overy, Richard (1994). War and Economy in the Third Reich. New York: Clarendon 
Press. ISBN 978-0-19-820290-5.
——— (1995). Why the Allies Won. London: Pimlico. ISBN 978-0-7126-7453-9.
——— (2004). The Dictators: Hitler's Germany, Stalin's Russia. New York: 
W.W. Norton. ISBN 978-0-393-02030-4.
———; Wheatcroft, Andrew (1999). The Road to War (2nd ed.). London: 
Penguin Books. ISBN 978-0-14-028530-7.
O'Reilly, Charles T. (2001). Forgotten Battles: Italy's War of Liberation, 
1943–1945. Lanham, MD: Lexington Books. ISBN 978-0-7391-0195-7.
Painter, David S. (2012). "Oil and the American Century" (PDF). The Journal of 
American History. 99 (1): 24–39. doi:10.1093/jahist/jas073.
Padfield, Peter (1998). War Beneath the Sea: Submarine Conflict During World 
War II. New York: John Wiley. ISBN 978-0-471-24945-0.
Pape, Robert A. (1993). "Why Japan Surrendered". International Security. 18 
(2): 154–201. doi:10.2307/2539100. JSTOR 2539100.
Parker, Danny S. (2004). Battle of the Bulge: Hitler's Ardennes Offensive, 
1944–1945 (New ed.). Cambridge, MA: Da Capo Press. ISBN 978-0-306-81391-7.
Payne, Stanley G. (2008). Franco and Hitler: Spain, Germany, and World War II. 
New Haven, CT: Yale University Press. ISBN 978-0-300-12282-4.
Perez, Louis G. (1998). The History of Japan. Westport, CT: Greenwood 
Publishing Group. ISBN 978-0-313-30296-1.
Petrov, Vladimir (1967). Money and Conquest: Allied Occupation Currencies in 
World War II. Baltimore, MD: Johns Hopkins University Press. ISBN 
978-0-8018-0530-1.
Polley, Martin (2000). An A–Z of Modern Europe Since 1789. London & New York: 
Routledge. ISBN 978-0-415-18597-4.
Portelli, Alessandro (2003). The Order Has Been Carried Out: History, Memory, 
and Meaning of a Nazi Massacre in Rome. Basingstoke & New YorkPalgrave 
Macmillan978-1403980083.
Preston, P. W. (1998). Pacific Asia in the Global System: An Introduction. 
Oxford & Malden, MA: Blackwell Publishers. ISBN 978-0-631-20238-7.
Prins, Gwyn (2002). The Heart of War: On Power, Conflict and Obligation in the 
Twenty-First Century. London & New York: Routledge. ISBN 978-0-415-36960-2.
Radtke, K.W. (1997). "'Strategic' concepts underlying the so-called Hirota 
foreign policy, 1933–7". In Aiko Ikeo, ed., Economic Development in Twentieth 
Century East Asia: The International Context (pp. 100–20). London & New York: 
Routledge. ISBN 978-0-415-14900-6.
Rahn, Werner (2001). "The War in the Pacific". In Horst Boog, Werner Rahn, 
Reinhard Stumpf and Bernd Wegner, eds., Germany and the Second World War, 
Volume VI: The Global War (pp. 191–298). Oxford: Clarendon Press. ISBN 
978-0-19-822888-2.
Ratcliff, R.A. (2006). Delusions of Intelligence: Enigma, Ultra, and the End of 
Secure Ciphers. New York: Cambridge University Press. ISBN 978-0-521-85522-8.
Read, Anthony (2004). The Devil's Disciples: Hitler's Inner Circle. New York: 
W.W. Norton. ISBN 978-0-393-04800-1.
Read, Anthony; Fisher, David (2002) [1992]. The Fall Of Berlin. London: 
Pimlico. ISBN 978-0-7126-0695-0.
Record, Jeffery (2005). Appeasement Reconsidered: Investigating the Mythology 
of the 1930s (PDF). Diane Publishing. p. 50. ISBN 978-1-58487-216-0. Retrieved 
15 November 2009.
Rees, Laurence (2008). World War II Behind Closed Doors: Stalin, the Nazis and 
the West. London: BBC Books. ISBN 978-0-563-49335-8.
Regan, Geoffrey (2004). The Brassey's Book of Military Blunders. Brassey's. 
ISBN 978-1-57488-252-0.
Reinhardt, Klaus (1992). Moscow – The Turning Point: The Failure of Hitler's 
Strategy in the Winter of 1941–42. Oxford: Berg. ISBN 978-0-85496-695-0.
Reynolds, David (2006). From World War to Cold War: Churchill, Roosevelt, and 
the International History of the 1940s. Oxford University Press. ISBN 
978-0-19-928411-5.
Rich, Norman (1992) [1973]. Hitler's War Aims, Volume I: Ideology, the Nazi 
State, and the Course of Expansion. New York: W.W. Norton. ISBN 
978-0-393-00802-9.
Ritchie, Ella (1992). "France". In Martin Harrop, ed., Power and Policy in 
Liberal Democracies (pp. 23–48). Cambridge: Cambridge University Press. ISBN 
978-0-521-34579-8.
Roberts, Cynthia A. (1995). "Planning for War: The Red Army and the Catastrophe 
of 1941". Europe-Asia Studies. 47 (8): 1293–1326. 
doi:10.1080/09668139508412322. JSTOR 153299.
Roberts, Geoffrey (2006). Stalin's Wars: From World War to Cold War, 
1939–1953. New Haven, CT: Yale University Press. ISBN 978-0-300-11204-7.
Roberts, J.M. (1997). The Penguin History of Europe. London: Penguin Books. 
ISBN 978-0-14-026561-3.
Ropp, Theodore (2000). War in the Modern World (Revised ed.). Baltimore, MD: 
Johns Hopkins University Press. ISBN 978-0-8018-6445-2.
Roskill, S.W. (1954). The War at Sea 1939–1945, Volume 1: The Defensive. 
History of the Second World War. United Kingdom Military Series. London: HMSO.
Ross, Steven T. (1997). American War Plans, 1941–1945: The Test of Battle. 
Abingdon & New York: Routledge. ISBN 978-0-7146-4634-3.
Rottman, Gordon L. (2002). World War II Pacific Island Guide: A Geo-Military 
Study. Westport, CT: Greenwood Press. ISBN 978-0-313-31395-0.
Rotundo, Louis (1986). "The Creation of Soviet Reserves and the 1941 Campaign". 
Military Affairs. 50 (1): 21–28. doi:10.2307/1988530. JSTOR 1988530.
Salecker, Gene Eric (2001). Fortress Against the Sun: The B-17 Flying Fortress 
in the Pacific. Conshohocken, PA: Combined Publishing. ISBN 978-1-58097-049-5.
Schain, Martin A., ed. (2001). The Marshall Plan Fifty Years Later. London: 
Palgrave Macmillan. ISBN 978-0-333-92983-4.
Schmitz, David F. (2000). Henry L. Stimson: The First Wise Man. Lanham, MD: 
Rowman & Littlefield. ISBN 978-0-8420-2632-1.
Schofield, B.B. (1981). "The Defeat of the U-Boats during World War II". 
Journal of Contemporary History. 16 (1): 119–29. 
doi:10.1177/002200948101600107. JSTOR 260619.
Schoppa, R. Keith (2011). In a Sea of Bitterness, Refugees during the 
Sino-Japanese War. Harvard University Press. ISBN 978-0-674-05988-7.
Sella, Amnon (1978). ""Barbarossa": Surprise Attack and Communication". Journal 
of Contemporary History. 13 (3): 555–83. doi:10.1177/002200947801300308. 
JSTOR 260209.
——— (1983). "Khalkhin-Gol: The Forgotten War". Journal of Contemporary 
History. 18 (4): 651–87. JSTOR 260307.
Senn, Alfred Erich (2007). Lithuania 1940: Revolution from Above. Amsterdam & 
New York: Rodopi. ISBN 978-90-420-2225-6.
Shaw, Anthony (2000). World War II: Day by Day. Osceola, WI: MBI Publishing 
Company. ISBN 978-0-7603-0939-1.
Shepardson, Donald E. (1998). "The Fall of Berlin and the Rise of a Myth". 
Journal of Military History. 62 (1): 135–54. doi:10.2307/120398. JSTOR 
120398.
Shirer, William L. (1990) [1960]. The Rise and Fall of the Third Reich: A 
History of Nazi Germany. New York: Simon & Schuster. ISBN 978-0-671-72868-7.
Shore, Zachary (2003). What Hitler Knew: The Battle for Information in Nazi 
Foreign Policy. New York: Oxford University Press. ISBN 978-0-19-518261-3.
Slim, William (1956). Defeat into Victory. London: Cassell. ISBN 
978-0-304-29114-4.
Smith, Alan (1993). Russia and the World Economy: Problems of Integration. 
London: Routledge. ISBN 978-0-415-08924-1.
Smith, J.W. (1994). The World's Wasted Wealth 2: Save Our Wealth, Save Our 
Environment. Institute for Economic Democracy. ISBN 978-0-9624423-2-2.
Smith, Peter C. (2002) [1970]. Pedestal: The Convoy That Saved Malta (5th ed.). 
Manchester: Goodall. ISBN 978-0-907579-19-9.
Smith, David J.; Pabriks, Artis; Purs, Aldis; Lane, Thomas (2002). The Baltic 
States: Estonia, Latvia and Lithuania. London: Routledge. ISBN 
978-0-415-28580-3.
Smith, Winston; Steadman, Ralph (2004). All Riot on the Western Front, Volume 
3. Last Gasp. ISBN 978-0-86719-616-0.
Snyder, Timothy (2010). Bloodlands: Europe Between Hitler and Stalin. London: 
The Bodley Head. ISBN 978-0-224-08141-2.
Sommerville, Donald (2008). The Complete Illustrated History of World War Two: 
An Authoritative Account of the Deadliest Conflict in Human History with 
Analysis of Decisive Encounters and Landmark Engagements. Leicester: Lorenz 
Books. ISBN 978-0-7548-1898-4.
Spring, D. W. (1986). "The Soviet Decision for War against Finland, 30 November 
1939". Soviet Studies. 38 (2): 207–26. doi:10.1080/09668138608411636. JSTOR 
151203.
Steinberg, Jonathan (1995). "The Third Reich Reflected: German Civil 
Administration in the Occupied Soviet Union, 1941–4". The English Historical 
Review. 110 (437): 620–51. doi:10.1093/ehr/cx.437.620. JSTOR 578338.
Steury, Donald P. (1987). "Naval Intelligence, the Atlantic Campaign and the 
Sinking of the Bismarck: A Study in the Integration of Intelligence into the 
Conduct of Naval Warfare". Journal of Contemporary History. 22 (2): 209–33. 
doi:10.1177/002200948702200202. JSTOR 260931.
Stueck, William (2010). "The Korean War". In Melvyn P. Leffler and Odd Arne 
Westad, eds., The Cambridge History of the Cold War, Volume I: Origins (pp. 
266–87). Cambridge: Cambridge University Press. ISBN 978-0-521-83719-4.
Sumner, Ian; Baker, Alix (2001). The Royal Navy 1939–45. Oxford: Osprey 
Publishing. ISBN 978-1-84176-195-4.
Swain, Bruce (2001). A Chronology of Australian Armed Forces at War 1939–45. 
Crows Nest: Allen & Unwin. ISBN 978-1-86508-352-0.
Swain, Geoffrey (1992). "The Cominform: Tito's International?". The Historical 
Journal. 35 (3): 641–63. doi:10.1017/S0018246X00026017.
Tanaka, Yuki (1996). Hidden Horrors: Japanese War Crimes in World War II. 
Boulder, CO: Westview Press. ISBN 978-0-8133-2717-4.
Taylor, A.J.P. (1961). The Origins of the Second World War. London: Hamish 
Hamilton.
——— (1979). How Wars Begin. London: Hamish Hamilton. ISBN 
978-0-241-10017-2.
Taylor, Jay (2009). The Generalissimo: Chiang Kai-shek and the Struggle for 
Modern China. Cambridge, MA: Harvard University Press. ISBN 978-0-674-03338-2.
Thomas, Nigel; Andrew, Stephen (1998). German Army 1939–1945 (2): North 
Africa & Balkans. Oxford: Osprey Publishing. ISBN 978-1-85532-640-8.
Thompson, John Herd; Randall, Stephen J. (2008). Canada and the United States: 
Ambivalent Allies (4th ed.). Athens, GA: University of Georgia Press. ISBN 
978-0-8203-3113-3.
Trachtenberg, Marc (1999). A Constructed Peace: The Making of the European 
Settlement, 1945–1963. Princeton, NJ: Princeton University Press. ISBN 
978-0-691-00273-6.
Tucker, Spencer C.; Roberts, Priscilla Mary (2004). Encyclopedia of World War 
II: A Political, Social, and Military History. ABC-CIO. ISBN 978-1-57607-999-7.
Umbreit, Hans (1991). "The Battle for Hegemony in Western Europe". In P. S. 
Falla, ed., Germany and the Second World War, Volume 2: Germany's Initial 
Conquests in Europe (pp. 227–326). Oxford: Oxford University Press. ISBN 
978-0-19-822885-1.
United States Army (1986) [1953]. The German Campaigns in the Balkans (Spring 
1941). Washington, DC: Department of the Army.
Waltz, Susan (2002). "Reclaiming and Rebuilding the History of the Universal 
Declaration of Human Rights". Third World Quarterly. 23 (3): 437–48. 
doi:10.1080/01436590220138378. JSTOR 3993535.
Ward, Thomas A. (2010). Aerospace Propulsion Systems. Singapore: John Wiley & 
Sons. ISBN 978-0-470-82497-9.
Watson, William E. (2003). Tricolor and Crescent: France and the Islamic World. 
Westport, CT: Praeger. ISBN 978-0-275-97470-1.
Weinberg, Gerhard L. (2005). A World at Arms: A Global History of World War II 
(2nd ed.). Cambridge: Cambridge University Press. ISBN 978-0-521-85316-3.; 
comprehensive overview with emphasis on diplomacy
Wettig, Gerhard (2008). Stalin and the Cold War in Europe: The Emergence and 
Development of East-West Conflict, 1939–1953. Lanham, MD: Rowman & 
Littlefield. ISBN 978-0-7425-5542-6.
Wiest, Andrew; Barbier, M.K. (2002). Strategy and Tactics: Infantry Warfare. St 
Paul, MN: MBI Publishing Company. ISBN 978-0-7603-1401-2.
Williams, Andrew (2006). Liberalism and War: The Victors and the Vanquished. 
Abingdon & New York: Routledge. ISBN 978-0-415-35980-1.
Wilt, Alan F. (1981). "Hitler's Late Summer Pause in 1941". Military Affairs. 
45 (4): 187–91. doi:10.2307/1987464. JSTOR 1987464.
Wohlstetter, Roberta (1962). Pearl Harbor: Warning and Decision. Palo Alto, CA: 
Stanford University Press. ISBN 978-0-8047-0597-4.
Wolf, Holger C. (1993). "The Lucky Miracle: Germany 1945–1951". In Rudiger 
Dornbusch, Wilhelm Nölling and Richard Layard, eds., Postwar Economic 
Reconstruction and Lessons for the East Today (pp. 29–56). Cambridge: MIT 
Press. ISBN 978-0-262-04136-2.
Wood, James B. (2007). Japanese Military Strategy in the Pacific War: Was 
Defeat Inevitable?. Lanham, MD: Rowman & Littlefield. ISBN 978-0-7425-5339-2.
Yoder, Amos (1997). The Evolution of the United Nations System (3rd ed.). 
London & Washington, DC: Taylor & Francis. ISBN 978-1-56032-546-8.
Zalampas, Michael (1989). Adolf Hitler and the Third Reich in American 
magazines, 1923–1939. Bowling Green University Popular Press. ISBN 
978-0-87972-462-7.
Zaloga, Steven J. (1996). Bagration 1944: The Destruction of Army Group Centre. 
Oxford: Osprey Publishing. ISBN 978-1-85532-478-7.
——— (2002). Poland 1939: The Birth of Blitzkrieg. Oxford: Osprey 
Publishing. ISBN 978-1-84176-408-5.
Zeiler, Thomas W. (2004). Unconditional Defeat: Japan, America, and the End of 
World War II. Wilmington, DE: Scholarly Resources. ISBN 978-0-8420-2991-9.
Zetterling, Niklas; Tamelander, Michael (2009). Bismarck: The Final Days of 
Germany's Greatest Battleship. Drexel Hill, PA: Casemate. ISBN 
978-1-935149-04-0.



Aircraft
From Wikipedia, the free encyclopedia
Jump to navigationJump to search

The Mil Mi-8 is the most-produced helicopter in history.

The Cessna 172 Skyhawk is the most produced aircraft in history.
An aircraft is a vehicle that is able to fly by gaining support from the air. 
It counters the force of gravity by using either static lift or by using the 
dynamic lift of an airfoil,[1] or in a few cases the downward thrust from jet 
engines. Common examples of aircraft include airplanes, helicopters, airships 
(including blimps), gliders, paramotors and hot air balloons.[2]

The human activity that surrounds aircraft is called aviation. The science of 
aviation, including designing and building aircraft, is called aeronautics. 
Crewed aircraft are flown by an onboard pilot, but unmanned aerial vehicles may 
be remotely controlled or self-controlled by onboard computers. Aircraft may be 
classified by different criteria, such as lift type, aircraft propulsion, usage 
and others.


Contents
1	History
2	Methods of lift
2.1	Lighter than air – aerostats
2.2	Heavier-than-air – aerodynes
2.2.1	Fixed-wing
2.2.2	Rotorcraft
2.2.3	Other methods of lift
3	Scale, sizes and speeds
3.1	Sizes
3.2	Speeds
4	Propulsion
4.1	Unpowered aircraft
4.2	Powered aircraft
4.2.1	Propeller aircraft
4.2.2	Jet aircraft
4.2.3	Rotorcraft
4.2.4	Other types of powered aircraft
5	Design and construction
5.1	Structure
5.1.1	Aerostats
5.1.2	Aerodynes
5.2	Avionics
6	Flight characteristics
6.1	Flight envelope
6.2	Range
6.3	Flight dynamics
6.3.1	Stability
6.3.2	Control
7	Impacts of aircraft use
8	Uses for aircraft
8.1	Military
8.2	Civil
8.3	Experimental
8.4	Model
9	See also
9.1	Lists
9.2	Topics
10	References
11	External links
History
Main article: History of aviation
See also: Timeline of aviation
Flying model craft and stories of manned flight go back many centuries; 
however, the first manned ascent — and safe descent — in modern times took 
place by larger hot-air balloons developed in the 18th century. Each of the two 
World Wars led to great technical advances. Consequently, the history of 
aircraft can be divided into five eras:

Pioneers of flight, from the earliest experiments to 1914.
First World War, 1914 to 1918.
Aviation between the World Wars, 1918 to 1939.
Second World War, 1939 to 1945.
Postwar era, also called the Jet Age, 1945 to the present day.
Methods of lift
Lighter than air – aerostats
Main article: Aerostat

Hot air balloons
Aerostats use buoyancy to float in the air in much the same way that ships 
float on the water. They are characterized by one or more large cells or 
canopies, filled with a relatively low-density gas such as helium, hydrogen, or 
hot air, which is less dense than the surrounding air. When the weight of this 
is added to the weight of the aircraft structure, it adds up to the same weight 
as the air that the craft displaces.

Small hot-air balloons, called sky lanterns, were first invented in ancient 
China prior to the 3rd century BC and used primarily in cultural celebrations, 
and were only the second type of aircraft to fly, the first being kites, which 
were first invented in ancient China over two thousand years ago. (See Han 
Dynasty)


Airship USS Akron over Manhattan in the 1930s
A balloon was originally any aerostat, while the term airship was used for 
large, powered aircraft designs — usually fixed-wing.[3][4][5][6][7][8] In 
1919 Frederick Handley Page was reported as referring to "ships of the air," 
with smaller passenger types as "Air yachts."[9] In the 1930s, large 
intercontinental flying boats were also sometimes referred to as "ships of the 
air" or "flying-ships".[10][11] — though none had yet been built. The advent 
of powered balloons, called dirigible balloons, and later of rigid hulls 
allowing a great increase in size, began to change the way these words were 
used. Huge powered aerostats, characterized by a rigid outer framework and 
separate aerodynamic skin surrounding the gas bags, were produced, the 
Zeppelins being the largest and most famous. There were still no fixed-wing 
aircraft or non-rigid balloons large enough to be called airships, so "airship" 
came to be synonymous with these aircraft. Then several accidents, such as the 
Hindenburg disaster in 1937, led to the demise of these airships. Nowadays a 
"balloon" is an unpowered aerostat and an "airship" is a powered one.

A powered, steerable aerostat is called a dirigible. Sometimes this term is 
applied only to non-rigid balloons, and sometimes dirigible balloon is regarded 
as the definition of an airship (which may then be rigid or non-rigid). 
Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with 
stabilizing fins at the back. These soon became known as blimps. During World 
War II, this shape was widely adopted for tethered balloons; in windy weather, 
this both reduces the strain on the tether and stabilizes the balloon. The 
nickname blimp was adopted along with the shape. In modern times, any small 
dirigible or airship is called a blimp, though a blimp may be unpowered as well 
as powered.

Heavier-than-air – aerodynes
Heavier-than-air aircraft, such as airplanes, must find some way to push air or 
gas downwards, so that a reaction occurs (by Newton's laws of motion) to push 
the aircraft upwards. This dynamic movement through the air is the origin of 
the term aerodyne. There are two ways to produce dynamic upthrust — 
aerodynamic lift, and powered lift in the form of engine thrust.

Aerodynamic lift involving wings is the most common, with fixed-wing aircraft 
being kept in the air by the forward movement of wings, and rotorcraft by 
spinning wing-shaped rotors sometimes called rotary wings. A wing is a flat, 
horizontal surface, usually shaped in cross-section as an aerofoil. To fly, air 
must flow over the wing and generate lift. A flexible wing is a wing made of 
fabric or thin sheet material, often stretched over a rigid frame. A kite is 
tethered to the ground and relies on the speed of the wind over its wings, 
which may be flexible or rigid, fixed, or rotary.

With powered lift, the aircraft directs its engine thrust vertically downward. 
V/STOL aircraft, such as the Harrier Jump Jet and Lockheed Martin F-35B take 
off and land vertically using powered lift and transfer to aerodynamic lift in 
steady flight.

A pure rocket is not usually regarded as an aerodyne, because it does not 
depend on the air for its lift (and can even fly into space); however, many 
aerodynamic lift vehicles have been powered or assisted by rocket motors. 
Rocket-powered missiles that obtain aerodynamic lift at very high speed due to 
airflow over their bodies are a marginal case.

Fixed-wing
Main article: fixed-wing aircraft

An Airbus A380, the world's largest passenger airliner
The forerunner of the fixed-wing aircraft is the kite. Whereas a fixed-wing 
aircraft relies on its forward speed to create airflow over the wings, a kite 
is tethered to the ground and relies on the wind blowing over its wings to 
provide lift. Kites were the first kind of aircraft to fly, and were invented 
in China around 500 BC. Much aerodynamic research was done with kites before 
test aircraft, wind tunnels, and computer modelling programs became available.

The first heavier-than-air craft capable of controlled free-flight were 
gliders. A glider designed by George Cayley carried out the first true manned, 
controlled flight in 1853.

The practical, powered, fixed-wing aircraft (the airplane or aeroplane) was 
invented by Wilbur and Orville Wright. Besides the method of propulsion, 
fixed-wing aircraft are in general characterized by their wing configuration. 
The most important wing characteristics are:

Number of wings — monoplane, biplane, etc.
Wing support — Braced or cantilever, rigid, or flexible.
Wing planform — including aspect ratio, angle of sweep, and any variations 
along the span (including the important class of delta wings).
Location of the horizontal stabilizer, if any.
Dihedral angle — positive, zero, or negative (anhedral).
A variable geometry aircraft can change its wing configuration during flight.

A flying wing has no fuselage, though it may have small blisters or pods. The 
opposite of this is a lifting body, which has no wings, though it may have 
small stabilizing and control surfaces.

Wing-in-ground-effect vehicles are not considered aircraft. They "fly" 
efficiently close to the surface of the ground or water, like conventional 
aircraft during takeoff. An example is the Russian ekranoplan (nicknamed the 
"Caspian Sea Monster"). Man-powered aircraft also rely on ground effect to 
remain airborne with a minimal pilot power, but this is only because they are 
so underpowered—in fact, the airframe is capable of flying higher.


Aircraft parked on the ground in Afghanistan
Rotorcraft
Main article: Rotorcraft

An autogyro
Rotorcraft, or rotary-wing aircraft, use a spinning rotor with aerofoil section 
blades (a rotary wing) to provide lift. Types include helicopters, autogyros, 
and various hybrids such as gyrodynes and compound rotorcraft.

Helicopters have a rotor turned by an engine-driven shaft. The rotor pushes air 
downward to create lift. By tilting the rotor forward, the downward flow is 
tilted backward, producing thrust for forward flight. Some helicopters have 
more than one rotor and a few have rotors turned by gas jets at the tips.

Autogyros have unpowered rotors, with a separate power plant to provide thrust. 
The rotor is tilted backward. As the autogyro moves forward, air blows upward 
across the rotor, making it spin. This spinning increases the speed of airflow 
over the rotor, to provide lift. Rotor kites are unpowered autogyros, which are 
towed to give them forward speed or tethered to a static anchor in high-wind 
for kited flight.

Cyclogyros rotate their wings about a horizontal axis.

Compound rotorcraft have wings that provide some or all of the lift in forward 
flight. They are nowadays classified as powered lift types and not as 
rotorcraft. Tiltrotor aircraft (such as the Bell Boeing V-22 Osprey), tiltwing, 
tail-sitter, and coleopter aircraft have their rotors/propellers horizontal for 
vertical flight and vertical for forward flight.

Other methods of lift

X-24B lifting body.
A lifting body is an aircraft body shaped to produce lift. If there are any 
wings, they are too small to provide significant lift and are used only for 
stability and control. Lifting bodies are not efficient: they suffer from high 
drag, and must also travel at high speed to generate enough lift to fly. Many 
of the research prototypes, such as the Martin Marietta X-24, which led up to 
the Space Shuttle, were lifting bodies, though the Space Shuttle is not, and 
some supersonic missiles obtain lift from the airflow over a tubular body.
Powered lift types rely on engine-derived lift for vertical takeoff and landing 
(VTOL). Most types transition to fixed-wing lift for horizontal flight. Classes 
of powered lift types include VTOL jet aircraft (such as the Harrier Jump Jet) 
and tiltrotors, such as the Bell Boeing V-22 Osprey, among others. A few 
experimental designs rely entirely on engine thrust to provide lift throughout 
the whole flight, including personal fan-lift hover platforms and jetpacks. 
VTOL research designs include the Rolls-Royce Thrust Measuring Rig.
The Flettner airplane uses a rotating cylinder in place of a fixed wing, 
obtaining lift from the Magnus effect.
The ornithopter obtains thrust by flapping its wings.
Scale, sizes and speeds
Sizes
The smallest aircraft are toys/recreational items, and even smaller, 
nano-aircraft.

The largest aircraft by dimensions and volume (as of 2016) is the 302-foot-long 
(about 95 meters) British Airlander 10, a hybrid blimp, with helicopter and 
fixed-wing features, and reportedly capable of speeds up to 90 mph (about 150 
km/h), and an airborne endurance of two weeks with a payload of up to 22,050 
pounds (11 tons).[12][13][14]

The largest aircraft by weight and largest regular fixed-wing aircraft ever 
built, as of 2016, is the Antonov An-225 Mriya. That Ukrainian-built six-engine 
Russian transport of the 1980s is 84 meters (276 feet) long, with an 88-meter 
(289 foot) wingspan. It holds the world payload record, after transporting 
428,834 pounds (200 tons) of goods, and has recently flown 100-ton loads 
commercially. Weighing in at somewhere between 1.1 and 1.4 million pounds 
(550–700 tons) maximum loaded weight, it is also the heaviest aircraft to be 
built, to date. It can cruise at 500 mph.[15][16][17][18][19]

The largest military airplanes are the Ukrainian/Russian Antonov An-124 Ruslan 
(world's second-largest airplane, also used as a civilian transport),[20] and 
American Lockheed C-5 Galaxy transport, weighing, loaded, over 765,000 pounds 
(over 380 tons).[19][21] The 8-engine, piston/propeller Hughes H-4 Hercules 
"Spruce Goose" — an American World War II wooden flying boat transport with a 
greater wingspan (94 meters / 260 feet) than any current aircraft and a tail 
height equal to the tallest (Airbus A380-800 at 24.1 meters / 78 feet) — flew 
only one short hop in the late 1940s and never flew out of ground effect.[19]

The largest civilian airplanes, apart from the above-noted An-225 and An-124, 
are the Airbus Beluga cargo transport derivative of the Airbus A300 jet 
airliner, the Boeing Dreamlifter cargo transport derivative of the Boeing 747 
jet airliner/transport (the 747-200B was, at its creation in the 1960s, the 
heaviest aircraft ever built, with a maximum weight of 836,000 pounds (over 400 
tons)),[21] and the double-decker Airbus A380 "super-jumbo" jet airliner (the 
world's largest passenger airliner).[19][22]

Main article: List of large aircraft
Speeds
The fastest recorded powered aircraft flight and fastest recorded aircraft 
flight of an air-breathing powered aircraft was of the NASA X-43A Pegasus, a 
scramjet-powered, hypersonic, lifting body experimental research aircraft, at 
Mach 9.6 (nearly 7,000 mph). The X-43A set that new mark, and broke its own 
world record of Mach 6.3, nearly 5,000 mph, set in March 2004, on its third and 
final flight on 16 November 2004.[23][24]

Prior to the X-43A, the fastest recorded powered airplane flight (and still the 
record for the fastest manned, powered airplane / fastest manned, 
non-spacecraft aircraft) was of the North American X-15A-2, rocket-powered 
airplane at 4,520 mph (7,274 km/h), Mach 6.72, on 3 October 1967. On one flight 
it reached an altitude of 354,300 feet.[25][26][27]

The fastest known, production aircraft (other than rockets and missiles) 
currently or formerly operational (as of 2016) are:

The fastest fixed-wing aircraft, and fastest glider, is the Space Shuttle, a 
rocket-glider hybrid, which has re-entered the atmosphere as a fixed-wing 
glider at more than Mach 25 — over 25 times the speed of sound, about 17,000 
mph at re-entry to Earth's atmosphere.[25][28]
The fastest military airplane ever built: Lockheed SR-71 Blackbird, a U.S. 
reconnaissance jet fixed-wing aircraft, known to fly beyond Mach 3.3 (about 
2,200 mph at cruising altitude). On 28 July 1976, an SR-71 set the record for 
the fastest and highest-flying operational aircraft with an absolute speed 
record of 2,193 mph and an absolute altitude record of 85,068 feet. At its 
retirement in the January 1990, it was the fastest air-breathing aircraft / 
fastest jet aircraft in the world, a record still standing as of August 
2016.[25][29][30][31][32][33]
Note: Some sources refer to the above-mentioned X-15 as the "fastest military 
airplane" because it was partly a project of the U.S. Navy and Air Force; 
however, the X-15 was not used in non-experimental actual military 
operations.[27]
The fastest current military aircraft are the Soviet/Russian Mikoyan-Gurevich 
MiG-25 — capable of Mach 3.2 (2,170 mph), at the expense of engine damage, or 
Mach 2.83 (1,920 mph) normally — and the Russian Mikoyan MiG-31E (also 
capable of Mach 2.83 normally). Both are fighter-interceptor jet airplanes, in 
active operations as of 2016.[34][35][36]
The fastest civilian airplane ever built, and fastest passenger airliner ever 
built: the briefly operated Tupolev Tu-144 supersonic jet airliner (Mach 2.35, 
1,600 mph, 2,587 km/h), which was believed to cruise at about Mach 2.2. The 
Tu-144 (officially operated from 1968 to 1978, ending after two crashes of the 
small fleet) was outlived by its rival, the Concorde (Mach 2.23), a 
French/British supersonic airliner, known to cruise at Mach 2.02 (1.450 mph, 
2,333 kmh at cruising altitude), operating from 1976 until the small Concorde 
fleet was grounded permanently in 2003, following the crash of one in the early 
2000s.[25][27][37][38]
The fastest civilian airplane currently flying: the Cessna Citation X, an 
American business jet, capable of Mach 0.935 (over 600 mph at cruising 
altitude). Its rival, the American Gulfstream G650 business jet, can reach Mach 
0.925[25][27][39][40]
The fastest airliner currently flying is the Boeing 747, quoted as being 
capable of cruising over Mach 0.885 (over 550 mph). Previously, the fastest 
were the troubled, short-lived Russian (Soviet Union) Tupolev Tu-144 SST (Mach 
2.35) and the French/British Concorde (Mach 2.23, normally cruising at Mach 2) 
.[25][37][38] Before them, the Convair 990 Coronado jet airliner of the 1960s 
flew at over 600 mph.
Main article: Flight airspeed record
Propulsion
Unpowered aircraft
Main article: Unpowered aircraft
Gliders are heavier-than-air aircraft that do not employ propulsion once 
airborne. Take-off may be by launching forward and downward from a high 
location, or by pulling into the air on a tow-line, either by a ground-based 
winch or vehicle, or by a powered "tug" aircraft. For a glider to maintain its 
forward air speed and lift, it must descend in relation to the air (but not 
necessarily in relation to the ground). Many gliders can "soar", i.e., gain 
height from updrafts such as thermal currents. The first practical, 
controllable example was designed and built by the British scientist and 
pioneer George Cayley, whom many recognise as the first aeronautical engineer. 
Common examples of gliders are sailplanes, hang gliders and paragliders.

Balloons drift with the wind, though normally the pilot can control the 
altitude, either by heating the air or by releasing ballast, giving some 
directional control (since the wind direction changes with altitude). A 
wing-shaped hybrid balloon can glide directionally when rising or falling; but 
a spherically shaped balloon does not have such directional control.

Kites are aircraft[41] that are tethered to the ground or other object (fixed 
or mobile) that maintains tension in the tether or kite line; they rely on 
virtual or real wind blowing over and under them to generate lift and drag. 
Kytoons are balloon-kite hybrids that are shaped and tethered to obtain kiting 
deflections, and can be lighter-than-air, neutrally buoyant, or 
heavier-than-air.

Powered aircraft
Main article: Powered aircraft
Powered aircraft have one or more onboard sources of mechanical power, 
typically aircraft engines although rubber and manpower have also been used. 
Most aircraft engines are either lightweight reciprocating engines or gas 
turbines. Engine fuel is stored in tanks, usually in the wings but larger 
aircraft also have additional fuel tanks in the fuselage.

Propeller aircraft
Main article: Powered aircraft § Propeller aircraft

A turboprop-engined DeHavilland Twin Otter adapted as a floatplane
Propeller aircraft use one or more propellers (airscrews) to create thrust in a 
forward direction. The propeller is usually mounted in front of the power 
source in tractor configuration but can be mounted behind in pusher 
configuration. Variations of propeller layout include contra-rotating 
propellers and ducted fans.

Many kinds of power plant have been used to drive propellers. Early airships 
used man power or steam engines. The more practical internal combustion piston 
engine was used for virtually all fixed-wing aircraft until World War II and is 
still used in many smaller aircraft. Some types use turbine engines to drive a 
propeller in the form of a turboprop or propfan. Human-powered flight has been 
achieved, but has not become a practical means of transport. Unmanned aircraft 
and models have also used power sources such as electric motors and rubber 
bands.

Jet aircraft
Main article: Jet aircraft

Lockheed Martin F-22A Raptor
Jet aircraft use airbreathing jet engines, which take in air, burn fuel with it 
in a combustion chamber, and accelerate the exhaust rearwards to provide 
thrust.

Turbojet and turbofan engines use a spinning turbine to drive one or more fans, 
which provide additional thrust. An afterburner may be used to inject extra 
fuel into the hot exhaust, especially on military "fast jets". Use of a turbine 
is not absolutely necessary: other designs include the pulsejet and ramjet. 
These mechanically simple designs cannot work when stationary, so the aircraft 
must be launched to flying speed by some other method. Other variants have also 
been used, including the motorjet and hybrids such as the Pratt & Whitney J58, 
which can convert between turbojet and ramjet operation.

Compared to propellers, jet engines can provide much higher thrust, higher 
speeds and, above about 40,000 ft (12,000 m), greater efficiency.[42] They are 
also much more fuel-efficient than rockets. As a consequence nearly all large, 
high-speed or high-altitude aircraft use jet engines.

Rotorcraft
Main article: Rotorcraft
Some rotorcraft, such as helicopters, have a powered rotary wing or rotor, 
where the rotor disc can be angled slightly forward so that a proportion of its 
lift is directed forwards. The rotor may, like a propeller, be powered by a 
variety of methods such as a piston engine or turbine. Experiments have also 
used jet nozzles at the rotor blade tips.

Other types of powered aircraft
Rocket-powered aircraft have occasionally been experimented with, and the 
Messerschmitt Me 163 Komet fighter even saw action in the Second World War. 
Since then, they have been restricted to research aircraft, such as the North 
American X-15, which traveled up into space where air-breathing engines cannot 
work (rockets carry their own oxidant). Rockets have more often been used as a 
supplement to the main power plant, typically for the rocket-assisted take off 
of heavily loaded aircraft, but also to provide high-speed dash capability in 
some hybrid designs such as the Saunders-Roe SR.53.
The ornithopter obtains thrust by flapping its wings. It has found practical 
use in a model hawk used to freeze prey animals into stillness so that they can 
be captured, and in toy birds.
Design and construction
Aircraft are designed according to many factors such as customer and 
manufacturer demand, safety protocols and physical and economic constraints. 
For many types of aircraft the design process is regulated by national 
airworthiness authorities.

The key parts of an aircraft are generally divided into three categories:

The structure comprises the main load-bearing elements and associated 
equipment.
The propulsion system (if it is powered) comprises the power source and 
associated equipment, as described above.
The avionics comprise the control, navigation and communication systems, 
usually electrical in nature.
Structure
The approach to structural design varies widely between different types of 
aircraft. Some, such as paragliders, comprise only flexible materials that act 
in tension and rely on aerodynamic pressure to hold their shape. A balloon 
similarly relies on internal gas pressure, but may have a rigid basket or 
gondola slung below it to carry its payload. Early aircraft, including 
airships, often employed flexible doped aircraft fabric covering to give a 
reasonably smooth aeroshell stretched over a rigid frame. Later aircraft 
employed semi-monocoque techniques, where the skin of the aircraft is stiff 
enough to share much of the flight loads. In a true monocoque design there is 
no internal structure left.

The key structural parts of an aircraft depend on what type it is.

Aerostats
Main article: Aerostat
Lighter-than-air types are characterised by one or more gasbags, typically with 
a supporting structure of flexible cables or a rigid framework called its hull. 
Other elements such as engines or a gondola may also be attached to the 
supporting structure.

Aerodynes

Airframe diagram for an AgustaWestland AW101 helicopter
Heavier-than-air types are characterised by one or more wings and a central 
fuselage. The fuselage typically also carries a tail or empennage for stability 
and control, and an undercarriage for takeoff and landing. Engines may be 
located on the fuselage or wings. On a fixed-wing aircraft the wings are 
rigidly attached to the fuselage, while on a rotorcraft the wings are attached 
to a rotating vertical shaft. Smaller designs sometimes use flexible materials 
for part or all of the structure, held in place either by a rigid frame or by 
air pressure. The fixed parts of the structure comprise the airframe.

Avionics
Main article: Avionics
The avionics comprise the aircraft flight control systems and related 
equipment, including the cockpit instrumentation, navigation, radar, 
monitoring, and communications systems.

Flight characteristics
Flight envelope
Main article: Flight envelope
The flight envelope of an aircraft refers to its approved design capabilities 
in terms of airspeed, load factor and altitude.[43][44] The term can also refer 
to other assessments of aircraft performance such as maneuverability. When an 
aircraft is abused, for instance by diving it at too-high a speed, it is said 
to be flown outside the envelope, something considered foolhardy since it has 
been taken beyond the design limits which have been established by the 
manufacturer. Going beyond the envelope may have a known outcome such as 
flutter or entry to a non-recoverable spin (possible reasons for the boundary).

Range

The Boeing 777-200LR is one of the longest-range airliners, capable of flights 
of more than halfway around the world.
Main article: Range (aeronautics)
The range is the distance an aircraft can fly between takeoff and landing, as 
limited by the time it can remain airborne.

For a powered aircraft the time limit is determined by the fuel load and rate 
of consumption.

For an unpowered aircraft, the maximum flight time is limited by factors such 
as weather conditions and pilot endurance. Many aircraft types are restricted 
to daylight hours, while balloons are limited by their supply of lifting gas. 
The range can be seen as the average ground speed multiplied by the maximum 
time in the air.

The Airbus A350 is now the longest range airliner.

Flight dynamics
Main article: Flight dynamics (fixed-wing aircraft)
Flight dynamics with text.png
Flight dynamics is the science of air vehicle orientation and control in three 
dimensions. The three critical flight dynamics parameters are the angles of 
rotation around three axes which pass through the vehicle's center of gravity, 
known as pitch, roll, and yaw.

Roll is a rotation about the longitudinal axis (equivalent to the rolling or 
heeling of a ship) giving an up-down movement of the wing tips measured by the 
roll or bank angle.
Pitch is a rotation about the sideways horizontal axis giving an up-down 
movement of the aircraft nose measured by the angle of attack.
Yaw is a rotation about the vertical axis giving a side-to-side movement of the 
nose known as sideslip.
Flight dynamics is concerned with the stability and control of an aircraft's 
rotation about each of these axes.

Stability

The empennage of a Boeing 747-200
An aircraft that is unstable tends to diverge from its intended flight path and 
so is difficult to fly. A very stable aircraft tends to stay on its flight path 
and is difficult to maneuver. Therefore, it is important for any design to 
achieve the desired degree of stability. Since the widespread use of digital 
computers, it is increasingly common for designs to be inherently unstable and 
rely on computerised control systems to provide artificial stability.

A fixed wing is typically unstable in pitch, roll, and yaw. Pitch and yaw 
stabilities of conventional fixed wing designs require horizontal and vertical 
stabilisers,[45][46] which act similarly to the feathers on an arrow.[47] These 
stabilizing surfaces allow equilibrium of aerodynamic forces and to stabilise 
the flight dynamics of pitch and yaw.[45][46] They are usually mounted on the 
tail section (empennage), although in the canard layout, the main aft wing 
replaces the canard foreplane as pitch stabilizer. Tandem wing and tailless 
aircraft rely on the same general rule to achieve stability, the aft surface 
being the stabilising one.

A rotary wing is typically unstable in yaw, requiring a vertical stabiliser.

A balloon is typically very stable in pitch and roll due to the way the payload 
is slung underneath the center of lift.

Control
Flight control surfaces enable the pilot to control an aircraft's flight 
attitude and are usually part of the wing or mounted on, or integral with, the 
associated stabilizing surface. Their development was a critical advance in the 
history of aircraft, which had until that point been uncontrollable in flight.

Aerospace engineers develop control systems for a vehicle's orientation 
(attitude) about its center of mass. The control systems include actuators, 
which exert forces in various directions, and generate rotational forces or 
moments about the aerodynamic center of the aircraft, and thus rotate the 
aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical 
force applied at a distance forward or aft from the aerodynamic center of the 
aircraft, causing the aircraft to pitch up or down. Control systems are also 
sometimes used to increase or decrease drag, for example to slow the aircraft 
to a safe speed for landing.

The two main aerodynamic forces acting on any aircraft are lift supporting it 
in the air and drag opposing its motion. Control surfaces or other techniques 
may also be used to affect these forces directly, without inducing any 
rotation.

Impacts of aircraft use
Main article: Environmental impact of aviation
Aircraft permit long distance, high speed travel and may be a more fuel 
efficient mode of transportation in some circumstances. Aircraft have 
environmental and climate impacts beyond fuel efficiency considerations, 
however. They are also relatively noisy compared to other forms of travel and 
high altitude aircraft generate contrails, which experimental evidence suggests 
may alter weather patterns.

Uses for aircraft
Aircraft are produced in several different types optimized for various uses; 
military aircraft, which includes not just combat types but many types of 
supporting aircraft, and civil aircraft, which include all non-military types, 
experimental and model.

Military

Boeing B-17E in flight
Main article: Military aircraft
A military aircraft is any aircraft that is operated by a legal or 
insurrectionary armed service of any type.[48] Military aircraft can be either 
combat or non-combat:

Combat aircraft are aircraft designed to destroy enemy equipment using its own 
armament.[48] Combat aircraft divide broadly into fighters and bombers, with 
several in-between types, such as fighter-bombers and attack aircraft, 
including attack helicopters.
Non-combat aircraft are not designed for combat as their primary function, but 
may carry weapons for self-defense. Non-combat roles include search and rescue, 
reconnaissance, observation, transport, training, and aerial refueling. These 
aircraft are often variants of civil aircraft.
Most military aircraft are powered heavier-than-air types. Other types, such as 
gliders and balloons, have also been used as military aircraft; for example, 
balloons were used for observation during the American Civil War and World War 
I, and military gliders were used during World War II to land troops.

Civil

Agusta A109 helicopter of the Swiss air rescue service
Main article: Civil aviation
Civil aircraft divide into commercial and general types, however there are some 
overlaps.

Commercial aircraft include types designed for scheduled and charter airline 
flights, carrying passengers, mail and other cargo. The larger 
passenger-carrying types are the airliners, the largest of which are wide-body 
aircraft. Some of the smaller types are also used in general aviation, and some 
of the larger types are used as VIP aircraft.

General aviation is a catch-all covering other kinds of private (where the 
pilot is not paid for time or expenses) and commercial use, and involving a 
wide range of aircraft types such as business jets (bizjets), trainers, 
homebuilt, gliders, warbirds and hot air balloons to name a few. The vast 
majority of aircraft today are general aviation types.

Experimental
Main article: Experimental aircraft
An experimental aircraft is one that has not been fully proven in flight, or 
that carries a Special Airworthiness Certificate, called an Experimental 
Certificate in United States parlance. This often implies that the aircraft is 
testing new aerospace technologies, though the term also refers to 
amateur-built and kit-built aircraft, many of which are based on proven 
designs.


A model aircraft, weighing six grams
Model
Main article: Model aircraft
A model aircraft is a small unmanned type made to fly for fun, for static 
display, for aerodynamic research or for other purposes. A scale model is a 
replica of some larger design.

See also
Lists
Early flying machines
Flight altitude record
List of aircraft
Flight altitude record
List of aviation, aerospace and aeronautical terms
List of civil aircraft
List of fighter aircraft
List of individual aircraft
List of large aircraft
Topics
Aircraft spotting
Air traffic control
Airport
Flying car
Personal air vehicle
Powered parachute
 	
Rocket
Spacecraft
Spaceplane
Steam aircraft
References
 "Aircraft — Define Aircraft at Dictionary.com". Dictionary.com. Archived 
from the original on 28 March 2015. Retrieved 1 April 2015.
 "Different Kinds & Types of Aircraft". www.wingsoverkansas.com. Archived from 
the original on 21 November 2016.
 US patent 467069 Archived 23 February 2014 at the Wayback Machine "Air-ship" 
referring to a compound aerostat/rotorcraft.
 Ezekiel Airship (1902) wright-brothers.org Archived 3 December 2013 at the 
Wayback Machinealtereddimensions.net Archived 22 February 2014 at the Wayback 
Machine "airship," – referring to an HTA aeroplane.
 The Bridgeport Herald, August 18, 1901 Archived 3 August 2013 at the Wayback 
Machine – "air ship" referring to Whitehead's aeroplane.
 Cooley Airship of 1910, also called the Cooley monoplane."Unbelievable Flying 
Objects". Archived from the original on 2 November 2013. Retrieved 10 February 
2014."Archived copy". Archived from the original on 2 April 2012. Retrieved 7 
September 2011. – a heavier-than-air monoplane.
 Frater, A.; The Balloon Factory, Picador (2009), p. 163. Wright brothers' 
"airship."
 George Griffith, The angel of the Revolution, 1893 Archived 22 February 2014 
at the Wayback Machine — "air-ship," "vessel" referring to a VTOL compound 
rotorcraft (not clear from the reference if it might be an aerostat hybrid.)
 Auckland Star, 24 February 1919 Archived 24 March 2014 at the Wayback Machine 
"Ships of the air," "Air yachts" – passenger landplanes large and small
 The Sydney Morning Herald, Monday 11 April 1938 – "ship of the airs," 
"flying-ship," referring to a large flying-boat.
 Smithsonian, America by air Archived 18 January 2014 at the Wayback Machine 
"Ships of the Air" referring to Pan Am's Boeing Clipper flying-boat fleet.
 "World's largest aircraft the Airlander makes maiden flight in UK," Archived 
22 November 2016 at the Wayback Machine 16 August 2016, London 'Daily 
Telegraph' via Telegraph.co.uk, retrieved 22 November 2016.
 ["Airlander 10, the world's largest aircraft, takes off for the first time,"] 
19 August 2016, CBS News(TV) retrieved 22 November 2016.
 Kottasova, Ivana "The world's largest aircraft crashes after 2nd test flight" 
Archived 22 November 2016 at the Wayback Machine, 24 August 2016, CNN Tech on 
CNN, the Cable News Network, retrieved 22 November 2016.
 July, Dyre. "Fly Drive Aanbiedingen". www.flydrivereizen.nl. Archived from the 
original on 4 November 2016.
 "Watch the world's biggest plane land in Australia," Archived 22 November 2016 
at the Wayback Machine 16 May 2016, Fox News, retrieved 22 November 2016.
 Rumbaugh, Andrea, "World's biggest airplane lands at Bush airport," Archived 
23 November 2016 at the Wayback Machine Updated 18 November 2016, Houston 
Chonicle / Chron.com, retrieved 22 November 2016.
 Lewis, Danny, "The World's Largest Aircraft Might Lose its Title to a Blimp,", 
18 September 2015, Smart News, Smithsonian.com, Smithsonian Institution, 
Washington, D.C., retrieved 22 November 2016.
 "Ask Us – Largest Plane in the World," Aerospaceweb.org, retrieved 22 
November 2016.
 "World's Second Largest Aircraft," Archived 22 November 2016 at the Wayback 
Machine 28 July 2013, NASA, retrieved 22 November 2016.
 Loftin, Laurence K., Jr., "Wide-Body Transports" Archived 7 June 2013 at the 
Wayback Machine, in Chapter 13, "Jet Transports," in Part II, "The Jet Age," in 
Quest for Performance: The Evolution of Modern Aircraft, NASA SP-468, 1985, 
Scientific and Technical Information Branch, NASA, Washington, D.C., Updated: 6 
August 2004, retrieved 22 November 2016.
 "Airbus reviews A380 schedule," Archived 2 February 2017 at the Wayback 
Machine 29 April 2008, The New York Times, retrieved 22 November 2016.
 "Hypersonic X-43A Takes Flight.htm," Archived 2 November 2016 at the Wayback 
Machine NASA retrieved November 2016.
 "Fastest aircraft, air-breathing engine," Archived 20 December 2016 at the 
Wayback Machine Guinness World Records, retrieved 2 December 2016.
 Jackson, Doug, "Ask Us – Aircraft Speed Records," 22 April 2001, 
Aerospaceweb.org, retrieved 22 November 2016.
 "Fastest speed in a non-spacecraft aircraft," Archived 20 December 2016 at the 
Wayback Machine Guinness World Records, retrieved 2 December 2016.
 Bergqvist, Pia, "Fastest Airplanes: Top Performers in Their Class," Archived 3 
September 2017 at the Wayback Machine 17 September 2014, Flying, retrieved 3 
December 2016
 Benson, Tom, ed., "Speed Regimes: Hypersonic Re-Entry," Archived 23 November 
2016 at the Wayback Machine Glenn Research Center, NASA, retrieved 22 November 
2016.
 "NASA Armstrong Fact Sheet: SR-71 Blackbird" Archived 23 November 2016 at the 
Wayback Machine NASA. Retrieved 22 November 2016
 "Lockheed SR-71A," Archived 20 December 2016 at the Wayback Machine display 
notes, 29 May 2015, National Museum of the United States Air Force retrieved 2 
December 2016
 Trujillo, Staff Sgt. Robert M.,"SR-71 Blackbird: Gone but not forgotten," 
Archived 20 December 2016 at the Wayback Machine 26 January 2016, 9th 
Reconnaissance Wing Public Affairs, U.S. Air Force, retrieved 2 December 2016
 "Absolute speed record still stands 40 years later," 27 July 2016 General 
Aviation News, retrieved 22 November 2016.
 Woolen, Angela, "SR-71 pilots, crew relive absolute speed record," Archived 20 
December 2016 at the Wayback Machine 9 August 2016, 78th Air Base Wing Public 
Affairs, United States Air Force, retrieved 2 December 2016
 Bender, Jeremy and Amanda Macias, "The 9 fastest piloted planes in the world," 
Archived 20 December 2016 at the Wayback Machine 18 September 2015, Business 
Insider, retrieved 3 December 2016
 "Fast and furious — the world's fastest military aircraft," Archived 20 
December 2016 at the Wayback Machine Airforce Technology, retrieved 3 December 
2016
 The Five Fastest Military Jets Ever Made"," Archived 6 August 2016 at the 
Wayback Machine 2016, Bloomberg, retrieved 3 December 2016
 "Ask Us – Fastest Airliner and Area Rule," Aerospaceweb.org, retrieved 22 
November 2016.
 "Fastest aircraft, airliner," Archived 20 December 2016 at the Wayback Machine 
Guinness World Records, retrieved 2 December. 2016.
 Whitfield, Bethany, "Cessna Citation Ten Chases Mach 0.935 Top Speed: Jet 
would retake speed prize from G650," Archived 20 July 2016 at the Wayback 
Machine 28 August 2012, Flying, retrieved 22 November 2016.
 "Cessna rolls out first production unit of new Citation X," Archived 22 
November 2016 at the Wayback Machine 15 April 2013, Wichita Business Journal, 
retrieved 22 November 2016.
 "Guided Tours of the BGA". nasa.gov. Archived from the original on 25 March 
2015. Retrieved 1 April 2015.
 "ch10-3". Hq.nasa.gov. Archived from the original on 14 September 2010. 
Retrieved 26 March 2010.
 "eCFR — Code of Federal Regulations". gpoaccess.gov. Archived from the 
original on 2 April 2012. Retrieved 1 April 2015.
 "Wayback Machine" (PDF). web.archive.org. 1 June 2010.
 Crane, Dale: Dictionary of Aeronautical Terms, third edition, p. 194. Aviation 
Supplies & Academics, 1997. ISBN 1-56027-287-2
 Aviation Publishers Co. Limited, From the Ground Up, p. 10 (27th revised 
edition) ISBN 0-9690054-9-0
 "Airline Handbook Chapter 5: How Aircraft Fly". Airline Handbook. Air 
Transport Association. Archived from the original on 20 June 2010.
 Gunston 1986, p. 274

 World Wide Web
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
"WWW" and "The Web" redirect here. For other uses of WWW, see WWW 
(disambiguation). For uses of web, see Web (disambiguation).
For the first web software, see WorldWideWeb.
Not to be confused with the Internet.

A web page can be displayed using a web browser. Web browsers often highlight 
and underline hypertext links and web pages can contain images.

A global map of the web index for countries in 2014
The World Wide Web (WWW), commonly known as the Web, is an information system 
where documents and other web resources are identified by Uniform Resource 
Locators (URLs, such as https://www.example.com/), which may be interlinked by 
hypertext, and are accessible over the Internet.[1][2] The resources of the WWW 
are transferred via the Hypertext Transfer Protocol (HTTP) and may be accessed 
by users by a software application called a web browser and are published by a 
software application called a web server.

English scientist Tim Berners-Lee invented the World Wide Web in 1989. He wrote 
the first web browser in 1990 while employed at CERN near Geneva, 
Switzerland.[3][4] The browser was released outside CERN in 1991, first to 
other research institutions starting in January 1991 and then to the general 
public in August 1991. The World Wide Web has been central to the development 
of the Information Age and is the primary tool billions of people use to 
interact on the Internet.[5][6][7][8][9]

Web resources may be any type of downloaded media, but web pages are hypertext 
media that have been formatted in Hypertext Markup Language (HTML).[10] Such 
formatting allows for embedded hyperlinks that contain URLs and permit users to 
navigate to other web resources. In addition to text, web pages may contain 
references to images, video, audio, and software components which are displayed 
in the user's web browser as coherent pages of multimedia content.

Multiple web resources with a common theme, a common domain name, or both, make 
up a website. Websites are stored in computers that are running a program 
called a web server that responds to requests made over the Internet from web 
browsers running on a user's computer. Website content can be largely provided 
by a publisher, or interactively where users contribute content or the content 
depends upon the users or their actions. Websites may be provided for a myriad 
of informative, entertainment, commercial, governmental, or non-governmental 
reasons.


Contents
1	History
2	Function
2.1	HTML
2.2	Linking
2.3	WWW prefix
2.4	Scheme specifiers
2.5	Pages
2.5.1	Static page
2.5.2	Dynamic pages
2.6	Website
2.7	Browser
2.8	Server
2.9	Cookie
2.10	Search engine
2.11	Deep web
2.12	Caching
3	Security
4	Privacy
5	Standards
6	Accessibility
7	Internationalisation
8	See also
9	References
10	Further reading
11	External links
History
Main article: History of the World Wide Web

The NeXT Computer used by Tim Berners-Lee at CERN.

The corridor where WWW was born. CERN, ground floor of building No.1
Tim Berners-Lee's vision of a global hyperlinked information system became a 
possibility by the second half of the 1980s.[11] By 1985, the global Internet 
began to proliferate in Europe and the Domain Name System (upon which the 
Uniform Resource Locator is built) came into being. In 1988 the first direct IP 
connection between Europe and North America was made and Berners-Lee began to 
openly discuss the possibility of a web-like system at CERN.[12]

While working at CERN, Berners-Lee became frustrated with the inefficiencies 
and difficulties posed by finding information stored on different 
computers.[13] On March 12, 1989, he submitted a memorandum, titled 
"Information Management: A Proposal",[14] to the management at CERN for a 
system called "Mesh" that referenced ENQUIRE, a database and software project 
he had built in 1980, which used the term "web" and described a more elaborate 
information management system based on links embedded as text: "Imagine, then, 
the references in this document all being associated with the network address 
of the thing to which they referred, so that while reading this document, you 
could skip to them with a click of the mouse." Such a system, he explained, 
could be referred to using one of the existing meanings of the word hypertext, 
a term that he says was coined in the 1950s. There is no reason, the proposal 
continues, why such hypertext links could not encompass multimedia documents 
including graphics, speech and video, so that Berners-Lee goes on to use the 
term hypermedia.[15]

With help from his colleague and fellow hypertext enthusiast Robert Cailliau he 
published a more formal proposal on 12 November 1990 to build a "Hypertext 
project" called "WorldWideWeb" (one word) as a "web" of "hypertext documents" 
to be viewed by "browsers" using a client–server architecture.[16] At this 
point HTML and HTTP had already been in development for about two months and 
the first Web server was about a month from completing its first successful 
test. This proposal estimated that a read-only web would be developed within 
three months and that it would take six months to achieve "the creation of new 
links and new material by readers, [so that] authorship becomes universal" as 
well as "the automatic notification of a reader when new material of interest 
to him/her has become available". While the read-only goal was met, accessible 
authorship of web content took longer to mature, with the wiki concept, WebDAV, 
blogs, Web 2.0 and RSS/Atom.[17]


The CERN data centre in 2010 housing some WWW servers
The proposal was modelled after the SGML reader Dynatext by Electronic Book 
Technology, a spin-off from the Institute for Research in Information and 
Scholarship at Brown University. The Dynatext system, licensed by CERN, was a 
key player in the extension of SGML ISO 8879:1986 to Hypermedia within HyTime, 
but it was considered too expensive and had an inappropriate licensing policy 
for use in the general high energy physics community, namely a fee for each 
document and each document alteration.[citation needed] A NeXT Computer was 
used by Berners-Lee as the world's first web server and also to write the first 
web browser in 1990. By Christmas 1990, Berners-Lee had built all the tools 
necessary for a working Web:[18] the first web browser (WorldWideWeb, which was 
a web editor as well) and the first web server. The first web site,[19] which 
described the project itself, was published on 20 December 1990.[20]

The first web page may be lost, but Paul Jones of UNC-Chapel Hill in North 
Carolina announced in May 2013 that Berners-Lee gave him what he says is the 
oldest known web page during a visit to UNC in 1991. Jones stored it on a 
magneto-optical drive and on his NeXT computer.[21] On 6 August 1991, 
Berners-Lee published a short summary of the World Wide Web project on the 
newsgroup alt.hypertext.[22] This date is sometimes confused with the public 
availability of the first web servers, which had occurred months earlier. As 
another example of such confusion, several news media reported that the first 
photo on the Web was published by Berners-Lee in 1992, an image of the CERN 
house band Les Horribles Cernettes taken by Silvano de Gennaro; Gennaro has 
disclaimed this story, writing that media were "totally distorting our words 
for the sake of cheap sensationalism".[23]

The first server outside Europe was installed in Dec 1991 at the Stanford 
Linear Accelerator Center (SLAC) in Palo Alto, California, to host the 
SPIRES-HEP database.[24][25][26][27] The underlying concept of hypertext 
originated in previous projects from the 1960s, such as the Hypertext Editing 
System (HES) at Brown University, Ted Nelson's Project Xanadu, and Douglas 
Engelbart's oN-Line System (NLS). Both Nelson and Engelbart were in turn 
inspired by Vannevar Bush's microfilm-based memex, which was described in the 
1945 essay "As We May Think".[28]

Berners-Lee's breakthrough was to marry hypertext to the Internet. In his book 
Weaving The Web, he explains that he had repeatedly suggested to members of 
both technical communities that a marriage between the two technologies was 
possible. But, when no one took up his invitation, he finally assumed the 
project himself. In the process, he developed three essential technologies:

a system of globally unique identifiers for resources on the Web and elsewhere, 
the universal document identifier (UDI), later known as uniform resource 
locator (URL) and uniform resource identifier (URI);
the publishing language Hypertext Markup Language (HTML);
the Hypertext Transfer Protocol (HTTP).[29]
The World Wide Web had several differences from other hypertext systems 
available at the time. The Web required only unidirectional links rather than 
bidirectional ones, making it possible for someone to link to another resource 
without action by the owner of that resource. It also significantly reduced the 
difficulty of implementing web servers and browsers (in comparison to earlier 
systems), but in turn presented the chronic problem of link rot. Unlike 
predecessors such as HyperCard, the World Wide Web was non-proprietary, making 
it possible to develop servers and clients independently and to add extensions 
without licensing restrictions. On 30 April 1993, CERN announced that the World 
Wide Web would be free to anyone, with no fees due.[30] Coming two months after 
the announcement that the server implementation of the Gopher protocol was no 
longer free to use, this produced a rapid shift away from Gopher and toward the 
Web. An early popular web browser was ViolaWWW for Unix and the X Window 
System.


Robert Cailliau, Jean-François Abramatic, and Tim Berners-Lee at the 10th 
anniversary of the World Wide Web Consortium.
Historians generally agree that a turning point for the Web began with the 1993 
introduction of Mosaic,[31][32] a graphical web browser developed at the 
National Center for Supercomputing Applications at the University of Illinois 
at Urbana–Champaign (NCSA-UIUC). The development was led by Marc Andreessen, 
while funding came from the US High-Performance Computing and Communications 
Initiative and the High Performance Computing Act of 1991, one of several 
computing developments initiated by US Senator Al Gore.[33] Prior to the 
release of Mosaic, graphics were not commonly mixed with text in web pages, and 
the Web was less popular than older protocols such as Gopher and Wide Area 
Information Servers (WAIS). Mosaic's graphical user interface allowed the Web 
to become by far the most popular protocol on the Internet. The World Wide Web 
Consortium (W3C) was founded by Tim Berners-Lee after he left the European 
Organization for Nuclear Research (CERN) in October 1994. It was founded at the 
Massachusetts Institute of Technology Laboratory for Computer Science (MIT/LCS) 
with support from the Defense Advanced Research Projects Agency (DARPA), which 
had pioneered the Internet; a year later, a second site was founded at INRIA (a 
French national computer research lab) with support from the European 
Commission DG InfSo; and in 1996, a third continental site was created in Japan 
at Keio University. By the end of 1994, the total number of websites was still 
relatively small, but many notable websites were already active that 
foreshadowed or inspired today's most popular services.

Connected by the Internet, other websites were created around the world. This 
motivated international standards development for protocols and formatting. 
Berners-Lee continued to stay involved in guiding the development of web 
standards, such as the markup languages to compose web pages and he advocated 
his vision of a Semantic Web. The World Wide Web enabled the spread of 
information over the Internet through an easy-to-use and flexible format. It 
thus played an important role in popularising use of the Internet.[34] Although 
the two terms are sometimes conflated in popular use, World Wide Web is not 
synonymous with Internet.[35] The Web is an information space containing 
hyperlinked documents and other resources, identified by their URIs.[36] It is 
implemented as both client and server software using Internet protocols such as 
TCP/IP and HTTP.

Berners-Lee was knighted in 2004 by Queen Elizabeth II for "services to the 
global development of the Internet".[37][38] He never patented his invention.

Function
Main articles: HTTP and HTML

The World Wide Web functions as an application layer protocol that is run "on 
top of" (figuratively) the Internet, helping to make it more functional. The 
advent of the Mosaic web browser helped to make the web much more usable, to 
include the display of images and moving images (GIFs).
The terms Internet and World Wide Web are often used without much distinction. 
However, the two terms do not mean the same thing. The Internet is a global 
system of interconnected computer networks. In contrast, the World Wide Web is 
a global collection of documents and other resources, linked by hyperlinks and 
URIs. Web resources are accessed using HTTP or HTTPS, which are 
application-level Internet protocols that use the Internet's transport 
protocols.[39]

Viewing a web page on the World Wide Web normally begins either by typing the 
URL of the page into a web browser, or by following a hyperlink to that page or 
resource. The web browser then initiates a series of background communication 
messages to fetch and display the requested page. In the 1990s, using a browser 
to view web pages—and to move from one web page to another through 
hyperlinks—came to be known as 'browsing,' 'web surfing' (after channel 
surfing), or 'navigating the Web'. Early studies of this new behaviour 
investigated user patterns in using web browsers. One study, for example, found 
five user patterns: exploratory surfing, window surfing, evolved surfing, 
bounded navigation and targeted navigation.[40]

The following example demonstrates the functioning of a web browser when 
accessing a page at the URL http://www.example.org/home.html. The browser 
resolves the server name of the URL (www.example.org) into an Internet Protocol 
address using the globally distributed Domain Name System (DNS). This lookup 
returns an IP address such as 203.0.113.4 or 2001:db8:2e::7334. The browser 
then requests the resource by sending an HTTP request across the Internet to 
the computer at that address. It requests service from a specific TCP port 
number that is well known for the HTTP service, so that the receiving host can 
distinguish an HTTP request from other network protocols it may be servicing. 
The HTTP protocol normally uses port number 80 and for HTTPS protocol it 
normally uses port number 443. The content of the HTTP request can be as simple 
as two lines of text:

GET /home.html HTTP/1.1
Host: www.example.org
The computer receiving the HTTP request delivers it to web server software 
listening for requests on port 80. If the web server can fulfil the request it 
sends an HTTP response back to the browser indicating success:

HTTP/1.1 200 OK
Content-Type: text/html; charset=UTF-8
followed by the content of the requested page. HyperText Markup Language (HTML) 
for a basic web page might look like this:

<html>
  <head>
    <title>www.Example.org – The World Wide Web</title>
  </head>
  <body>
    <p>The World Wide Web, abbreviated as WWW and commonly known ...</p>
  </body>
</html>
The web browser parses the HTML and interprets the markup (<title>, <p> for 
paragraph, and such) that surrounds the words to format the text on the screen. 
Many web pages use HTML to reference the URLs of other resources such as 
images, other embedded media, scripts that affect page behaviour, and Cascading 
Style Sheets that affect page layout. The browser makes additional HTTP 
requests to the web server for these other Internet media types. As it receives 
their content from the web server, the browser progressively renders the page 
onto the screen as specified by its HTML and these additional resources.

HTML
Main article: HTML
Hypertext Markup Language (HTML) is the standard markup language for creating 
web pages and web applications. With Cascading Style Sheets (CSS) and 
JavaScript, it forms a triad of cornerstone technologies for the World Wide 
Web.[41]

Web browsers receive HTML documents from a web server or from local storage and 
render the documents into multimedia web pages. HTML describes the structure of 
a web page semantically and originally included cues for the appearance of the 
document.

HTML elements are the building blocks of HTML pages. With HTML constructs, 
images and other objects such as interactive forms may be embedded into the 
rendered page. HTML provides a means to create structured documents by denoting 
structural semantics for text such as headings, paragraphs, lists, links, 
quotes and other items. HTML elements are delineated by tags, written using 
angle brackets. Tags such as <img /> and <input /> directly introduce content 
into the page. Other tags such as <p> surround and provide information about 
document text and may include other tags as sub-elements. Browsers do not 
display the HTML tags, but use them to interpret the content of the page.

HTML can embed programs written in a scripting language such as JavaScript, 
which affects the behavior and content of web pages. Inclusion of CSS defines 
the look and layout of content. The World Wide Web Consortium (W3C), maintainer 
of both the HTML and the CSS standards, has encouraged the use of CSS over 
explicit presentational HTML since 1997.[42]

Linking
Most web pages contain hyperlinks to other related pages and perhaps to 
downloadable files, source documents, definitions and other web resources. In 
the underlying HTML, a hyperlink looks like this: <a 
href="http://www.example.org/home.html">www.Example.org Homepage</a>


Graphic representation of a minute fraction of the WWW, demonstrating 
hyperlinks
Such a collection of useful, related resources, interconnected via hypertext 
links is dubbed a web of information. Publication on the Internet created what 
Tim Berners-Lee first called the WorldWideWeb (in its original CamelCase, which 
was subsequently discarded) in November 1990.[16]

The hyperlink structure of the WWW is described by the webgraph: the nodes of 
the web graph correspond to the web pages (or URLs) the directed edges between 
them to the hyperlinks. Over time, many web resources pointed to by hyperlinks 
disappear, relocate, or are replaced with different content. This makes 
hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and 
the hyperlinks affected by it are often called dead links. The ephemeral nature 
of the Web has prompted many efforts to archive web sites. The Internet 
Archive, active since 1996, is the best known of such efforts.

WWW prefix
Many hostnames used for the World Wide Web begin with www because of the 
long-standing practice of naming Internet hosts according to the services they 
provide. The hostname of a web server is often www, in the same way that it may 
be ftp for an FTP server, and news or nntp for a Usenet news server. These host 
names appear as Domain Name System (DNS) or subdomain names, as in 
www.example.com. The use of www is not required by any technical or policy 
standard and many web sites do not use it; the first web server was 
nxoc01.cern.ch.[43] According to Paolo Palazzi,[44] who worked at CERN along 
with Tim Berners-Lee, the popular use of www as subdomain was accidental; the 
World Wide Web project page was intended to be published at www.cern.ch while 
info.cern.ch was intended to be the CERN home page, however the DNS records 
were never switched, and the practice of prepending www to an institution's 
website domain name was subsequently copied. Many established websites still 
use the prefix, or they employ other subdomain names such as www2, secure or en 
for special purposes. Many such web servers are set up so that both the main 
domain name (e.g., example.com) and the www subdomain (e.g., www.example.com) 
refer to the same site; others require one form or the other, or they may map 
to different web sites. The use of a subdomain name is useful for load 
balancing incoming web traffic by creating a CNAME record that points to a 
cluster of web servers. Since, currently, only a subdomain can be used in a 
CNAME, the same result cannot be achieved by using the bare domain 
root.[45][dubious – discuss]

When a user submits an incomplete domain name to a web browser in its address 
bar input field, some web browsers automatically try adding the prefix "www" to 
the beginning of it and possibly ".com", ".org" and ".net" at the end, 
depending on what might be missing. For example, entering 'microsoft' may be 
transformed to http://www.microsoft.com/ and 'openoffice' to 
http://www.openoffice.org. This feature started appearing in early versions of 
Firefox, when it still had the working title 'Firebird' in early 2003, from an 
earlier practice in browsers such as Lynx.[46][unreliable source?] It is 
reported that Microsoft was granted a US patent for the same idea in 2008, but 
only for mobile devices.[47]

In English, www is usually read as double-u double-u double-u.[48] Some users 
pronounce it dub-dub-dub, particularly in New Zealand. Stephen Fry, in his 
"Podgrams" series of podcasts, pronounces it wuh wuh wuh.[49] The English 
writer Douglas Adams once quipped in The Independent on Sunday (1999): "The 
World Wide Web is the only thing I know of whose shortened form takes three 
times longer to say than what it's short for".[50] In Mandarin Chinese, World 
Wide Web is commonly translated via a phono-semantic matching to wàn wéi 
wǎng (万维网), which satisfies www and literally means "myriad dimensional 
net",[51][better source needed] a translation that reflects the design concept 
and proliferation of the World Wide Web. Tim Berners-Lee's web-space states 
that World Wide Web is officially spelled as three separate words, each 
capitalised, with no intervening hyphens.[52] Use of the www prefix has been 
declining, especially when Web 2.0 web applications sought to brand their 
domain names and make them easily pronounceable.[53] As the mobile Web grew in 
popularity, services like Gmail.com, Outlook.com, Myspace.com, Facebook.com and 
Twitter.com are most often mentioned without adding "www." (or, indeed, ".com") 
to the domain.

Scheme specifiers
The scheme specifiers http:// and https:// at the start of a web URI refer to 
Hypertext Transfer Protocol or HTTP Secure, respectively. They specify the 
communication protocol to use for the request and response. The HTTP protocol 
is fundamental to the operation of the World Wide Web, and the added encryption 
layer in HTTPS is essential when browsers send or retrieve confidential data, 
such as passwords or banking information. Web browsers usually automatically 
prepend http:// to user-entered URIs, if omitted.

Pages
Main article: Web page

A screenshot of a web page on Wikimedia Commons
A web page (also written as webpage) is a document that is suitable for the 
World Wide Web and web browsers. A web browser displays a web page on a monitor 
or mobile device.

The term web page usually refers to what is visible, but may also refer to the 
contents of the computer file itself, which is usually a text file containing 
hypertext written in HTML or a comparable markup language. Typical web pages 
provide hypertext for browsing to other web pages via hyperlinks, often 
referred to as links. Web browsers will frequently have to access multiple web 
resource elements, such as reading style sheets, scripts, and images, while 
presenting each web page.

On a network, a web browser can retrieve a web page from a remote web server. 
The web server may restrict access to a private network such as a corporate 
intranet. The web browser uses the Hypertext Transfer Protocol (HTTP) to make 
such requests to the web server.

A static web page is delivered exactly as stored, as web content in the web 
server's file system. In contrast, a dynamic web page is generated by a web 
application, usually driven by server-side software. Dynamic web pages are used 
when each user may require completely different information, for example, bank 
websites, web email etc.

Static page
Main article: Static web page
A static web page (sometimes called a flat page/stationary page) is a web page 
that is delivered to the user exactly as stored, in contrast to dynamic web 
pages which are generated by a web application.

Consequently, a static web page displays the same information for all users, 
from all contexts, subject to modern capabilities of a web server to negotiate 
content-type or language of the document where such versions are available and 
the server is configured to do so.

Dynamic pages
Main articles: Dynamic web page and Ajax (programming)

Dynamic web page: example of server-side scripting (PHP and MySQL).
A server-side dynamic web page is a web page whose construction is controlled 
by an application server processing server-side scripts. In server-side 
scripting, parameters determine how the assembly of every new web page 
proceeds, including the setting up of more client-side processing.

A client-side dynamic web page processes the web page using HTML scripting 
running in the browser as it loads. JavaScript and other scripting languages 
determine the way the HTML in the received page is parsed into the Document 
Object Model, or DOM, that represents the loaded web page. The same client-side 
techniques can then dynamically update or change the DOM in the same way.

A dynamic web page is then reloaded by the user or by a computer program to 
change some variable content. The updating information could come from the 
server, or from changes made to that page's DOM. This may or may not truncate 
the browsing history or create a saved version to go back to, but a dynamic web 
page update using Ajax technologies will neither create a page to go back to, 
nor truncate the web browsing history forward of the displayed page. Using Ajax 
technologies the end user gets one dynamic page managed as a single page in the 
web browser while the actual web content rendered on that page can vary. The 
Ajax engine sits only on the browser requesting parts of its DOM, the DOM, for 
its client, from an application server.

DHTML is the umbrella term for technologies and methods used to create web 
pages that are not static web pages, though it has fallen out of common use 
since the popularization of AJAX, a term which is now itself rarely used. 
Client-side-scripting, server-side scripting, or a combination of these make 
for the dynamic web experience in a browser.

JavaScript is a scripting language that was initially developed in 1995 by 
Brendan Eich, then of Netscape, for use within web pages.[54] The standardised 
version is ECMAScript.[54] To make web pages more interactive, some web 
applications also use JavaScript techniques such as Ajax (asynchronous 
JavaScript and XML). Client-side script is delivered with the page that can 
make additional HTTP requests to the server, either in response to user actions 
such as mouse movements or clicks, or based on elapsed time. The server's 
responses are used to modify the current page rather than creating a new page 
with each response, so the server needs only to provide limited, incremental 
information. Multiple Ajax requests can be handled at the same time, and users 
can interact with the page while data is retrieved. Web pages may also 
regularly poll the server to check whether new information is available.[55]

Website

The usap.gov website
Main article: website
A website[56] is a collection of related web resources including web pages, 
multimedia content, typically identified with a common domain name, and 
published on at least one web server. Notable examples are wikipedia.org, 
google.com, and amazon.com.

A website may be accessible via a public Internet Protocol (IP) network, such 
as the Internet, or a private local area network (LAN), by referencing a 
uniform resource locator (URL) that identifies the site.

Websites can have many functions and can be used in various fashions; a website 
can be a personal website, a corporate website for a company, a government 
website, an organization website, etc. Websites are typically dedicated to a 
particular topic or purpose, ranging from entertainment and social networking 
to providing news and education. All publicly accessible websites collectively 
constitute the World Wide Web, while private websites, such as a company's 
website for its employees, are typically a part of an intranet.

Web pages, which are the building blocks of websites, are documents, typically 
composed in plain text interspersed with formatting instructions of Hypertext 
Markup Language (HTML, XHTML). They may incorporate elements from other 
websites with suitable markup anchors. Web pages are accessed and transported 
with the Hypertext Transfer Protocol (HTTP), which may optionally employ 
encryption (HTTP Secure, HTTPS) to provide security and privacy for the user. 
The user's application, often a web browser, renders the page content according 
to its HTML markup instructions onto a display terminal.

Hyperlinking between web pages conveys to the reader the site structure and 
guides the navigation of the site, which often starts with a home page 
containing a directory of the site web content. Some websites require user 
registration or subscription to access content. Examples of subscription 
websites include many business sites, news websites, academic journal websites, 
gaming websites, file-sharing websites, message boards, web-based email, social 
networking websites, websites providing real-time stock market data, as well as 
sites providing various other services. End users can access websites on a 
range of devices, including desktop and laptop computers, tablet computers, 
smartphones and smart TVs.

Browser
Main article: Web browser
A web browser (commonly referred to as a browser) is a software user agent for 
accessing information on the World Wide Web. To connect to a website's server 
and display its pages, a user needs to have a web browser program. This is the 
program that the user runs to download, format and display a web page on the 
user's computer.[57]

In addition to allowing users to find, displaying and moving between web pages, 
a web browser will usually have features like keeping bookmarks, recording 
history, managing cookies (see below) and home pages and may have facilities 
for recording passwords for logging into web sites.

The most popular browsers are Chrome, Firefox, Safari, Internet Explorer, and 
Edge.

Server
Main article: Web server

The inside and front of a Dell PowerEdge web server, a computer designed for 
rack mounting
A Web server is server software, or hardware dedicated to running said 
software, that can satisfy World Wide Web client requests. A web server can, in 
general, contain one or more websites. A web server processes incoming network 
requests over HTTP and several other related protocols.

The primary function of a web server is to store, process and deliver web pages 
to clients.[58] The communication between client and server takes place using 
the Hypertext Transfer Protocol (HTTP). Pages delivered are most frequently 
HTML documents, which may include images, style sheets and scripts in addition 
to the text content.


Multiple web servers may be used for a high traffic website; here, Dell servers 
are installed together being used for the Wikimedia Foundation.
A user agent, commonly a web browser or web crawler, initiates communication by 
making a request for a specific resource using HTTP and the server responds 
with the content of that resource or an error message if unable to do so. The 
resource is typically a real file on the server's secondary storage, but this 
is not necessarily the case and depends on how the web server is implemented.

While the primary function is to serve content, a full implementation of HTTP 
also includes ways of receiving content from clients. This feature is used for 
submitting web forms, including uploading of files.

Many generic web servers also support server-side scripting using Active Server 
Pages (ASP), PHP (Hypertext Preprocessor), or other scripting languages. This 
means that the behaviour of the web server can be scripted in separate files, 
while the actual server software remains unchanged. Usually, this function is 
used to generate HTML documents dynamically ("on-the-fly") as opposed to 
returning static documents. The former is primarily used for retrieving or 
modifying information from databases. The latter is typically much faster and 
more easily cached but cannot deliver dynamic content.

Web servers can also frequently be found embedded in devices such as printers, 
routers, webcams and serving only a local network. The web server may then be 
used as a part of a system for monitoring or administering the device in 
question. This usually means that no additional software has to be installed on 
the client computer since only a web browser is required (which now is included 
with most operating systems).

Cookie
Main article: HTTP cookie
An HTTP cookie (also called web cookie, Internet cookie, browser cookie, or 
simply cookie) is a small piece of data sent from a website and stored on the 
user's computer by the user's web browser while the user is browsing. Cookies 
were designed to be a reliable mechanism for websites to remember stateful 
information (such as items added in the shopping cart in an online store) or to 
record the user's browsing activity (including clicking particular buttons, 
logging in, or recording which pages were visited in the past). They can also 
be used to remember arbitrary pieces of information that the user previously 
entered into form fields such as names, addresses, passwords, and credit card 
numbers.

Cookies perform essential functions in the modern web. Perhaps most 
importantly, authentication cookies are the most common method used by web 
servers to know whether the user is logged in or not, and which account they 
are logged in with. Without such a mechanism, the site would not know whether 
to send a page containing sensitive information, or require the user to 
authenticate themselves by logging in. The security of an authentication cookie 
generally depends on the security of the issuing website and the user's web 
browser, and on whether the cookie data is encrypted. Security vulnerabilities 
may allow a cookie's data to be read by a hacker, used to gain access to user 
data, or used to gain access (with the user's credentials) to the website to 
which the cookie belongs (see cross-site scripting and cross-site request 
forgery for examples).[59]

Tracking cookies, and especially third-party tracking cookies, are commonly 
used as ways to compile long-term records of individuals' browsing histories 
– a potential privacy concern that prompted European[60] and U.S. lawmakers 
to take action in 2011.[61][62] European law requires that all websites 
targeting European Union member states gain "informed consent" from users 
before storing non-essential cookies on their device.

Google Project Zero researcher Jann Horn describes ways cookies can be read by 
intermediaries, like Wi-Fi hotspot providers. He recommends to use the browser 
in incognito mode in such circumstances.[63]

Search engine
Main article: Search engine

The results of a search for the term "lunar eclipse" in a web-based image 
search engine
A web search engine or Internet search engine is a software system that is 
designed to carry out web search (Internet search), which means to search the 
World Wide Web in a systematic way for particular information specified in a 
web search query. The search results are generally presented in a line of 
results, often referred to as search engine results pages (SERPs). The 
information may be a mix of web pages, images, videos, infographics, articles, 
research papers and other types of files. Some search engines also mine data 
available in databases or open directories. Unlike web directories, which are 
maintained only by human editors, search engines also maintain real-time 
information by running an algorithm on a web crawler. Internet content that is 
not capable of being searched by a web search engine is generally described as 
the deep web.

Deep web
Main article: Deep web
The deep web,[64] invisible web,[65] or hidden web[66] are parts of the World 
Wide Web whose contents are not indexed by standard web search engines. The 
opposite term to the deep web is the surface web, which is accessible to anyone 
using the Internet.[67] Computer scientist Michael K. Bergman is credited with 
coining the term deep web in 2001 as a search indexing term.[68]

The content of the deep web is hidden behind HTTP forms,[69][70] and includes 
many very common uses such as web mail, online banking, and services that users 
must pay for, and which is protected by a paywall, such as video on demand, 
some online magazines and newspapers, among others.

The content of the deep web can be located and accessed by a direct URL or IP 
address, and may require a password or other security access past the public 
website page.

Caching
A web cache is a server computer located either on the public Internet, or 
within an enterprise that stores recently accessed web pages to improve 
response time for users when the same content is requested within a certain 
time after the original request. Most web browsers also implement a browser 
cache by writing recently obtained data to a local data storage device. HTTP 
requests by a browser may ask only for data that has changed since the last 
access. Web pages and resources may contain expiration information to control 
caching to secure sensitive data, such as in online banking, or to facilitate 
frequently updated sites, such as news media. Even sites with highly dynamic 
content may permit basic resources to be refreshed only occasionally. Web site 
designers find it worthwhile to collate resources such as CSS data and 
JavaScript into a few site-wide files so that they can be cached efficiently. 
Enterprise firewalls often cache Web resources requested by one user for the 
benefit of many users. Some search engines store cached content of frequently 
accessed websites.

Security
For criminals, the Web has become a venue to spread malware and engage in a 
range of cybercrimes, including (but not limited to) identity theft, fraud, 
espionage and intelligence gathering.[71] Web-based vulnerabilities now 
outnumber traditional computer security concerns,[72][73] and as measured by 
Google, about one in ten web pages may contain malicious code.[74] Most 
web-based attacks take place on legitimate websites, and most, as measured by 
Sophos, are hosted in the United States, China and Russia.[75] The most common 
of all malware threats is SQL injection attacks against websites.[76] Through 
HTML and URIs, the Web was vulnerable to attacks like cross-site scripting 
(XSS) that came with the introduction of JavaScript[77] and were exacerbated to 
some degree by Web 2.0 and Ajax web design that favours the use of scripts.[78] 
Today by one estimate, 70% of all websites are open to XSS attacks on their 
users.[79] Phishing is another common threat to the Web. In February 2013, RSA 
(the security division of EMC) estimated the global losses from phishing at 
$1.5 billion in 2012.[80] Two of the well-known phishing methods are Covert 
Redirect and Open Redirect.

Proposed solutions vary. Large security companies like McAfee already design 
governance and compliance suites to meet post-9/11 regulations,[81] and some, 
like Finjan have recommended active real-time inspection of programming code 
and all content regardless of its source.[71] Some have argued that for 
enterprises to see Web security as a business opportunity rather than a cost 
centre,[82] while others call for "ubiquitous, always-on digital rights 
management" enforced in the infrastructure to replace the hundreds of companies 
that secure data and networks.[83] Jonathan Zittrain has said users sharing 
responsibility for computing safety is far preferable to locking down the 
Internet.[84]

Privacy
Main article: Internet privacy
Every time a client requests a web page, the server can identify the request's 
IP address and usually logs it. Also, unless set not to do so, most web 
browsers record requested web pages in a viewable history feature, and usually 
cache much of the content locally. Unless the server-browser communication uses 
HTTPS encryption, web requests and responses travel in plain text across the 
Internet and can be viewed, recorded, and cached by intermediate systems. 
Another way to hide personally identifiable information is by using a VPN. A 
VPN encrypts online traffic and masks original IP address lowering the chance 
of user identification. When a web page asks for, and the user supplies, 
personally identifiable information—such as their real name, address, e-mail 
address, etc.—web-based entities can associate current web traffic with that 
individual. If the website uses HTTP cookies, username and password 
authentication, or other tracking techniques, it can relate other web visits, 
before and after, to the identifiable information provided. In this way it is 
possible for a web-based organisation to develop and build a profile of the 
individual people who use its site or sites. It may be able to build a record 
for an individual that includes information about their leisure activities, 
their shopping interests, their profession, and other aspects of their 
demographic profile. These profiles are obviously of potential interest to 
marketeers, advertisers and others. Depending on the website's terms and 
conditions and the local laws that apply information from these profiles may be 
sold, shared, or passed to other organisations without the user being informed. 
For many ordinary people, this means little more than some unexpected e-mails 
in their in-box or some uncannily relevant advertising on a future web page. 
For others, it can mean that time spent indulging an unusual interest can 
result in a deluge of further targeted marketing that may be unwelcome. Law 
enforcement, counter terrorism, and espionage agencies can also identify, 
target and track individuals based on their interests or proclivities on the 
Web.

Social networking sites try to get users to use their real names, interests, 
and locations, rather than pseudonyms, as their executives believe that this 
makes the social networking experience more engaging for users. On the other 
hand, uploaded photographs or unguarded statements can be identified to an 
individual, who may regret this exposure. Employers, schools, parents, and 
other relatives may be influenced by aspects of social networking profiles, 
such as text posts or digital photos, that the posting individual did not 
intend for these audiences. On-line bullies may make use of personal 
information to harass or stalk users. Modern social networking websites allow 
fine grained control of the privacy settings for each individual posting, but 
these can be complex and not easy to find or use, especially for beginners.[85] 
Photographs and videos posted onto websites have caused particular problems, as 
they can add a person's face to an on-line profile. With modern and potential 
facial recognition technology, it may then be possible to relate that face with 
other, previously anonymous, images, events and scenarios that have been imaged 
elsewhere. Due to image caching, mirroring and copying, it is difficult to 
remove an image from the World Wide Web.

Standards
Main article: Web standards
Web standards include many interdependent standards and specifications, some of 
which govern aspects of the Internet, not just the World Wide Web. Even when 
not web-focused, such standards directly or indirectly affect the development 
and administration of web sites and web services. Considerations include the 
interoperability, accessibility and usability of web pages and web sites.

Web standards, in the broader sense, consist of the following:

Recommendations published by the World Wide Web Consortium (W3C)[86]
"Living Standard" made by the Web Hypertext Application Technology Working 
Group (WHATWG)
Request for Comments (RFC) documents published by the Internet Engineering Task 
Force (IETF)[87]
Standards published by the International Organization for Standardization 
(ISO)[88]
Standards published by Ecma International (formerly ECMA)[89]
The Unicode Standard and various Unicode Technical Reports (UTRs) published by 
the Unicode Consortium[90]
Name and number registries maintained by the Internet Assigned Numbers 
Authority (IANA)[91]
Web standards are not fixed sets of rules, but are a constantly evolving set of 
finalized technical specifications of web technologies.[92] Web standards are 
developed by standards organizations—groups of interested and often competing 
parties chartered with the task of standardization—not technologies developed 
and declared to be a standard by a single individual or company. It is crucial 
to distinguish those specifications that are under development from the ones 
that already reached the final development status (in case of W3C 
specifications, the highest maturity level).

Accessibility
Main article: Web accessibility
There are methods for accessing the Web in alternative mediums and formats to 
facilitate use by individuals with disabilities. These disabilities may be 
visual, auditory, physical, speech-related, cognitive, neurological, or some 
combination. Accessibility features also help people with temporary 
disabilities, like a broken arm, or ageing users as their abilities change.[93] 
The Web receives information as well as providing information and interacting 
with society. The World Wide Web Consortium claims that it is essential that 
the Web be accessible, so it can provide equal access and equal opportunity to 
people with disabilities.[94] Tim Berners-Lee once noted, "The power of the Web 
is in its universality. Access by everyone regardless of disability is an 
essential aspect."[93] Many countries regulate web accessibility as a 
requirement for websites.[95] International co-operation in the W3C Web 
Accessibility Initiative led to simple guidelines that web content authors as 
well as software developers can use to make the Web accessible to persons who 
may or may not be using assistive technology.[93][96]

Internationalisation
The W3C Internationalisation Activity assures that web technology works in all 
languages, scripts, and cultures.[97] Beginning in 2004 or 2005, Unicode gained 
ground and eventually in December 2007 surpassed both ASCII and Western 
European as the Web's most frequently used character encoding.[98] Originally 
RFC 3986 allowed resources to be identified by URI in a subset of US-ASCII. RFC 
3987 allows more characters—any character in the Universal Character 
Set—and now a resource can be identified by IRI in any language.[99]

See also
Electronic publishing
Internet metaphors
Internet security
Lists of websites
Prestel
Streaming media
Web development tools
Web literacy
World Wide Telecom Web
Nuvola apps kcmsystem.svgEngineering portalCrystal Clear app 
linneighborhood.svgInternet portal
References
 Tobin, James (12 June 2012). Great Projects: The Epic Story of the Building of 
America, from the Taming of the Mississippi to the Invention of the Internet. 
Simon and Schuster. ISBN 978-0-7432-1476-6.
 "What is the difference between the Web and the Internet?". W3C Help and FAQ. 
W3C. 2009. Archived from the original on 9 July 2015. Retrieved 16 July 2015.
 McPherson, Stephanie Sammartino (2009). Tim Berners-Lee: Inventor of the World 
Wide Web. Twenty-First Century Books. ISBN 978-0-8225-7273-2.
 Quittner, Joshua (29 March 1999). "Network Designer Tim Berners-Lee". Time 
Magazine. Archived from the original on 15 August 2007. Retrieved 17 May 2010. 
He wove the World Wide Web and created a mass medium for the 21st century. The 
World Wide Web is Berners-Lee's alone. He designed it. He set it loose it on 
the world. And he more than anyone else has fought to keep it an open, 
non-proprietary and free.[page needed]
 In, Lee (30 June 2012). Electronic Commerce Management for Business Activities 
and Global Enterprises: Competitive Advantages: Competitive Advantages. IGI 
Global. ISBN 978-1-4666-1801-5.
 Misiroglu, Gina (26 March 2015). American Countercultures: An Encyclopedia of 
Nonconformists, Alternative Lifestyles, and Radical Ideas in U.S. History: An 
Encyclopedia of Nonconformists, Alternative Lifestyles, and Radical Ideas in 
U.S. History. Routledge. ISBN 978-1-317-47729-7.
 "World Wide Web Timeline". Pew Research Center. 11 March 2014. Archived from 
the original on 29 July 2015. Retrieved 1 August 2015.
 Dewey, Caitlin (12 March 2014). "36 Ways the Web Has Changed Us". The 
Washington Post. Archived from the original on 9 September 2015. Retrieved 1 
August 2015.
 "Internet Live Stats". Archived from the original on 2 July 2015. Retrieved 1 
August 2015.
 Joseph Adamski; Kathy Finnegan (2007). New Perspectives on Microsoft Office 
Access 2007, Comprehensive. Cengage Learning. p. 390. ISBN 978-1-4239-0589-9.
 Enzer, Larry (31 August 2018). "The Evolution of the World Wide Web". Monmouth 
Web Developers. Archived from the original on 18 November 2018. Retrieved 31 
August 2018.
 "Archived copy" (PDF). Archived from the original (PDF) on 17 November 2015. 
Retrieved 26 August 2015.
 May, Ashley (12 March 2019). "Happy 30th birthday, World Wide Web. Inventor 
outlines plan to combat hacking, hate speech". USA Today. Retrieved 12 March 
2019.
 Aja Romano (12 March 2019). "The World Wide Web – not the Internet – turns 
30 years old". Vox.com.
 Berners-Lee, Tim (March 1989). "Information Management: A Proposal". W3C. 
Archived from the original on 15 March 2009. Retrieved 27 July 2009.
 Berners-Lee, Tim; Cailliau, Robert (12 November 1990). "WorldWideWeb: Proposal 
for a HyperText Project". Archived from the original on 2 May 2015. Retrieved 
12 May 2015.
 "Tim Berners-Lee's original World Wide Web browser". Archived from the 
original on 17 July 2011. With recent phenomena like blogs and wikis, the Web 
is beginning to develop the kind of collaborative nature that its inventor 
envisaged from the start.
 "Tim Berners-Lee: client". W3.org. Archived from the original on 21 July 2009. 
Retrieved 27 July 2009.
 "First Web pages". W3.org. Archived from the original on 31 January 2010. 
Retrieved 27 July 2009.
 "The birth of the web". CERN. Archived from the original on 24 December 2015. 
Retrieved 23 December 2015.
 Murawski, John (24 May 2013). "Hunt for world's oldest WWW page leads to UNC 
Chapel Hill". News & Observer. Archived from the original on 8 June 2013.
 "Short summary of the World Wide Web project". 6 August 1991. Retrieved 27 
July 2009.
 "Silvano de Gennaro disclaims 'the first photo on the Web'". Archived from the 
original on 4 August 2012. Retrieved 27 July 2012. If you read well our 
website, it says that it was, to our knowledge, the 'first photo of a band'. 
Dozens of media are totally distorting our words for the sake of cheap 
sensationalism. Nobody knows which was the first photo on the Web.
 "The Early World Wide Web at SLAC". Archived from the original on 24 November 
2005.
 "About SPIRES". Archived from the original on 12 February 2010. Retrieved 30 
March 2010.
 "A Little History of the World Wide Web". Archived from the original on 6 May 
2013.
 "W3C10 Timeline Graphic". Retrieved 29 January 2020.
 Conklin, Jeff (1987), IEEE Computer, 20, pp. 17–41
 "Inventor of the Week Archive: The World Wide Web". Massachusetts Institute of 
Technology: MIT School of Engineering. Archived from the original on 8 June 
2010. Retrieved 23 July 2009.
 "Ten Years Public Domain for the Original Web Software". 
Tenyears-www.web.cern.ch. 30 April 2003. Archived from the original on 13 
August 2009. Retrieved 27 July 2009.
 "Mosaic Web Browser History – NCSA, Marc Andreessen, Eric Bina". 
Livinginternet.com. Retrieved 27 July 2009.
 "NCSA Mosaic – September 10, 1993 Demo". Totic.org. Retrieved 27 July 2009.
 "Vice President Al Gore's ENIAC Anniversary Speech". Cs.washington.edu. 14 
February 1996. Archived from the original on 20 February 2009. Retrieved 27 
July 2009.
 "Internet legal definition of Internet". West's Encyclopedia of American Law, 
edition 2. Free Online Law Dictionary. 15 July 2009. Retrieved 25 November 
2008.
 "WWW (World Wide Web) Definition". TechTerms. Archived from the original on 11 
May 2009. Retrieved 19 February 2010.
 Jacobs, Ian; Walsh, Norman (15 December 2004). "Architecture of the World Wide 
Web, Volume One". Introduction: W3C. Archived from the original on 9 February 
2015. Retrieved 11 February 2015.
 "Supplement no.1, Diplomatic and Overseas List, K.B.E." (PDF). 
thegazette.co.uk. The Gazette. 31 December 2003. Archived (PDF) from the 
original on 3 February 2016. Retrieved 7 February 2016.
 "Web's inventor gets a knighthood". BBC. 31 December 2003. Archived from the 
original on 23 December 2007. Retrieved 25 May 2008.
 "What is the difference between the Web and the Internet?". World Wide Web 
Consortium. Archived from the original on 22 April 2016. Retrieved 18 April 
2016.
 Muylle, Steve; Rudy Moenaert; Marc Despont (1999). "A grounded theory of World 
Wide Web search behaviour". Journal of Marketing Communications. 5 (3): 143. 
doi:10.1080/135272699345644.
 Flanagan, David. JavaScript – The definitive guide (6 ed.). p. 1. JavaScript 
is part of the triad of technologies that all Web developers must learn: HTML 
to specify the content of web pages, CSS to specify the presentation of web 
pages, and JavaScript to specify the behaviour of web pages.
 "HTML 4.0 Specification – W3C Recommendation – Conformance: requirements 
and recommendations". World Wide Web Consortium. 18 December 1997. Retrieved 6 
July 2015.
 Berners-Lee, Tim. "Frequently asked questions by the Press". W3C. Archived 
from the original on 2 August 2009. Retrieved 27 July 2009.
 Palazzi, P (2011) 'The Early Days of the WWW at CERN' Archived 23 July 2012 at 
the Wayback Machine
 Dominic Fraser (13 May 2018). "Why a domain's root can't be a CNAME – and 
other tidbits about the DNS". FreeCodeCamp.
 "automatically adding www.___.com". mozillaZine. 16 May 2003. Archived from 
the original on 27 June 2009. Retrieved 27 May 2009.
 Masnick, Mike (7 July 2008). "Microsoft Patents Adding 'www.' And '.com' To 
Text". Techdirt. Archived from the original on 27 June 2009. Retrieved 27 May 
2009.
 "Audible pronunciation of 'WWW'". Oxford University Press. Archived from the 
original on 25 May 2014. Retrieved 25 May 2014.
 "Stephen Fry's pronunciation of 'WWW'". Podcasts.com. Archived from the 
original on 4 April 2017.
 Simonite, Tom (22 July 2008). "Help us find a better way to pronounce www". 
newscientist.com. New Scientist, Technology. Archived from the original on 13 
March 2016. Retrieved 7 February 2016.
 "MDBG Chinese-English dictionary – Translate". Archived from the original on 
12 November 2008. Retrieved 27 July 2009.
 "Frequently asked questions by the Press – Tim BL". W3.org. Archived from 
the original on 2 August 2009. Retrieved 27 July 2009.
 Castelluccio, Michael (2010). "It's not your grandfather's Internet". 
thefreelibrary.com. Institute of Management Accountants. Retrieved 7 February 
2016.
 Hamilton, Naomi (31 July 2008). "The A-Z of Programming Languages: 
JavaScript". Computerworld. IDG. Archived from the original on 24 May 2009. 
Retrieved 12 May 2009.
 Buntin, Seth (23 September 2008). "jQuery Polling plugin". Archived from the 
original on 13 August 2009. Retrieved 22 August 2009.
 "website". TheFreeDictionary.com. Retrieved 2 July 2011.
 "Difference Between Search Engine and Browser".
 Patrick, Killelea (2002). Web performance tuning (2nd ed.). Beijing: O'Reilly. 
p. 264. ISBN 978-0596001728. OCLC 49502686.
 Vamosi, Robert (14 April 2008). "Gmail cookie stolen via Google Spreadsheets". 
News.cnet.com. Retrieved 19 October 2017.
 "What about the "EU Cookie Directive"?". WebCookies.org. 2013. Retrieved 19 
October 2017.
 "New net rules set to make cookies crumble". BBC. 8 March 2011.
 "Sen. Rockefeller: Get Ready for a Real Do-Not-Track Bill for Online 
Advertising". Adage.com. 6 May 2011.
 Want to use my wifi?, Jann Horn, accessed 2018-01-05.
 Hamilton, Nigel. "The Mechanics of a Deep Net Metasearch Engine". CiteSeerX 
10.1.1.90.5847.
 Devine, Jane; Egger-Sider, Francine (July 2004). "Beyond google: the invisible 
web in the academic library". The Journal of Academic Librarianship. 30 (4): 
265–269. doi:10.1016/j.acalib.2004.04.010.
 Raghavan, Sriram; Garcia-Molina, Hector (11–14 September 2001). "Crawling 
the Hidden Web". 27th International Conference on Very Large Data Bases.
 "Surface Web". Computer Hope. Retrieved 20 June 2018.
 Wright, Alex (22 February 2009). "Exploring a 'Deep Web' That Google Can't 
Grasp". The New York Times. Retrieved 23 February 2009.
 Madhavan, J., Ko, D., Kot, Ł., Ganapathy, V., Rasmussen, A., & Halevy, A. 
(2008). Google's deep web crawl. Proceedings of the VLDB Endowment, 1(2), 
1241–52.
 Shedden, Sam (8 June 2014). "How Do You Want Me to Do It? Does It Have to Look 
like an Accident? – an Assassin Selling a Hit on the Net; Revealed Inside the 
Deep Web". Sunday Mail. Retrieved 5 May 2017 – via Questia.
 Ben-Itzhak, Yuval (18 April 2008). "Infosecurity 2008 – New defence strategy 
in battle against e-crime". ComputerWeekly. Reed Business Information. Archived 
from the original on 4 June 2008. Retrieved 20 April 2008.
 Christey, Steve & Martin, Robert A. (22 May 2007). "Vulnerability Type 
Distributions in CVE (version 1.1)". MITRE Corporation. Archived from the 
original on 17 March 2013. Retrieved 7 June 2008.
 "Symantec Internet Security Threat Report: Trends for July–December 2007 
(Executive Summary)" (PDF). XIII. Symantec Corp. April 2008: 1–2. Archived 
(PDF) from the original on 25 June 2008. Retrieved 11 May 2008.
 "Google searches web's dark side". BBC News. 11 May 2007. Archived from the 
original on 7 March 2008. Retrieved 26 April 2008.
 "Security Threat Report (Q1 2008)" (PDF). Sophos. Archived (PDF) from the 
original on 31 December 2013. Retrieved 24 April 2008.
 "Security threat report" (PDF). Sophos. July 2008. Archived (PDF) from the 
original on 31 December 2013. Retrieved 24 August 2008.
 Fogie, Seth, Jeremiah Grossman, Robert Hansen, and Anton Rager (2007). Cross 
Site Scripting Attacks: XSS Exploits and Defense (PDF). Syngress, Elsevier 
Science & Technology. pp. 68–69, 127. ISBN 978-1-59749-154-9. Archived from 
the original (PDF) on 25 June 2008. Retrieved 6 June 2008.
 O'Reilly, Tim (30 September 2005). "What Is Web 2.0". O'Reilly Media. pp. 
4–5. Archived from the original on 15 April 2013. Retrieved 4 June 2008. and 
AJAX web applications can introduce security vulnerabilities like "client-side 
security controls, increased attack surfaces, and new possibilities for 
Cross-Site Scripting (XSS)", in Ritchie, Paul (March 2007). "The security risks 
of AJAX/web 2.0 applications" (PDF). Infosecurity. Archived from the original 
(PDF) on 25 June 2008. Retrieved 6 June 2008. which cites Hayre, Jaswinder S. & 
Kelath, Jayasankar (22 June 2006). "Ajax Security Basics". SecurityFocus. 
Archived from the original on 15 May 2008. Retrieved 6 June 2008.
 Berinato, Scott (1 January 2007). "Software Vulnerability Disclosure: The 
Chilling Effect". CSO. CXO Media. p. 7. Archived from the original on 18 April 
2008. Retrieved 7 June 2008.
 "2012 Global Losses From phishing Estimated At $1.5 Bn". FirstPost. 20 
February 2013. Archived from the original on 21 December 2014. Retrieved 25 
January 2019.
 Prince, Brian (9 April 2008). "McAfee Governance, Risk and Compliance Business 
Unit". eWEEK. Ziff Davis Enterprise Holdings. Retrieved 25 April 2008.
 Preston, Rob (12 April 2008). "Down To Business: It's Past Time To Elevate The 
Infosec Conversation". InformationWeek. United Business Media. Archived from 
the original on 14 April 2008. Retrieved 25 April 2008.
 Claburn, Thomas (6 February 2007). "RSA's Coviello Predicts Security 
Consolidation". InformationWeek. United Business Media. Archived from the 
original on 7 February 2009. Retrieved 25 April 2008.
 Duffy Marsan, Carolyn (9 April 2008). "How the iPhone is killing the 'Net". 
Network World. IDG. Archived from the original on 14 April 2008. Retrieved 17 
April 2008.
 boyd, danah; Hargittai, Eszter (July 2010). "Facebook privacy settings: Who 
cares?". First Monday. 15 (8). doi:10.5210/fm.v15i8.3086.
 "W3C Technical Reports and Publications". W3C. Retrieved 19 January 2009.
 "IETF RFC page". IETF. Archived from the original on 2 February 2009. 
Retrieved 19 January 2009.
 "Search for World Wide Web in ISO standards". ISO. Retrieved 19 January 2009.
 "Ecma formal publications". Ecma. Retrieved 19 January 2009.
 "Unicode Technical Reports". Unicode Consortium. Retrieved 19 January 2009.
 "IANA home page". IANA. Retrieved 19 January 2009.
 Leslie Sikos (2011). Web standards – Mastering HTML5, CSS3, and XML. Apress. 
ISBN 978-1-4302-4041-9.
 "Web Accessibility Initiative (WAI)". World Wide Web Consortium. Archived from 
the original on 2 April 2009. Retrieved 7 April 2009.
 "Developing a Web Accessibility Business Case for Your Organization: 
Overview". World Wide Web Consortium. Archived from the original on 14 April 
2009. Retrieved 7 April 2009.
 "Legal and Policy Factors in Developing a Web Accessibility Business Case for 
Your Organization". World Wide Web Consortium. Archived from the original on 5 
April 2009. Retrieved 7 April 2009.
 "Web Content Accessibility Guidelines (WCAG) Overview". World Wide Web 
Consortium. Archived from the original on 1 April 2009. Retrieved 7 April 2009.
 "Internationalization (I18n) Activity". World Wide Web Consortium. Archived 
from the original on 16 April 2009. Retrieved 10 April 2009.
 Davis, Mark (5 April 2008). "Moving to Unicode 5.1". Archived from the 
original on 21 May 2009. Retrieved 10 April 2009.
 "World Wide Web Consortium Supports the IETF URI Standard and IRI Proposed 
Standard" (Press release). World Wide Web Consortium. 26 January 2005. Archived 
from the original on 7 February 2009. Retrieved 10 April 2009.

 Apple Inc.
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
"Apple (company)" redirects here. For other companies of similar name, see 
Apple (disambiguation) § Brands and enterprises.
Coordinates: 37.3349°N 122.0090°W

Apple Inc.
Apple logo black.svg
Apple park cupertino 2019.jpg
Overhead view of Apple Park located in Cupertino, California
Formerly
Apple Computer Company
(1976–1977)
Apple Computer, Inc.
(1977–2007)
Type
Public
Traded as	
NASDAQ: AAPL
NASDAQ-100 component
DJIA component
S&P 100 component
S&P 500 component
ISIN	US0378331005
Industry	
Computer hardware
Computer software
Consumer electronics
Cloud computing
Digital distribution
Fabless silicon design
Semiconductors
Financial technology
Artificial intelligence
Founded	April 1, 1976; 43 years ago
Founders	
Steve Jobs
Steve Wozniak
Ronald Wayne
Headquarters	1 Apple Park Way
Cupertino, California, United States
Number of locations
500+ retail stores (2019)
Area served
Worldwide
Key people
Arthur D. Levinson (Chairman)
Tim Cook (CEO)
Jeff Williams (COO)
Products	
MacintoshiPodiPhoneiPadApple WatchApple 
TVHomePodmacOSiOSiPadOSwatchOStvOSiLifeiWorkFinal Cut ProLogic 
ProGarageBandShazamSiri
Services	
App StoreApple ArcadeApple CardApple Music Beats 1Apple News+Apple Pay 
CashApple Store Genius BarApple TV+Apple BooksiCloudiMessageiTunes StoreMac App 
Store
Revenue	Decrease US$260.174 billion[1] (2019)
Operating income
Decrease US$63.930 billion[1] (2019)
Net income
Decrease US$55.256 billion[1] (2019)
Total assets	Decrease US$338.516 billion[1] (2019)
Total equity	Decrease US$90.488 billion[1] (2019)
Number of employees
137,000[2] (2019)
Subsidiaries	
Braeburn Capital
Beats Electronics
Claris
Apple Energy, LLC
Apple Sales International[3]
Apple Services[4]
Apple Worldwide Video[5]
Anobit
Beddit
Website	www.apple.com
Apple Inc. is an American multinational technology company headquartered in 
Cupertino, California, that designs, develops, and sells consumer electronics, 
computer software, and online services. It is considered one of the Big Four 
technology companies, alongside Amazon, Google, and Microsoft.[6][7][8]

The company's hardware products include the iPhone smartphone, the iPad tablet 
computer, the Mac personal computer, the iPod portable media player, the Apple 
Watch smartwatch, the Apple TV digital media player, the AirPods wireless 
earbuds and the HomePod smart speaker. Apple's software includes the macOS, 
iOS, iPadOS, watchOS, and tvOS operating systems, the iTunes media player, the 
Safari web browser, the Shazam acoustic fingerprint utility, and the iLife and 
iWork creativity and productivity suites, as well as professional applications 
like Final Cut Pro, Logic Pro, and Xcode. Its online services include the 
iTunes Store, the iOS App Store, Mac App Store, Apple Music, Apple TV+, 
iMessage, and iCloud. Other services include Apple Store, Genius Bar, 
AppleCare, Apple Pay, Apple Pay Cash, and Apple Card.

Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 
to develop and sell Wozniak's Apple I personal computer, though Wayne sold his 
share back within 12 days. It was incorporated as Apple Computer, Inc., in 
January 1977, and sales of its computers, including the Apple II, grew quickly. 
Within a few years, Jobs and Wozniak had hired a staff of computer designers 
and had a production line. Apple went public in 1980 to instant financial 
success. Over the next few years, Apple shipped new computers featuring 
innovative graphical user interfaces, such as the original Macintosh in 1984, 
and Apple's marketing advertisements for its products received widespread 
critical acclaim. However, the high price of its products and limited 
application library caused problems, as did power struggles between executives. 
In 1985, Wozniak departed Apple amicably and remained an honorary employee,[9] 
while Jobs and others resigned to found NeXT.[10]

As the market for personal computers expanded and evolved through the 1990s, 
Apple lost market share to the lower-priced duopoly of Microsoft Windows on 
Intel PC clones. The board recruited CEO Gil Amelio to what would be a 500-day 
charge for him to rehabilitate the financially troubled company—reshaping it 
with layoffs, executive restructuring, and product focus. In 1997, he led Apple 
to buy NeXT, solving the desperately failed operating system strategy and 
bringing Jobs back. Jobs pensively regained leadership status, becoming CEO in 
2000. Apple swiftly returned to profitability under the revitalizing Think 
different campaign, as he rebuilt Apple's status by launching the iMac in 1998, 
opening the retail chain of Apple Stores in 2001, and acquiring numerous 
companies to broaden the software portfolio. In January 2007, Jobs renamed the 
company Apple Inc., reflecting its shifted focus toward consumer electronics, 
and launched the iPhone to great critical acclaim and financial success. In 
August 2011, Jobs resigned as CEO due to health complications, and Tim Cook 
became the new CEO. Two months later, Jobs died, marking the end of an era for 
the company. In June 2019, Jony Ive, Apple's CDO, left the company to start his 
own firm but stated he would work with Apple as its primary client.

Apple is well known for its size and revenues. Its worldwide annual revenue 
totaled $265 billion for the 2018 fiscal year. Apple is the world's largest 
technology company by revenue and one of the world's most valuable companies. 
It is also the world's third-largest mobile phone manufacturer after Samsung 
and Huawei.[11] In August 2018, Apple became the first public U.S. company to 
be valued at over $1 trillion.[12][13] The company employs 123,000 full-time 
employees[14] and maintains 504 retail stores in 24 countries as of 2018.[15] 
It operates the iTunes Store, which is the world's largest music retailer. As 
of January 2018, more than 1.3 billion Apple products are actively in use 
worldwide.[16] The company also has a high level of brand loyalty and is ranked 
as the world's most valuable brand. However, Apple receives significant 
criticism regarding the labor practices of its contractors, its environmental 
practices and unethical business practices, including anti-competitive 
behavior, as well as the origins of source materials.


Contents
1	History
1.1	1976–1984: Founding and incorporation
1.2	1984–1991: Success with Macintosh
1.3	1991–1997: Decline and restructuring
1.4	1997–2007: Return to profitability
1.4.1	Intel transition and financial stability
1.5	2007–2011: Success with mobile devices
1.6	2011–present: Post–Steve Jobs era; Tim Cook leadership
2	Products
2.1	Macintosh
2.2	iPod
2.3	iPhone
2.4	iPad
2.5	Apple Watch
2.6	Apple TV
2.7	HomePod
2.8	Software
2.9	Electric vehicles
3	Corporate identity
3.1	Logo
3.2	Advertising
3.3	Brand Semiotics
3.4	Brand loyalty
3.5	Home page
3.6	Headquarters
3.7	Stores
4	Corporate affairs
4.1	Corporate culture
4.1.1	Lack of innovation
4.2	Manufacturing
4.2.1	Labor practices
4.3	Environmental practices and initiatives
4.3.1	Apple Energy
4.3.2	Energy and resources
4.3.3	Toxins
4.3.4	Green bonds
4.4	Finance
4.4.1	Tax practices
4.5	Board of directors
4.6	Executive management
4.7	Litigation
4.8	Privacy stance
4.9	Charitable causes
5	Criticism and controversies
6	See also
7	References
7.1	Bibliography
8	Further reading
9	External links
History
Main article: History of Apple Inc.
1976–1984: Founding and incorporation
See also: History of Apple § 1975–1985: Jobs and Wozniak

In 1976, Steve Jobs co-founded Apple in his parents' home on Crist Drive in Los 
Altos, California.[17] Although it is widely believed that the company was 
founded in the house's garage, Apple co-founder Steve Wozniak called it "a bit 
of a myth".[18] Jobs and Wozniak did, however, move some operations to the 
garage when the bedroom became too crowded.[19]

Apple's first product, the Apple I, invented by Steve Wozniak, was sold as an 
assembled circuit board and lacked basic features such as a keyboard, monitor, 
and case. The owner of this unit added a keyboard and wooden case.

The Apple II Plus, introduced in 1979, designed primarily by Wozniak
Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve 
Wozniak, and Ronald Wayne as a business partnership.[17][20] The company's 
first product was the Apple I, a computer designed and hand-built entirely by 
Wozniak,[21][22] and first shown to the public at the Homebrew Computer 
Club.[23][24] Apple I was sold as a motherboard (with CPU, RAM, and basic 
textual-video chips)—a base kit concept which would now not be marketed as a 
complete personal computer.[25] The Apple I went on sale in July 1976 and was 
market-priced at $666.66 ($2,995 in 2019 dollars, adjusted for 
inflation).[26][27][28][29][30][31]:180 Wozniak later said he had no idea about 
the relation between the number and the mark of the beast, and that he came up 
with the price because he liked "repeating digits".[32]

Apple Computer, Inc. was incorporated on January 3, 1977,[33][34] without 
Wayne, who had left and sold his share of the company back to Jobs and Wozniak 
for $800 only twelve days after having co-founded Apple.[35][36] 
Multimillionaire Mike Markkula provided essential business expertise and 
funding of $250,000 during the incorporation of Apple.[37][38] During the first 
five years of operations, revenues grew exponentially, doubling about every 
four months. Between September 1977 and September 1980, yearly sales grew from 
$775,000 to $118 million, an average annual growth rate of 533%.[39][40]

The Apple II, also invented by Wozniak, was introduced on April 16, 1977, at 
the first West Coast Computer Faire.[41] It differs from its major rivals, the 
TRS-80 and Commodore PET, because of its character cell-based color graphics 
and open architecture. While early Apple II models use ordinary cassette tapes 
as storage devices, they were superseded by the introduction of a ​5 
1⁄4-inch floppy disk drive and interface called the Disk II in 1978.[42][43] 
The Apple II was chosen to be the desktop platform for the first "killer 
application" of the business world: VisiCalc, a spreadsheet program released in 
1979.[42] VisiCalc created a business market for the Apple II and gave home 
users an additional reason to buy an Apple II: compatibility with the 
office.[42] Before VisiCalc, Apple had been a distant third place competitor to 
Commodore and Tandy.[44][45]

By the end of the 1970s, Apple had a staff of computer designers and a 
production line. The company introduced the Apple III in May 1980 in an attempt 
to compete with IBM in the business and corporate computing market.[46] Jobs 
and several Apple employees, including human–computer interface expert Jef 
Raskin, visited Xerox PARC in December 1979 to see a demonstration of the Xerox 
Alto. Xerox granted Apple engineers three days of access to the PARC facilities 
in return for the option to buy 100,000[citation needed] shares (5.6 million 
split-adjusted shares as of March 30, 2019)[34] of Apple at the pre-IPO price 
of $10 a share.[47]

Jobs was immediately convinced that all future computers would use a graphical 
user interface (GUI), and development of a GUI began for the Apple 
Lisa.[48][49] In 1982, however, he was pushed from the Lisa team due to 
infighting. Jobs then took over Wozniak's and Raskin's low-cost-computer 
project, the Macintosh, and redefined it as a graphical system cheaper and 
faster than Lisa.[50] In 1983, Lisa became the first personal computer sold to 
the public with a GUI, but was a commercial failure due to its high price and 
limited software titles, so in 1985 it would be repurposed as the high end 
Macintosh and discontinued in its second year.[51]

On December 12, 1980, Apple (ticker symbol "AAPL") went public selling 4.6 
million shares at $22 per share ($.39 per share when adjusting for stock splits 
as of March 30, 2019),[34] generating over $100 million, which was more capital 
than any IPO since Ford Motor Company in 1956.[52] By the end of the day, the 
stock rose to $29 per share and 300 millionaires were created.[53] Apple's 
market cap was $1.778 billion at the end of its first day of trading.[52][53]

1984–1991: Success with Macintosh
See also: Timeline of Macintosh models and Timeline of the Apple II family

The Macintosh, released in 1984, is the first mass-market personal computer to 
feature an integral graphical user interface and mouse.
In 1984, Apple launched the Macintosh, the first personal computer to be sold 
without a programming language.[54] Its debut was signified by "1984", a $1.5 
million television advertisement directed by Ridley Scott that aired during the 
third quarter of Super Bowl XVIII on January 22, 1984.[55] This is now hailed 
as a watershed event for Apple's success[56] and was called a "masterpiece" by 
CNN[57] and one of the greatest TV advertisements of all time by TV 
Guide.[58][59]

Macintosh sales were initially good, but began to taper off dramatically after 
the first three months due to its high price, slow speed, and limited range of 
available software.[60][61][62][63]:195 In early 1985, this sales slump 
triggered a power struggle between Steve Jobs and CEO John Sculley, who had 
been hired two years earlier by Jobs[64][65] using the famous line, "Do you 
want to sell sugar water for the rest of your life or come with me and change 
the world?"[66] Sculley decided to remove Jobs as the general manager of the 
Macintosh division, and gained unanimous support from the Apple board of 
directors.[67][64]

The board of directors instructed Sculley to contain Jobs and his ability to 
launch expensive forays into untested products. Rather than submit to Sculley's 
direction, Jobs attempted to oust him from his leadership role at Apple.[68] 
Informed by Jean-Louis Gassée, Sculley found out that Jobs had been attempting 
to organize a coup and called an emergency executive meeting at which Apple's 
executive staff sided with Sculley and stripped Jobs of all operational 
duties.[68] Jobs resigned from Apple in September 1985 and took a number of 
Apple employees with him to found NeXT Inc.[69] Wozniak had also quit his 
active employment at Apple earlier in 1985 to pursue other ventures, expressing 
his frustration with Apple's treatment of the Apple II division and stating 
that the company had "been going in the wrong direction for the last five 
years".[10][9][70] Despite Wozniak's grievances, he left the company amicably 
and both Jobs and Wozniak remained Apple shareholders.[71] Wozniak continues to 
represent the company at events or in interviews,[9] receiving a stipend 
estimated to be $120,000 per year for this role.[31]

The outlook on Macintosh improved with the introduction of the LaserWriter, the 
first reasonably priced PostScript laser printer, and PageMaker, an early 
desktop publishing application released in July 1985.[72] It has been suggested 
that the combination of Macintosh, LaserWriter, and PageMaker was responsible 
for the creation of the desktop publishing market.[73][better source needed]


The Macintosh Portable, released in 1989, is Apple's first battery-powered 
portable Macintosh personal computer.
After the departures of Jobs and Wozniak, the Macintosh product line underwent 
a steady change of focus to higher price points, the so-called "high-right 
policy" named for the position on a chart of price vs. profits. Jobs had argued 
the company should produce products aimed at the consumer market and aimed for 
a $1,000 price for the Macintosh, which they were unable to meet. Newer models 
selling at higher price points offered higher profit margin, and appeared to 
have no effect on total sales as power users snapped up every increase in 
power. Although some worried about pricing themselves out of the market, the 
high-right policy was in full force by the mid-1980s, notably due to Jean-Louis 
Gassée's mantra of "fifty-five or die", referring to the 55% profit margins of 
the Macintosh II.[74]:79–80 Selling Macintosh at such high profit margins was 
only possible because of its dominant position in the desktop publishing 
market.[75]

This policy began to backfire in the last years of the decade as new desktop 
publishing programs appeared on PC clones that offered some or much of the same 
functionality of the Macintosh but at far lower price points. The company lost 
its monopoly in this market and had already estranged many of its original 
consumer customer base who could no longer afford their high-priced products. 
The Christmas season of 1989 is the first in the company's history to have 
declining sales, which led to a 20% drop in Apple's stock price.[74]:117–129 
During this period, the relationship between Sculley and Gassée deteriorated, 
leading Sculley to effectively demote Gassée in January 1990 by appointing 
Michael Spindler as the chief operating officer.[76] Gassée left the company 
later that year.[77] In October 1990, Apple introduced three lower-cost models, 
the Macintosh Classic, Macintosh LC, and Macintosh IIsi,[78] all of which saw 
significant sales due to pent-up demand.

In 1991, Apple introduced the PowerBook, replacing the "luggable" Macintosh 
Portable with a design that set the current shape for almost all modern 
laptops. The same year, Apple introduced System 7, a major upgrade to the 
operating system which added color to the interface and introduced new 
networking capabilities. It remained the architectural basis for the Classic 
Mac OS. The success of the PowerBook and other products brought increasing 
revenue.[79] For some time, Apple was doing incredibly well, introducing fresh 
new products and generating increasing profits in the process. The magazine 
MacAddict named the period between 1989 and 1991 as the "first golden age" of 
the Macintosh.[80]

Apple believed the Apple II series was too expensive to produce and took away 
sales from the low-end Macintosh.[81] In October 1990, Apple released the 
Macintosh LC, and began efforts to promote that computer by advising developer 
technical support staff to recommend developing applications for Macintosh 
rather than Apple II, and authorizing salespersons to direct consumers towards 
Macintosh and away from Apple II.[82] The Apple IIe was discontinued in 
1993.[83]

1991–1997: Decline and restructuring
See also: Timeline of the Apple II family

The Penlite is Apple's first prototype of a tablet computer. Created in 1992, 
the project was designed to bring the Mac OS to a tablet – but was canceled 
in favor of the Newton.[84]
The success of Apple's lower-cost consumer models, especially the LC, also led 
to the cannibalization of their higher-priced machines. To address this, 
management introduced several new brands, selling largely identical machines at 
different price points aimed at different markets. These were the high-end 
Quadra, the mid-range Centris line, and the consumer-marketed Performa series. 
This led to significant market confusion, as customers did not understand the 
difference between models.[85]

Apple also experimented with a number of other unsuccessful consumer targeted 
products during the 1990s, including digital cameras, portable CD audio 
players, speakers, video consoles, the eWorld online service, and TV 
appliances. Enormous resources were also invested in the problem-plagued Newton 
division based on John Sculley's unrealistic market forecasts.[citation needed] 
Ultimately, none of these products helped and Apple's market share and stock 
prices continued to slide.[citation needed]

Throughout this period, Microsoft continued to gain market share with Windows 
by focusing on delivering software to cheap commodity personal computers, while 
Apple was delivering a richly engineered but expensive experience.[86] Apple 
relied on high profit margins and never developed a clear response; instead, 
they sued Microsoft for using a GUI similar to the Apple Lisa in Apple 
Computer, Inc. v. Microsoft Corp.[87] The lawsuit dragged on for years before 
it was finally dismissed. At this time, a series of major product flops and 
missed deadlines sullied Apple's reputation, and Sculley was replaced as CEO by 
Michael Spindler.[88]


The Newton is Apple's first PDA brought to market, as well as one of the first 
in the industry. Though failing financially at the time of its release, it 
helped pave the way for the PalmPilot and Apple's own iPhone and iPad in the 
future.
By the late 1980s, Apple was developing alternative platforms to System 6, such 
as A/UX and Pink. The System 6 platform itself was outdated because it was not 
originally built for multitasking. By the 1990s, Apple was facing competition 
from OS/2 and UNIX vendors such as Sun Microsystems. System 6 and 7 would need 
to be replaced by a new platform or reworked to run on modern hardware.[89]

In 1994, Apple, IBM, and Motorola formed the AIM alliance with the goal of 
creating a new computing platform (the PowerPC Reference Platform), which would 
use IBM and Motorola hardware coupled with Apple software. The AIM alliance 
hoped that PReP's performance and Apple's software would leave the PC far 
behind and thus counter Microsoft's monopoly. The same year, Apple introduced 
the Power Macintosh, the first of many Apple computers to use Motorola's 
PowerPC processor.[90]

In 1996, Spindler was replaced by Gil Amelio as CEO. Hired for his reputation 
as a corporate rehabilitator, Amelio made deep changes, including extensive 
layoffs and cost-cutting.[91] After numerous failed attempts to modernize Mac 
OS, first with the Pink project from 1988 and later with Copland from 1994, 
Apple in 1997 purchased NeXT for its NeXTSTEP operating system and to bring 
Steve Jobs back.[92] Apple was only weeks away from bankruptcy when Jobs 
returned.[93]

1997–2007: Return to profitability


Power Macintosh is a line of Apple Macintosh workstation-class personal 
computers based on various models of PowerPC microprocessors, that were 
developed from 1994 to 2006.
The NeXT acquisition was finalized on February 9, 1997,[94] bringing Jobs back 
to Apple as an advisor. On July 9, 1997, Amelio was ousted by the board of 
directors after overseeing a three-year record-low stock price and crippling 
financial losses. Jobs acted as the interim CEO and began restructuring the 
company's product line; it was during this period that he identified the design 
talent of Jonathan Ive, and the pair worked collaboratively to rebuild Apple's 
status.[95]

At the August 1997 Macworld Expo in Boston, Jobs announced that Apple would 
join Microsoft to release new versions of Microsoft Office for the Macintosh, 
and that Microsoft had made a $150 million investment in non-voting Apple 
stock.[96] On November 10, 1997, Apple introduced the Apple Store website, 
which was tied to a new build-to-order manufacturing strategy.[97][98]

On August 15, 1998, Apple introduced a new all-in-one computer reminiscent of 
the Macintosh 128K: the iMac. The iMac design team was led by Ive, who would 
later design the iPod and the iPhone.[99][100] The iMac featured modern 
technology and a unique design, and sold almost 800,000 units in its first five 
months.[101]

During this period,[when?] Apple completed numerous acquisitions to create a 
portfolio of digital production software for both professionals and consumers. 
In 1998, Apple purchased Macromedia's Key Grip software project, signaling an 
expansion into the digital video editing market. The sale was an outcome of 
Macromedia's decision to solely focus on web development software. The product, 
still unfinished at the time of the sale, was renamed "Final Cut Pro" when it 
was launched on the retail market in April 1999.[102][103] The development of 
Key Grip also led to Apple's release of the consumer video-editing product 
iMovie in October 1999.[104] Next, Apple successfully acquired the German 
company Astarte, which had developed DVD authoring technology, as well as 
Astarte's corresponding products and engineering team in April 2000. Astarte's 
digital tool DVDirector was subsequently transformed into the 
professional-oriented DVD Studio Pro software product. Apple then employed the 
same technology to create iDVD for the consumer market.[104] In July 2001, 
Apple acquired Spruce Technologies, a PC DVD authoring platform, to incorporate 
their technology into Apple's expanding portfolio of digital video 
projects.[105][106]

SoundJam MP, released by Casady & Greene in 1998, was renamed "iTunes" when 
Apple purchased it in 2000. The primary developers of the MP3 player and music 
library software moved to Apple as part of the acquisition, and simplified 
SoundJam's user interface, added the ability to burn CDs, and removed its 
recording feature and skin support.[107] SoundJam was Apple's second choice for 
the core of Apple's music software project, originally codenamed 
iMusic,[108][109] behind Panic's Audion.[110] Apple was not able to set up a 
meeting with Panic in time to be fully considered as the latter was in the 
middle of similar negotiations with AOL.[110]

In 2002, Apple purchased Nothing Real for their advanced digital compositing 
application Shake,[111] as well as Emagic for the music productivity 
application Logic. The purchase of Emagic made Apple the first computer 
manufacturer to own a music software company. The acquisition was followed by 
the development of Apple's consumer-level GarageBand application.[112] The 
release of iPhoto in the same year completed the iLife suite.[113]

Mac OS X, based on NeXT's NeXTSTEP, OPENSTEPand BSD Unix, was released on March 
24, 2001, after several years of development. Aimed at consumers and 
professionals alike, Mac OS X aimed to combine the stability, reliability, and 
security of Unix with the ease of use afforded by an overhauled user interface. 
To aid users in migrating from Mac OS 9, the new operating system allowed the 
use of OS 9 applications within Mac OS X via the Classic Environment.[114]

On May 19, 2001, Apple opened its first official eponymous retail stores in 
Virginia and California.[115] On October 23 of the same year, Apple debuted the 
iPod portable digital audio player. The product, which was first sold on 
November 10, 2001, was phenomenally successful with over 100 million units sold 
within six years.[116][117] In 2003, Apple's iTunes Store was introduced. The 
service offered online music downloads for $0.99 a song and integration with 
the iPod. The iTunes Store quickly became the market leader in online music 
services, with over five billion downloads by June 19, 2008.[118][119] Two 
years later, the iTunes Store was the world's largest music retailer.[120][121]

Intel transition and financial stability
Main article: Apple's transition to Intel processors

The MacBook Pro, Apple's first laptop with an Intel microprocessor, introduced 
in 2006.
At the Worldwide Developers Conference keynote address on June 6, 2005, Jobs 
announced that Apple would begin producing Intel-based Mac computers in 
2006.[122] On January 10, 2006, the new MacBook Pro and iMac became the first 
Apple computers to use Intel's Core Duo CPU. By August 7, 2006, Apple made the 
transition to Intel chips for the entire Mac product line—over one year 
sooner than announced.[122] The Power Mac, iBook, and PowerBook brands were 
retired during the transition; the Mac Pro, MacBook, and MacBook Pro became 
their respective successors.[123][124] On April 29, 2009, The Wall Street 
Journal reported that Apple was building its own team of engineers to design 
microchips.[125] Apple also introduced Boot Camp in 2006 to help users install 
Windows XP or Windows Vista on their Intel Macs alongside Mac OS X.[126]

Apple's success during this period was evident in its stock price. Between 
early 2003 and 2006, the price of Apple's stock increased more than tenfold, 
from around $6 per share (split-adjusted) to over $80.[citation needed] When 
Apple surpassed Dell's market cap in January 2006,[127] Jobs sent an email to 
Apple employees saying Dell's CEO Michael Dell should eat his words.[128] Nine 
years prior, Dell had said that if he ran Apple he would "shut it down and give 
the money back to the shareholders".[129]

Although Apple's market share in computers had grown, it remained far behind 
its competitor Microsoft Windows, accounting for about 8% of desktops and 
laptops in the US.[when?][citation needed]

Since 2001, Apple's design team has progressively abandoned the use of 
translucent colored plastics first used in the iMac G3. This design change 
began with the titanium-made PowerBook and was followed by the iBook's white 
polycarbonate structure and the flat-panel iMac.[130][131]

2007–2011: Success with mobile devices

Newly announced iPhone on display at the 2007 MacWorld Expo
During his keynote speech at the Macworld Expo on January 9, 2007, Jobs 
announced that Apple Computer, Inc. would thereafter be known as "Apple Inc.", 
because the company had shifted its emphasis from computers to consumer 
electronics.[132][133] This event also saw the announcement of the 
iPhone[134][135] and the Apple TV.[136][137] The company sold 270,000 iPhone 
units during the first 30 hours of sales,[138] and the device was called "a 
game changer for the industry".[139] Apple would achieve widespread success 
with its iPhone, iPod Touch, and iPad products, which introduced innovations in 
mobile phones, portable music players, and personal computers 
respectively.[140] Furthermore, by early 2007, 800,000 Final Cut Pro users were 
registered.[141]

In an article posted on Apple's website on February 6, 2007, Jobs wrote that 
Apple would be willing to sell music on the iTunes Store without digital rights 
management (DRM), thereby allowing tracks to be played on third-party players, 
if record labels would agree to drop the technology.[142] On April 2, 2007, 
Apple and EMI jointly announced the removal of DRM technology from EMI's 
catalog in the iTunes Store, effective in May 2007.[143] Other record labels 
eventually followed suit and Apple published a press release in January 2009 to 
announce that all songs on the iTunes Store are available without their 
FairPlay DRM.[144]

In July 2008, Apple launched the App Store to sell third-party applications for 
the iPhone and iPod Touch.[145] Within a month, the store sold 60 million 
applications and registered an average daily revenue of $1 million, with Jobs 
speculating in August 2008 that the App Store could become a billion-dollar 
business for Apple.[146] By October 2008, Apple was the third-largest mobile 
handset supplier in the world due to the popularity of the iPhone.[147]

On December 16, 2008, Apple announced that 2009 would be the last year the 
corporation would attend the Macworld Expo, after more than 20 years of 
attendance, and that senior vice president of Worldwide Product Marketing Phil 
Schiller would deliver the 2009 keynote address in lieu of the expected Jobs. 
The official press release explained that Apple was "scaling back" on trade 
shows in general, including Macworld Tokyo and the Apple Expo in Paris, France, 
primarily because the enormous successes of the Apple Retail Stores and website 
had rendered trade shows a minor promotional channel.[148][149]

On January 14, 2009, Jobs announced in an internal memo that he would be taking 
a six-month medical leave of absence from Apple until the end of June 2009 and 
would spend the time focusing on his health. In the email, Jobs stated that 
"the curiosity over my personal health continues to be a distraction not only 
for me and my family, but everyone else at Apple as well", and explained that 
the break would allow the company "to focus on delivering extraordinary 
products".[150] Though Jobs was absent, Apple recorded its best non-holiday 
quarter (Q1 FY 2009) during the recession with revenue of $8.16 billion and 
profit of $1.21 billion.[151][152]

After years of speculation and multiple rumored "leaks", Apple unveiled a large 
screen, tablet-like media device known as the iPad on January 27, 2010. The 
iPad ran the same touch-based operating system as the iPhone, and all iPhone 
apps were compatible with the iPad. This gave the iPad a large app catalog on 
launch, though having very little development time before the release. Later 
that year on April 3, 2010, the iPad was launched in the US. It sold more than 
300,000 units on its first day, and 500,000 by the end of the first week.[153] 
In May of the same year, Apple's market cap exceeded that of competitor 
Microsoft for the first time since 1989.[154]

In June 2010, Apple released the iPhone 4,[155][156] which introduced video 
calling, multitasking, and a new uninsulated stainless steel design that acted 
as the phone's antenna. Later that year, Apple again refreshed its iPod line of 
MP3 players by introducing a multi-touch iPod Nano, an iPod Touch with 
FaceTime, and an iPod Shuffle that brought back the clickwheel buttons of 
earlier generations.[157][158][159] It also introduced the smaller, cheaper 
second generation Apple TV which allowed renting of movies and shows.[160]

In October 2010, Apple shares hit an all-time high, eclipsing $300 (~$43 split 
adjusted).[161] Later that month, Apple updated the MacBook Air laptop, iLife 
suite of applications, and unveiled Mac OS X Lion,[162][163] the last version 
with the name Mac OS X.[citation needed]

On January 6, 2011, the company opened its Mac App Store, a digital software 
distribution platform similar to the iOS App Store.[164]

On January 17, 2011, Jobs announced in an internal Apple memo that he would 
take another medical leave of absence for an indefinite period to allow him to 
focus on his health. Chief Operating Officer Tim Cook assumed Jobs's day-to-day 
operations at Apple, although Jobs would still remain "involved in major 
strategic decisions".[165] Apple became the most valuable consumer-facing brand 
in the world.[166] In June 2011, Jobs surprisingly took the stage and unveiled 
iCloud, an online storage and syncing service for music, photos, files, and 
software which replaced MobileMe, Apple's previous attempt at content 
syncing.[167] This would be the last product launch Jobs would attend before 
his death.

Alongside peer entities such as Atari and Cisco Systems, Apple was featured in 
the documentary Something Ventured, which premiered in 2011 and explored the 
three-decade era that led to the establishment and dominance of Silicon 
Valley.[168] It has been argued that Apple has achieved such efficiency in its 
supply chain that the company operates as a monopsony (one buyer with many 
sellers) and can dictate terms to its suppliers.[169][170][171] In July 2011, 
due to the American debt-ceiling crisis, Apple's financial reserves were 
briefly larger than those of the U.S. Government.[172]

On August 24, 2011, Jobs resigned his position as CEO of Apple.[173] He was 
replaced by Cook and Jobs became Apple's chairman. Apple did not have a 
chairman at the time[citation needed] and instead had two co-lead directors, 
Andrea Jung and Arthur D. Levinson,[citation needed] who continued with those 
titles until Levinson replaced Jobs as chairman of the board in November after 
Jobs' death.[174]

2011–present: Post–Steve Jobs era; Tim Cook leadership
On October 5, 2011, Steve Jobs died, marking the end of an era for 
Apple.[175][176] The first major product announcement by Apple following Jobs's 
passing occurred on January 19, 2012, when Apple's Phil Schiller introduced 
iBooks Textbooks for iOS and iBook Author for Mac OS X in New York City.[177] 
Jobs had stated in his biography that he wanted to reinvent the textbook 
industry and education.[178]

From 2011 to 2012, Apple released the iPhone 4S[179][180] and iPhone 
5,[181][182] which featured improved cameras, an intelligent software assistant 
named Siri, and cloud-synced data with iCloud; the third and fourth generation 
iPads, which featured Retina displays;[183][184][185] and the iPad Mini, which 
featured a 7.9-inch screen in contrast to the iPad's 9.7-inch screen.[186] 
These launches were successful, with the iPhone 5 (released September 21, 2012) 
becoming Apple's biggest iPhone launch with over two million pre-orders[187] 
and sales of three million iPads in three days following the launch of the iPad 
Mini and fourth generation iPad (released November 3, 2012).[188] Apple also 
released a third-generation 13-inch MacBook Pro with a Retina display and new 
iMac and Mac Mini computers.[185][186][189]

On August 20, 2012, Apple's rising stock price increased the company's market 
capitalization to a world-record $624 billion. This beat the 
non-inflation-adjusted record for market capitalization set by Microsoft in 
1999.[190] On August 24, 2012, a US jury ruled that Samsung should pay Apple 
$1.05 billion (£665m) in damages in an intellectual property lawsuit.[191] 
Samsung appealed the damages award, which the Court reduced by $450 
million.[192] The Court further granted Samsung's request for a new trial.[192] 
On November 10, 2012, Apple confirmed a global settlement that would dismiss 
all lawsuits between Apple and HTC up to that date, in favor of a ten-year 
license agreement for current and future patents between the two 
companies.[193] It is predicted that Apple will make $280 million a year from 
this deal with HTC.[194]

A previously confidential email written by Jobs a year before his death was 
presented during the proceedings of the Apple Inc. v. Samsung Electronics Co. 
lawsuits and became publicly available in early April 2014. With a subject line 
that reads "Top 100 – A," the email was sent only to the company's 100 most 
senior employees and outlines Jobs's vision of Apple Inc.'s future under 10 
subheadings. Notably, Jobs declares a "Holy War with Google" for 2011 and 
schedules a "new campus" for 2015.[195]

In March 2013, Apple filed a patent for an augmented reality (AR) system that 
can identify objects in a live video stream and present information 
corresponding to these objects through a computer-generated information layer 
overlaid on top of the real-world image.[196] The company also made several 
high-profile hiring decisions in 2013. On July 2, 2013, Apple recruited Paul 
Deneve, Belgian President and CEO of Yves Saint Laurent as a vice president 
reporting directly to Tim Cook.[197] A mid-October 2013 announcement revealed 
that Burberry CEO Angela Ahrendts will commence as a senior vice president at 
Apple in mid-2014. Ahrendts oversaw Burberry's digital strategy for almost 
eight years and, during her tenure, sales increased to about $3.2 billion and 
shares gained more than threefold.[198] She resigned from Apple in 2019.[199]

Alongside Google vice-president Vint Cerf and AT&T CEO Randall Stephenson, Cook 
attended a closed-door summit held by President Obama on August 8, 2013, in 
regard to government surveillance and the Internet in the wake of the Edward 
Snowden NSA incident.[200][201] On February 4, 2014, Cook met with Abdullah 
Gül, the President of Turkey, in Ankara to discuss the company's involvement 
in the Fatih project.[202]

In the first quarter of 2014, Apple reported sales of 51 million iPhones and 26 
million iPads, becoming all-time quarterly sales records. It also experienced a 
significant year-over-year increase in Mac sales. This was contrasted with a 
significant drop in iPod sales.[203][204] In May 2014, the company confirmed 
its intent to acquire Dr. Dre and Jimmy Iovine's audio company Beats 
Electronics—producer of the "Beats by Dr. Dre" line of headphones and speaker 
products, and operator of the music streaming service Beats Music—for $3 
billion, and to sell their products through Apple's retail outlets and 
resellers. Iovine believed that Beats had always "belonged" with Apple, as the 
company modeled itself after Apple's "unmatched ability to marry culture and 
technology." The acquisition was the largest purchase in Apple's 
history.[205][206]

Apple was at the top of Interbrand's annual Best Global Brands report for six 
consecutive years; 2013,[207] 2014,[208] 2015,[209] 2016,[210] 2017,[211] and 
2018 with a valuation of $214.48 billion.[212]

In January 2016, it was announced that one billion Apple devices were in active 
use worldwide.[213][214]

On May 12, 2016, Apple Inc., invested $1 billion in DiDi, a Chinese 
transportation network company.[215][216][217] The Information reported in 
October 2016 that Apple had taken a board seat in Didi Chuxing,[218] a move 
that James Vincent of The Verge speculated could be a strategic company 
decision by Apple to get closer to the automobile industry,[219] particularly 
Didi Chuxing's reported interest in self-driving cars.[220]

On June 6, 2016, Fortune released Fortune 500, their list of companies ranked 
on revenue generation. In the trailing fiscal year (2015), Apple appeared on 
the list as the top tech company.[221] It ranked third, overall, with $233 
billion in revenue.[221] This represents a movement upward of two spots from 
the previous year's list.[221]

On April 6, 2017, Apple launched Clips, an app that allows iPad and iPhone 
users to make and edit short videos with text, graphics, and effects. The app 
provides a way to produce short videos to share with other users on the 
Messages app, Instagram, Facebook, and other social networks. Apple also 
introduced Live Titles for Clips that allows users to add live animated 
captions and titles using their voice.[222]

In May 2017, Apple refreshed two of its website designs. Their public relations 
"Apple Press Info" website was changed to an "Apple Newsroom" site, featuring a 
greater emphasis on imagery and therefore lower information density, and 
combines press releases, news items, and photos. Its "Apple Leadership" 
overview of company executives was also refreshed, adding a simpler layout with 
a prominent header image and two-column text fields. 9to5Mac noted the design 
similarities to several of Apple's redesigned apps in iOS 10, particularly its 
Apple Music and News software.[223]

In June 2017, Apple announced the HomePod, its smart speaker aimed to compete 
against Sonos, Google Home, and Amazon Echo.[224] Towards the end of the year, 
TechCrunch reported that Apple was acquiring Shazam, a company specializing in 
music, TV, film and advertising recognition.[225] The acquisition was confirmed 
a few days later, reportedly costing Apple $400 million, with media reports 
noting that the purchase looked like a move by Apple to get data and tools to 
bolster its Apple Music streaming service.[226] The purchase was approved by EU 
later in September 2018.[227][228]

Also in June 2017, Apple appointed Jamie Erlicht and Zack Van Amburg to head 
the newly formed worldwide video unit. In November 2017, Apple announced it was 
branching out into original scripted programming: a drama series starring 
Jennifer Aniston and Reese Witherspoon, and a reboot of the anthology series 
Amazing Stories with Steven Spielberg.[229] In June 2018, Apple signed the 
Writer's Guild of America's minimum basic agreement and Oprah Winfrey to a 
multi-year content partnership.[230][231] Additional partnerships for original 
series include Sesame Workshop and DHX Media and its subsidiary Peanuts 
Worldwide, as well as a partnership with A24 to create original 
films.[232][233][234] As of January 2019, Apple has ordered twenty-one 
television series and one film. There are five series in development at Apple.

On June 5, 2018, Apple deprecated OpenGL and OpenGL ES across all operating 
systems and urged developers to use Metal instead.[235]

In August 2018, Apple purchased Akonia Holographics for its augmented reality 
goggle lens.[236][237] On February 14, 2019, Apple acquired DataTiger for its 
digital marketing technology.[238]

On January 29, 2019, Apple reported its first decline in revenues and profits 
in a decade.[239][240][241]

In February 2019 they bought Conversational computing company PullString 
(formerly ToyTalk)[242]

On July 25, 2019, Apple and Intel announced an agreement for Apple to acquire 
the smartphone modem business of Intel Mobile Communications for US$1 
billion.[243]

Products
See also: Timeline of Apple Inc. products
Macintosh
Main article: Macintosh
See also: Timeline of Macintosh models, List of Macintosh models grouped by CPU 
type, and List of Macintosh models by case type
Macintoshes currently in production:

iMac: Consumer all-in-one desktop computer, introduced in 1998.
Mac Mini: Consumer sub-desktop computer, introduced in 2005.
MacBook Pro: Professional notebook, introduced in 2006.
Mac Pro: Workstation desktop computer, introduced in 2006.
MacBook Air: Consumer ultra-thin, ultra-portable notebook, introduced in 2008.
Apple sells a variety of computer accessories for Macs, including Thunderbolt 
Display, Magic Mouse, Magic Trackpad, Magic Keyboard, the AirPort wireless 
networking products, and Time Capsule.

iPod
Main article: iPod

From left to right: iPod Shuffle, iPod Nano, iPod Touch.
On October 23, 2001, Apple introduced the iPod digital music player. Several 
updated models have since been introduced, and the iPod brand is now the market 
leader in portable music players by a significant margin. More than 390 million 
units have shipped as of September 2015.[244] Apple has partnered with Nike to 
offer the Nike+iPod Sports Kit, enabling runners to synchronize and monitor 
their runs with iTunes and the Nike+ website.

In late July 2017, Apple discontinued its iPod Nano and iPod Shuffle models, 
leaving only the iPod Touch available for purchase.[245][246][247]

iPhone
Main article: iPhone

The first-generation iPhone, 3G, 4, 5, 5C and 5S to scale.
At the Macworld Conference & Expo in January 2007, Steve Jobs introduced the 
long-anticipated[248] iPhone, a convergence of an Internet-enabled smartphone 
and iPod.[249] The first-generation iPhone was released on June 29, 2007, for 
$499 (4 GB) and $599 (8 GB) with an AT&T contract.[250] On February 5, 2008, it 
was updated to have 16 GB of memory, in addition to the 8 GB and 4 GB 
models.[251] It combined a 2.5G quad band GSM and EDGE cellular phone with 
features found in handheld devices, running a scaled-down version of OS X 
(dubbed iPhone OS after the launch and later renamed to iOS), with various Mac 
OS X applications such as Safari and Mail. It also includes web-based and 
Dashboard apps such as Google Maps and Weather. The iPhone features a 3.5-inch 
(89 mm) touchscreen display, Bluetooth, and Wi-Fi (both "b" and "g").[249]

A second version, the iPhone 3G, was released on July 11, 2008, with a reduced 
price of $199 for the 8 GB model and $299 for the 16 GB model.[252] This 
version added support for 3G networking and assisted GPS navigation. The flat 
silver back and large antenna square of the original model were eliminated in 
favor of a glossy, curved black or white back. Software capabilities were 
improved with the release of the App Store, which provided iPhone-compatible 
applications to download. On April 24, 2009, the App Store[253] surpassed one 
billion downloads.[254] On June 8, 2009, Apple announced the iPhone 3GS. It 
provided an incremental update to the device, including faster internal 
components, support for faster 3G speeds, video recording capability, and voice 
control.

At the Worldwide Developers Conference (WWDC) on June 7, 2010, Apple announced 
the redesigned iPhone 4.[255] It featured a 960 × 640 display, the Apple A4 
processor, a gyroscope for enhanced gaming, a 5MP camera with LED flash, 
front-facing VGA camera and FaceTime video calling. Shortly after its release, 
reception issues were discovered by consumers, due to the stainless steel band 
around the edge of the device, which also serves as the phone's cellular signal 
and Wi-Fi antenna. The issue was corrected by a "Bumper Case" distributed by 
Apple for free to all owners for a few months. In June 2011, Apple overtook 
Nokia to become the world's biggest smartphone maker by volume.[256] On October 
4, 2011, Apple unveiled the iPhone 4S, which was first released on October 14, 
2011.[257] It features the Apple A5 processor and Siri voice assistant 
technology, the latter of which Apple had acquired in 2010 from SRI 
International Artificial Intelligence Center.[258] It also features an updated 
8MP camera with new optics. Apple began a new accessibility feature, Made for 
iPhone Hearing Aids with the iPhone 4S.[259] Made for iPhone Hearing Aids 
feature Live Listen, it can help the user hear a conversation in a noisy room 
or hear someone speaking across the room.[260] Apple sold 4 million iPhone 4S 
phones in the first three days of availability.[261]

On September 12, 2012, Apple introduced the iPhone 5.[262] It has a 4-inch 
display, 4G LTE connectivity, and the upgraded Apple A6 chip, among several 
other improvements.[263] Two million iPhones were sold in the first twenty-four 
hours of pre-ordering[264] and over five million handsets were sold in the 
first three days of its launch.[265] Upon the launch of the iPhone 5S and 
iPhone 5C, Apple set a new record for first-weekend smartphone sales by selling 
over nine million devices in the first three days of its launch.[266] The 
release of the iPhone 5S and 5C is the first time that Apple simultaneously 
launched two models.[267]

A patent filed in July 2013 revealed the development of a new iPhone battery 
system that uses location data in combination with data on the user's habits to 
moderate the handsets' power settings accordingly. Apple is working towards a 
power management system that will provide features such as the ability of the 
iPhone to estimate the length of time a user will be away from a power source 
to modify energy usage and a detection function that adjusts the charging rate 
to best suit the type of power source that is being used.[268]

In a March 2014 interview, Apple designer Jonathan Ive used the iPhone as an 
example of Apple's ethos of creating high-quality, life-changing products. He 
explained that the phones are comparatively expensive due to the intensive 
effort that is used to make them:

We don't take so long and make the way we make for fiscal reasons ... Quite the 
reverse. The body is made from a single piece of machined aluminum... The whole 
thing is polished first to a mirror finish and then is very finely textured, 
except for the Apple logo. The chamfers [smoothed-off edges] are cut with 
diamond-tipped cutters. The cutters don't usually last very long, so we had to 
figure out a way of mass-manufacturing long-lasting ones. The camera cover is 
sapphire crystal. Look at the details around the SIM-card slot. It's 
extraordinary![95]

On September 9, 2014, Apple introduced the iPhone 6, alongside the iPhone 6 
Plus that both have screen sizes over 4-inches.[269] One year later, Apple 
introduced the iPhone 6S, and iPhone 6S Plus, which introduced a new technology 
called 3D Touch, including an increase of the rear camera to 12 MP, and the 
FaceTime camera to 5 MP.[270] On March 21, 2016, Apple introduced the iPhone SE 
that has a 4-inch screen size last used with the 5S and has nearly the same 
internal hardware as the 6S.[271]

In July 2016, Apple announced that one billion iPhones had been sold.[272][273]

On September 7, 2016, Apple introduced the iPhone 7 and the iPhone 7 Plus, 
which feature improved system and graphics performance, add water resistance, a 
new rear dual-camera system on the 7 Plus model, and, controversially, remove 
the 3.5 mm headphone jack.[274][275]


A gold iPhone 8 alongside a gold 8 Plus.
On September 12, 2017, Apple introduced the iPhone 8 and iPhone 8 Plus, 
standing as evolutionary updates to its previous phones with a faster 
processor, improved display technology, upgraded camera systems and wireless 
charging.[276] The company also announced iPhone X, which radically changes the 
hardware of the iPhone lineup, removing the home button in favor of facial 
recognition technology and featuring a near bezel-less design along with 
wireless charging.[277][278]

On September 12, 2018, Apple introduced the iPhone XS, iPhone XS Max and iPhone 
XR. The iPhone XS and iPhone XS Max features Super Retina displays, a faster 
and improved dual camera system that offers breakthrough photo and video 
features, the first 7-nanometer chip in a smartphone — the A12 Bionic chip 
with next-generation Neural Engine — faster Face ID, wider stereo sound and 
introduces Dual SIM to iPhone. The iPhone XR comes in an all-screen glass and 
aluminium design with the most advanced LCD in a smartphone featuring a 
6.1-inch Liquid Retina display, A12 Bionic chip with next-generation Neural 
Engine, the TrueDepth camera system, Face ID and an advanced camera system that 
creates dramatic portraits using a single camera lens.[279][280]

On September 10, 2019, Apple introduced the iPhone 11, iPhone 11 Pro, and the 
iPhone 11 Pro Max. The iPhone 11 features the same Liquid Retina LCD display 
used in 2018's iPhone XR. Overall, the iPhone 11 retains the same glass and 
aluminum design as the iPhone XR while adding in new features such as the 
addition of an Ultra-Wide 12mp camera, a battery that lasts 1 hour longer than 
the iPhone XR and an IP 68 rating. The iPhone 11 Pro and iPhone 11 Pro Max 
features an all-new textured matte glass and stainless steel design and a 
triple camera setup that included an Ultra Wide, Wide and Telephoto camera. The 
iPhone 11 Pro series' battery life was capable of lasting up to 5 hours more 
than the iPhone XS and XS Max. The iPhone 11 Pro and Pro Max also features a 
new Super Retina XDR OLED display that was capable of a screen brightness of 
800 nits. All new iPhones announced at Apple's September 2019 feature an A13 
Bionic chip with a third-generation Neural Engine, an Apple U1 chip, spatial 
audio playback, a low light photo mode and an improved Face ID 
system.[281][282]

iPad
Main article: iPad
On January 27, 2010, Apple introduced their much-anticipated media tablet, the 
iPad.[283][284] It offers multi-touch interaction with multimedia formats 
including newspapers, e-books, photos, videos, music, word processing 
documents, video games, and most existing iPhone apps using a 9.7-inch 
screen.[285] It also includes a mobile version of Safari for web browsing, as 
well as access to the App Store, iTunes Library, iBookstore, Contacts, and 
Notes. Content is downloadable via Wi-Fi and optional 3G service or synced 
through the user's computer.[286] AT&T was initially the sole U.S. provider of 
3G wireless access for the iPad.[287]


iPad Air 2 in gold
On March 2, 2011, Apple introduced the iPad 2, which had a faster processor and 
a camera on the front and back. It also added support for optional 3G service 
provided by Verizon in addition to AT&T.[288] The availability of the iPad 2 
was initially limited as a result of a devastating earthquake and tsunami in 
Japan in March 2011.[289]

The third-generation iPad was released on March 7, 2012, and marketed as "the 
new iPad". It added LTE service from AT&T or Verizon, an upgraded A5X 
processor, and Retina display. The dimensions and form factor remained 
relatively unchanged, with the new iPad being a fraction thicker and heavier 
than the previous version and featuring minor positioning changes.[290]


The iPad Pro 2nd generation
On October 23, 2012, Apple's fourth-generation iPad came out, marketed as the 
"iPad with Retina display". It added the upgraded A6X processor and replaced 
the traditional 30-pin dock connector with the all-digital Lightning 
connector.[291] The iPad Mini was also introduced. It featured a reduced 
7.9-inch display and much of the same internal specifications as the iPad 
2.[292]

On October 22, 2013, Apple introduced the iPad Air and the iPad Mini with 
Retina Display, both featuring a new 64-bit Apple A7 processor.[293]

The iPad Air 2 was unveiled on October 16, 2014. It added better graphics and 
central processing and a camera burst mode as well as minor updates. The iPad 
Mini 3 was unveiled at the same time.[293]

Since its launch, iPad users have downloaded over three billion apps. The total 
number of App Store downloads, as of June 2015, is over 100 billion.[294]

On September 9, 2015, Apple announced the iPad Pro, an iPad with a 12.9-inch 
display that supports two new accessories, the Smart Keyboard and Apple 
Pencil.[295] An updated IPad Mini 4 was announced at the same time.[296] A 
9.7-inch iPad Pro was announced on March 21, 2016.[297] On June 5, 2017, Apple 
announced a new iPad Pro with a 10.5-inch display to replace the 9.7 inch model 
and an updated 12.9-inch model.[298]

Apple Watch
Main article: Apple Watch

The Apple Watch quickly became the best-selling wearable device, with the 
shipment of 11.4 million smart watches in the first half of 2015, according to 
analyst firm Canalys.[299]
The original Apple Watch smartwatch was announced by Tim Cook on September 9, 
2014, being introduced as a product with health and fitness-tracking.[300][301] 
It was released on April 24, 2015.[302][303][304]

The second generation of Apple Watch, Apple Watch Series 2, was released in 
September 2016, featuring greater water resistance, a faster processor, and 
brighter display. It was also released alongside a cheaper Series 1.[305]

On September 12, 2017, Apple introduced the Apple Watch Series 3 featuring LTE 
cellular connectivity, giving the wearable independence from an iPhone[306] 
except for the setup process.[307]

On September 12, 2018, Apple introduced the Apple Watch Series 4, featuring new 
display, electrocardiogram, and fall detection.[308]

On September 10, 2019, Apple introduced the Apple Watch Series 5, featuring a 
new magnetometer, a faster processor, and a new always-on display. The Series 4 
was discontinued.

Apple TV
Main article: Apple TV
At the 2007 Macworld conference, Jobs demonstrated the Apple TV (Jobs 
accidentally referred to the device as "iTV", its codename, while on stage), a 
set-top video device intended to bridge the sale of content from iTunes with 
high-definition televisions.[309] The device, running a variant of Mac OS X, 
links up to a user's TV and syncs over the wireless or wired network with one 
computer's iTunes library and can stream content from an additional four. The 
Apple TV originally incorporated a 40 GB hard drive for storage, included 
outputs for HDMI and component video, and played video at a maximum resolution 
of 720p.[137] On May 30, 2007, a 160 GB hard disk drive was released alongside 
the existing 40 GB model.[310] A software update released on January 15, 2008, 
allowed media to be purchased directly from the Apple TV.[311]

In September 2009, Apple discontinued the original 40 GB Apple TV but continued 
to produce and sell the 160 GB Apple TV. On September 1, 2010, Apple released a 
completely redesigned Apple TV running on an iOS variant and discontinued the 
older model, which ran on a Mac OS X variant. The new device is one-fourth the 
size, runs quieter, and replaces the need for a hard drive with media streaming 
from any iTunes library on the network along with 8 GB of flash memory to cache 
downloaded media. Like the iPad and the iPhone, Apple TV runs on an A4 
processor. The memory included in the device is half of that in the iPhone 4 at 
256 MB; the same as the iPad, iPhone 3GS, third and fourth-generation iPod 
Touch.[312]

It has HDMI out as the only video output source. Features include access to the 
iTunes Store to rent movies and TV shows (purchasing has been discontinued), 
streaming from internet video sources, including YouTube and Netflix, and media 
streaming from an iTunes library. Apple also reduced the price of the device to 
$99. A third generation of the device was introduced at an Apple event on March 
7, 2012, with new features such as higher resolution (1080p) and a new user 
interface.

At the September 9, 2015, event, Apple unveiled an overhauled Apple TV, which 
now runs a subsequent variant of iOS called tvOS, and contains 32 GB or 64 GB 
of NAND Flash to store games, programs, and to cache the current media playing. 
The release also coincided with the opening of a separate Apple TV App Store 
and a new Siri Remote with a glass touchpad, gyroscope, and microphone.

On December 12, 2016, Apple released a new iOS and tvOS media player app called 
TV to replace the existing "Videos" iOS application.

At the September 12, 2017, event, Apple released a new 4K Apple TV with the 
same form factor as the 4th Generation model. The 4K model is powered by the 
A10X SoC designed in-house that also powers their second-generation iPad Pro. 
The 4K model also has support for high dynamic range.

On March 25, 2019, Apple announced Apple TV+, their upcoming over-the-top 
subscription video on-demand web television service, will arrive Fall 2019. TV+ 
features exclusive original shows, movies, and documentaries.[313] They also 
announced an update to the TV app with a new "Channels" feature and that the TV 
app will expand to macOS, numerous smart television models, Roku devices, and 
Amazon Fire TV devices later in 2019.

HomePod
Main article: HomePod

A white HomePod on display
Apple's first smart speaker, the HomePod was released on February 9, 2018, 
after being delayed from its initial December 2017 release. It also features 
seven tweeters in the base, a four-inch woofer in the top, and six microphones 
for voice control and acoustic optimization[314][315][316] On September 12, 
2018, Apple announced that HomePod is adding new features—search by lyrics, 
set multiple timers, make and receive phone calls, Find My iPhone, Siri 
Shortcuts—and Siri languages.[317] In 2019, Apple, Google, Amazon, and Zigbee 
Alliance announced a partnership to make smart home products work 
together.[318]

Software
Main articles: macOS, iOS, watchOS, and tvOS

Apple Worldwide Developers Conference is held annually by Apple to showcase its 
new software and technologies for software developers.
Apple develops its own operating systems to run on its devices, including macOS 
for Mac personal computers,[319] iOS for its iPhone, iPad and iPod Touch 
smartphones and tablets,[320] watchOS for its Apple Watch smartwatches,[321] 
and tvOS for its Apple TV digital media player.[322]

For iOS and macOS, Apple also develops its own software titles, including Pages 
for writing, Numbers for spreadsheets, and Keynote for presentations, as part 
of its iWork productivity suite.[323] For macOS, it also offers iMovie and 
Final Cut Pro X for video editing,[324] and GarageBand and Logic Pro X for 
music creation.[325]

Apple's range of server software includes the operating system macOS 
Server;[326] Apple Remote Desktop, a remote systems management 
application;[327] and Xsan, a storage area network file system.[326]

Apple also offers online services with iCloud, which provides cloud storage and 
synchronization for a wide range of user data, including documents, photos, 
music, device backups, and application data,[328] and Apple Music, its music 
and video streaming service.[329]

Electric vehicles
Main article: Apple electric car project
According to the Sydney Morning Herald, Apple wants to start producing an 
electric car with autonomous driving as soon as 2020. Apple has made efforts to 
recruit battery development engineers and other electric automobile engineers 
from A123 Systems, LG Chem, Samsung Electronics, Panasonic, Toshiba, Johnson 
Controls and Tesla Motors.[330]

Corporate identity
Logo

First Apple logo
(1976–77)[331]

First official logo
(1977–98)[331]

Apple third logo
(1998–03)[331]

Current logo
(since 2003)[331]
See also: Typography of Apple Inc.
"Apple logo" redirects here. For the programming language, see Apple Logo.
According to Steve Jobs, the company's name was inspired by his visit to an 
apple farm while on a fruitarian diet. Jobs thought the name "Apple" was "fun, 
spirited and not intimidating".[332]

Apple's first logo, designed by Ron Wayne, depicts Sir Isaac Newton sitting 
under an apple tree. It was almost immediately replaced by Rob Janoff's 
"rainbow Apple", the now-familiar rainbow-colored silhouette of an apple with a 
bite taken out of it. Janoff presented Jobs with several different 
monochromatic themes for the "bitten" logo, and Jobs immediately took a liking 
to it. However, Jobs insisted that the logo be colorized to humanize the 
company.[333][334] The logo was designed with a bite so that it would not be 
confused with a cherry.[335] The colored stripes were conceived to make the 
logo more accessible, and to represent the fact the Apple II could generate 
graphics in color.[335] This logo is often erroneously referred to as a tribute 
to Alan Turing, with the bite mark a reference to his method of 
suicide.[336][337] Both Janoff and Apple deny any homage to Turing in the 
design of the logo.[335][338]

On August 27, 1999[339] (the year following the introduction of the iMac G3), 
Apple officially dropped the rainbow scheme and began to use monochromatic 
logos nearly identical in shape to the previous rainbow incarnation. An 
Aqua-themed version of the monochrome logo was used from 1998 to 2003, and a 
glass-themed version was used from 2007 to 2013.[340]

Steve Jobs and Steve Wozniak were Beatles fans,[341][342] but Apple Inc. had 
name and logo trademark issues with Apple Corps Ltd., a multimedia company 
started by the Beatles in 1968. This resulted in a series of lawsuits and 
tension between the two companies. These issues ended with the settling of 
their lawsuit in 2007.[343]

Advertising
Main article: Apple Inc. advertising
Apple's first slogan, "Byte into an Apple", was coined in the late 1970s.[344] 
From 1997 to 2002, the slogan "Think Different" was used in advertising 
campaigns, and is still closely associated with Apple.[345] Apple also has 
slogans for specific product lines — for example, "iThink, therefore iMac" 
was used in 1998 to promote the iMac,[346] and "Say hello to iPhone" has been 
used in iPhone advertisements.[347] "Hello" was also used to introduce the 
original Macintosh, Newton, iMac ("hello (again)"), and iPod.[348]

From the introduction of the Macintosh in 1984, with the 1984 Super Bowl 
advertisement to the more modern Get a Mac adverts, Apple has been recognized 
for its efforts towards effective advertising and marketing for its products. 
However, claims made by later campaigns were criticized,[349] particularly the 
2005 Power Mac ads.[350] Apple's product advertisements gained a lot of 
attention as a result of their eye-popping graphics and catchy tunes.[351] 
Musicians who benefited from an improved profile as a result of their songs 
being included on Apple advertisements include Canadian singer Feist with the 
song "1234" and Yael Naïm with the song "New Soul".[351]

Apple owns a YouTube channel where they release advertisements, tips, and 
introductions for their devices.[352][353]

Brand Semiotics
Semiotics is the study of how meaning is derived from symbols and signs and 
provides major insight for understanding brand management and brand loyalty. 
Ferdinand de Saussure, a Swiss linguist and semiotician, created a semiotic 
model that identifies two parts of a sign: the signified and signifier. The 
signifier is the perceptual component that we physically see, and the signified 
is then the concept which the sign refers to. In Saussure’s model, the sign 
results from the recognition of a sound or object with a concept. In his model, 
the signified and signifier are “as inseparable as two sides of a piece of 
paper".[354] The second popular semiotic model that exists is the Peircean 
Model. Charles Sanders Pierce was a logician. His model, like Saussure’s 
model, involved the relationship between the elements of signs and objects. 
However, the Peircean model added that whoever is decoding the sign must have 
some previous understanding or knowledge about the transmitted message.[355] 
Peirce’s model can be represented using the three sides of triangle: the 
representamen (the sign), an object (what the sign represents), and the 
interpretant (the produced effect by the sign).[354]

The symbolic representation that a brand carries can affect how a consumer 
“recalls, internalizes, and relates” to the performance of a company. There 
is plenty of evidence to show that a company can easily fail if they do not 
keep track of how the brand changes with the media culture. Semiotic research 
can be used to help a company relate to their customer’s culture over time 
and help their brand to stand out in competitive markets [356].

The first two Apple logos are drastically different from each other. However, 
they both share the sign of an apple. In the original logo designed by Ronald 
Wayne, Sir Isaac Newton is seen sitting under the infamous apple tree about to 
bear fruit above, just before his discovery of gravity. When one analyzes the 
semiotics with Saussure’s model, the signified, or sign, is the apple. The 
signifier represents discovery, innovation, and the notion of thought.[357]

It was quickly realized that the original logo was too complicated and 
intellectual for the needed purpose. The company’s mission was, and still is, 
to simplify technology for the everyday life. A fun and clever logo that spoke 
to computer-savvy people was needed. In 1977, Rob Janoff created the iconic 
rainbow apple symbol that is still recognized today. The logo had double 
meaning and differed from the many serious logos in existence at the time 
[358].

Apple Inc. is well known for being an innovative company who challenge the 
status quo and established standards. Again, using Saussure’s semiotic model, 
the signified, is an apple, but with a bite taken out of it. Because Apple is 
seen as a “challenger” in the industry, the most common signifier is the 
forbidden fruit from the Biblical reference, the Garden of Eden.[359] The 
signified is the bite from the apple, and the represented signifier is the tree 
of knowledge. Thus, symbolizing Apple as a “rebellious young company” ready 
to challenge the world.[357] An even further semiotic analysis is that Adam and 
Eve gained a great deal of knowledge after the apple was bitten. So, the bitten 
apple is a sign for the “promise of knowledge” that an Apple user may gain 
from the product.

The semiotics of the bite and the color of the logo can also be looked at from 
a technologic viewpoint. The bite is the signified and the computer storage 
unit, byte, is the signifier. The rainbow color of the logo portrays the 
message that the its computer monitor could be produce color images.[357] Steve 
Jobs also argued that color was crucial for “humanizing the company” at 
that time.[359]

The only thing to change with the logo since 1977 has been the color. In 1998, 
a monochromatic logo was implemented with the release of the first iMac. This 
was the first Mac to not have the iconic rainbow-colored apple since its 
creation 20 years prior.[360] The new look was the future or new era of Apple 
Inc. The color of the logo may have changed; however, the Apple logo has become 
an iconic signifier for innovation and knowledge to an entire culture of Apple 
users.[359] The logo’s shape had become untouchable and Apple’s message was 
as clear as ever, it’s better to be different.[360]

Brand loyalty
"The scenes I witnessed at the opening of the new Apple store in London's 
Covent Garden were more like an evangelical prayer meeting than a chance to buy 
a phone or a laptop."
—Alex Riley, writing for the BBC[361]
Apple customers gained a reputation for devotion and loyalty early in the 
company's history. In 1984, BYTE stated that:[362]

There are two kinds of people in the world: people who say Apple isn't just a 
company, it's a cause; and people who say Apple isn't a cause, it's just a 
company. Both groups are right. Nature has suspended the principle of 
noncontradiction where Apple is concerned. Apple is more than just a company 
because its founding has some of the qualities of myth ... Apple is two guys in 
a garage undertaking the mission of bringing computing power, once reserved for 
big corporations, to ordinary individuals with ordinary budgets. The company's 
growth from two guys to a billion-dollar corporation exemplifies the American 
Dream. Even as a large corporation, Apple plays David to IBM's Goliath, and 
thus has the sympathetic role in that myth.


Apple aficionados wait in line around an Apple Store in Shanghai in 
anticipation of a new product.
Apple evangelists were actively engaged by the company at one time, but this 
was after the phenomenon had already been firmly established. Apple evangelist 
Guy Kawasaki has called the brand fanaticism "something that was stumbled 
upon,"[363] while Ive explained in 2014 that "People have an incredibly 
personal relationship" with Apple's products.[95] Apple Store openings and new 
product releases can draw crowds of hundreds, with some waiting in line as much 
as a day before the opening.[364][365][366][367] The opening of New York City's 
Fifth Avenue "Cube" store in 2006 became the setting of a marriage proposal, 
and had visitors from Europe who flew in for the event.[368] In June 2017, a 
newlywed couple took their wedding photos inside the then-recently opened 
Orchard Road Apple Store in Singapore.[369][370] The high level of brand 
loyalty has been criticized and ridiculed, applying the epithet "Apple fanboy" 
and mocking the lengthy lines before a product launch.[371] An internal memo 
leaked in 2015 suggested the company planned to discourage long lines and 
direct customers to purchase its products on its website.[372]

Fortune magazine named Apple the most admired company in the United States in 
2008, and in the world from 2008 to 2012.[373][374][375][376][377] On September 
30, 2013, Apple surpassed Coca-Cola to become the world's most valuable brand 
in the Omnicom Group's "Best Global Brands" report.[378] Boston Consulting 
Group has ranked Apple as the world's most innovative brand every year since 
2005.[379]

The New York Times in 1985 stated that "Apple above all else is a marketing 
company".[380] John Sculley agreed, telling The Guardian newspaper in 1997 that 
"People talk about technology, but Apple was a marketing company. It was the 
marketing company of the decade."[381] Research in 2002 by NetRatings indicate 
that the average Apple consumer was usually more affluent and better educated 
than other PC company consumers. The research indicated that this correlation 
could stem from the fact that on average Apple Inc. products were more 
expensive than other PC products.[382][383]

In response to a query about the devotion of loyal Apple consumers, Jonathan 
Ive responded:
What people are responding to is much bigger than the object. They are 
responding to something rare—a group of people who do more than simply make 
something work, they make the very best products they possibly can. It's a 
demonstration against thoughtlessness and carelessness.[95]

Home page
The Apple website home page has been used to commemorate, or pay tribute to, 
milestones and events outside of Apple's product offerings, including:

2020: International Women's Day [384]
2020: Martin Luther King Jr.[385]
2019: Martin Luther King Jr.[386]
2018: Martin Luther King Jr.[387]
2017: Martin Luther King Jr.[388]
2016: Muhammad Ali[389]
2016: Bill Campbell (board member and friend)[390]
2016: Martin Luther King Jr.[391]
2015: Martin Luther King Jr.[392]
2014: Robin Williams[393]
2013: Nelson Mandela[394]
2012: Steve Jobs[395]
2011: Steve Jobs[396]
2010: Jerome B. York (board member)[397]
2007: Al Gore (board member; in honor of his Nobel Peace Prize)[398]
2005: Rosa Parks[399]
2003: Gregory Hines[400]
2001: George Harrison[401]
Headquarters
Main articles: Apple Park and Apple Campus
Apple Inc.'s world corporate headquarters are located in the middle of Silicon 
Valley, at 1–6 Infinite Loop, Cupertino, California. This Apple campus has 
six buildings that total 850,000 square feet (79,000 m2) and was built in 1993 
by Sobrato Development Cos.[402]

Apple has a satellite campus in neighboring Sunnyvale, California, where it 
houses a testing and research laboratory.[403] AppleInsider claimed in March 
2014 that Apple has a top-secret facility for development of the SG5 electric 
vehicle project codenamed "Titan" under the shell company name SixtyEight 
Research.[404]


Panorama of the Auditorium in the Steve Jobs Theater at Apple Park in 2018
In 2006, Apple announced its intention to build a second campus in Cupertino 
about 1 mile (1.6 km) east of the current campus and next to Interstate 
280.[405] The new campus building has been designed by Norman Foster.[406] The 
Cupertino City Council approved the proposed "spaceship" design campus on 
October 15, 2013, after a 2011 presentation by Jobs detailing the architectural 
design of the new building and its environs. The new campus is planned to house 
up to 13,000 employees in one central, four-storied, circular building 
surrounded by extensive landscape. It will feature a café with room for 3,000 
sitting people and parking underground as well as in a parking structure. The 
2.8 million square foot facility will also include Jobs's original designs for 
a fitness center and a corporate auditorium.[407]


External view of the Steve Jobs Theater at Apple Park in 2018
Apple has expanded its campuses in Austin, Texas, concurrently with building 
Apple Park in Cupertino. The expansion consists of two locations, with one 
having 1.1 million square feet (100,000 m2) of workspace, and the other 216,000 
square feet (20,100 m2).[408] Apple will invest $1 billion to build the North 
Austin campus.[409] At the biggest location, 6,000 employees work on technical 
support, manage Apple's network of suppliers to fulfill product shipments, aid 
in maintaining iTunes Store and App Store, handle economy, and continuously 
update Apple Maps with new data. At its smaller campus, 500 engineers work on 
next-generation processor chips to run in future Apple products.[410]

Apple's headquarters for Europe, the Middle East and Africa (EMEA) are located 
in Cork in the south of Ireland.[411][412][413][414][415][416][417] The 
facility, which opened in 1980, is Apple's first location outside of the United 
States.[418] Apple Sales International, which deals with all of Apple's 
international sales outside of the US, is located at Apple's campus in 
Cork[419] along with Apple Distribution International, which similarly deals 
with Apple's international distribution network.[420] On April 20, 2012, Apple 
added 500 new jobs at its European headquarters, increasing the total workforce 
from around 2,800 to 3,300 employees.[407][412][421] The company will build a 
new office block on its Hollyhill Campus to accommodate the additional 
staff.[422] Its United Kingdom headquarters is at Stockley Park on the 
outskirts of London.[423]

In February 2015, Apple opened their new 180,000-square-foot headquarters in 
Herzliya, Israel, designed to accommodate approximately 800 employees. This is 
Apple's third office located within Israel; the first, also in Herzliya, was 
obtained as part of the Anobit acquisition, and the other is a research center 
in Haifa.[424][425]

In December 2015, Apple bought the 70,000-square-foot manufacturing facility in 
North San Jose previously used by Maxim Integrated in an $18.2 million 
deal.[426][427]

Stores
Main article: Apple Store

Fifth Avenue, New York City

Omotesando, Tokyo, Japan
The first Apple Stores were originally opened as two locations in May 2001 by 
then-CEO Steve Jobs,[428] after years of attempting but failing 
store-within-a-store concepts.[429] Seeing a need for improved retail 
presentation of the company's products, he began an effort in 1997 to revamp 
the retail program to get an improved relationship to consumers, and hired Ron 
Johnson in 2000.[429] Jobs relaunched Apple's online store in 1997,[430] and 
opened the first two physical stores in 2001.[428] The media initially 
speculated that Apple would fail,[431] but its stores were highly successful, 
bypassing the sales numbers of competing nearby stores and within three years 
reached US$1 billion in annual sales, becoming the fastest retailer in history 
to do so.[431] Over the years, Apple has expanded the number of retail 
locations and its geographical coverage, with 499 stores across 22 countries 
worldwide as of December 2017.[432] Strong product sales have placed Apple 
among the top-tier retail stores, with sales over $16 billion globally in 
2011.[433]

In May 2016, Angela Ahrendts, Apple's then Senior Vice President of Retail, 
unveiled a significantly redesigned Apple Store in Union Square, San Francisco, 
featuring large glass doors for the entry, open spaces, and rebranded rooms. In 
addition to purchasing products, consumers can get advice and help from 
"Creative Pros" – individuals with specialized knowledge of creative arts; 
get product support in a tree-lined Genius Grove; and attend sessions, 
conferences and community events,[434][435] with Ahrendts commenting that the 
goal is to make Apple Stores into "town squares", a place where people 
naturally meet up and spend time.[436] The new design will be applied to all 
Apple Stores worldwide,[437] a process that has seen stores temporarily 
relocate[438] or close.[439]

Many Apple Stores are located inside shopping malls, but Apple has built 
several stand-alone "flagship" stores in high-profile locations.[429] It has 
been granted design patents and received architectural awards for its stores' 
designs and construction, specifically for its use of glass staircases and 
cubes.[440] The success of Apple Stores have had significant influence over 
other consumer electronics retailers, who have lost traffic, control and 
profits due to a perceived higher quality of service and products at Apple 
Stores.[441][442] Apple's notable brand loyalty among consumers causes long 
lines of hundreds of people at new Apple Store openings or product 
releases.[364][365][366][367] Due to the popularity of the brand, Apple 
receives a large number of job applications, many of which come from young 
workers.[433] Although Apple Store employees receive above-average pay, are 
offered money toward education and health care, and receive product 
discounts,[433] there are limited or no paths of career advancement.[433] A May 
2016 report with an anonymous retail employee highlighted a hostile work 
environment with harassment from customers, intense internal criticism, and a 
lack of significant bonuses for securing major business contracts.[443]

Due to the 2019-20 coronavirus pandemic outbreak, Apple announced that they 
will be closing its stores outside China until March 27, 2020. Despite the 
stores being closed, hourly workers will continue to be paid. Workers across 
the company will be allowed to work remotely if their jobs permit it.[444] On 
March 24, 2020, in a memo, Senior Vice President of People and Retail Deirdre 
O’Brien announced that some of its retail stores are expected to reopen at 
the beginning of April.[445]

Corporate affairs
See also: List of mergers and acquisitions by Apple, Braeburn Capital, and 
FileMaker Inc.
Corporate culture

Universities with the most alumni at Apple
Apple is one of several highly successful companies founded in the 1970s that 
bucked the traditional notions of corporate culture. Jobs often walked around 
the office barefoot even after Apple became a Fortune 500 company. By the time 
of the "1984" television advertisement, Apple's informal culture had become a 
key trait that differentiated it from its competitors.[446] According to a 2011 
report in Fortune, this has resulted in a corporate culture more akin to a 
startup rather than a multinational corporation.[447]

As the company has grown and been led by a series of differently opinionated 
chief executives, it has arguably lost some of its original character. 
Nonetheless, it has maintained a reputation for fostering individuality and 
excellence that reliably attracts talented workers, particularly after Jobs 
returned to the company. Numerous Apple employees have stated that projects 
without Jobs's involvement often took longer than projects with it.[448]

To recognize the best of its employees, Apple created the Apple Fellows program 
which awards individuals who make extraordinary technical or leadership 
contributions to personal computing while at the company. The Apple Fellowship 
has so far been awarded to individuals including Bill Atkinson,[449] Steve 
Capps,[450] Rod Holt,[449] Alan Kay,[451][452] Guy Kawasaki,[451][453] Al 
Alcorn,[454] Don Norman,[451] Rich Page,[449] and Steve Wozniak.[449]

At Apple, employees are intended to be specialists who are not exposed to 
functions outside their area of expertise. Jobs saw this as a means of having 
"best-in-class" employees in every role. For instance, Ron Johnson—Senior 
Vice President of Retail Operations until November 1, 2011—was responsible 
for site selection, in-store service, and store layout, yet had no control of 
the inventory in his stores. This was done by Tim Cook, who had a background in 
supply-chain management.[455] Apple is known for strictly enforcing 
accountability. Each project has a "directly responsible individual" or "DRI" 
in Apple jargon.[447] As an example, when iOS senior vice president Scott 
Forstall refused to sign Apple's official apology for numerous errors in the 
redesigned Maps app, he was forced to resign.[456] Unlike other major U.S. 
companies, Apple provides a relatively simple compensation policy for 
executives that does not include perks enjoyed by other CEOs like country club 
fees or private use of company aircraft. The company typically grants stock 
options to executives every other year.[457]

In 2015, Apple had 110,000 full-time employees. This increased to 116,000 
full-time employees the next year, a notable hiring decrease, largely due to 
its first revenue decline. Apple does not specify how many of its employees 
work in retail, though its 2014 SEC filing put the number at approximately half 
of its employee base.[458] In September 2017, Apple announced that it had over 
123,000 full-time employees.[459]

Apple has a strong culture of corporate secrecy, and has an anti-leak Global 
Security team that recruits from the National Security Agency, the Federal 
Bureau of Investigation, and the United States Secret Service.[460][461][462]

In December 2017, Glassdoor named Facebook the best place to work, according to 
reviews from anonymous employees, with Apple dropping to 48th place, having 
originally entered at rank 19 in 2009, peaking at rank 10 in 2012, and falling 
down the ranks in subsequent years.[463][464]

Lack of innovation
An editorial article in The Verge in September 2016 by technology journalist 
Thomas Ricker explored some of the public's perceived lack of innovation at 
Apple in recent years, specifically stating that Samsung has "matched and even 
surpassed Apple in terms of smartphone industrial design" and citing the belief 
that Apple is incapable of producing another breakthrough moment in technology 
with its products. He goes on to write that the criticism focuses on individual 
pieces of hardware rather than the ecosystem as a whole, stating "Yes, 
iteration is boring. But it's also how Apple does business. [...] It enters a 
new market and then refines and refines and continues refining until it yields 
a success". He acknowledges that people are wishing for the "excitement of 
revolution", but argues that people want "the comfort that comes with harmony". 
Furthermore, he writes that "a device is only the starting point of an 
experience that will ultimately be ruled by the ecosystem in which it was 
spawned", referring to how decent hardware products can still fail without a 
proper ecosystem (specifically mentioning that Walkman didn't have an ecosystem 
to keep users from leaving once something better came along), but how Apple 
devices in different hardware segments are able to communicate and cooperate 
through the iCloud cloud service with features including Universal Clipboard 
(in which text copied on one device can be pasted on a different device) as 
well as inter-connected device functionality including Auto Unlock (in which an 
Apple Watch can unlock a Mac in close proximity). He argues that Apple's 
ecosystem is its greatest innovation.[465]

The Wall Street Journal reported in June 2017 that Apple's increased reliance 
on Siri, its virtual personal assistant, has raised questions about how much 
Apple can actually accomplish in terms of functionality. Whereas Google and 
Amazon make use of big data and analyze customer information to personalize 
results, Apple has a strong pro-privacy stance, intentionally not retaining 
user data. "Siri is a textbook of leading on something in tech and then losing 
an edge despite having all the money and the talent and sitting in Silicon 
Valley", Holger Mueller, a technology analyst, told the Journal. The report 
further claims that development on Siri has suffered due to team members and 
executives leaving the company for competitors, a lack of ambitious goals, and 
shifting strategies. Though switching Siri's functions to machine learning and 
algorithms, which dramatically cut its error rate, the company reportedly still 
failed to anticipate the popularity of Amazon's Echo, which features the Alexa 
personal assistant. Improvements to Siri stalled, executives clashed, and there 
were disagreements over the restrictions imposed on third-party app 
interactions. While Apple acquired an England-based startup specializing in 
conversational assistants, Google's Assistant had already become capable of 
helping users select Wi-Fi networks by voice, and Siri was lagging in 
functionality.[466][467]

In December 2017, two articles from The Verge and ZDNet debated what had been a 
particularly devastating week for Apple's macOS and iOS software platforms. The 
former had experienced a severe security vulnerability, in which Macs running 
the then-latest macOS High Sierra software were vulnerable to a bug that let 
anyone gain administrator privileges by entering "root" as the username in 
system prompts, leaving the password field empty and twice clicking "unlock", 
gaining full access.[468] The bug was publicly disclosed on Twitter, rather 
than through proper bug bounty programs.[469] Apple released a security fix 
within a day and issued an apology, stating that "regrettably we stumbled" in 
regards to the security of the latest updates.[470] After installing the 
security patch, however, file sharing was broken for users, with Apple 
releasing a support document with instructions to separately fix that 
issue.[471] Though Apple publicly stated the promise of "auditing our 
development processes to help prevent this from happening again", users who 
installed the security update while running the older 10.13.0 version of the 
High Sierra operating system rather than the then-newest 10.13.1 release 
experienced that the "root" security vulnerability was re-introduced, and 
persisted even after fully updating their systems.[472] On iOS, a date bug 
caused iOS devices that received local app notifications at 12:15am on December 
2, 2017 to repeatedly restart.[473] Users were recommended to turn off 
notifications for their apps.[474] Apple quickly released an update, done 
during the nighttime in Cupertino, California time[475][476] and outside of 
their usual software release window,[477] with one of the headlining features 
of the update needing to be delayed for a few days.[478][479] The combined 
problems of the week on both macOS and iOS caused The Verge's Tom Warren to 
call it a "nightmare" for Apple's software engineers and described it as a 
significant lapse in Apple's ability to protect its more than 1 billion 
devices.[477] ZDNet's Adrian Kingsley-Hughes wrote that "it's hard to not come 
away from the last week with the feeling that Apple is slipping".[480] 
Kingsley-Hughes also concluded his piece by referencing an earlier article, in 
which he wrote that "As much as I don't want to bring up the tired old 'Apple 
wouldn't have done this under Steve Jobs's watch' trope, a lot of what's 
happening at Apple lately is different from what they came to expect under 
Jobs. Not to say that things didn't go wrong under his watch, but product 
announcements and launches felt a lot tighter for sure, as did the overall 
quality of what Apple was releasing." He did, however, also acknowledge that 
such failures "may indeed have happened" with Jobs in charge, though returning 
to the previous praise for his demands of quality, stating "it's almost 
guaranteed that given his personality that heads would have rolled, which 
limits future failures".[480]

Manufacturing
The company's manufacturing, procurement, and logistics enable it to execute 
massive product launches without having to maintain large, profit-sapping 
inventories. In 2011, Apple's profit margins were 40 percent, compared with 
between 10 and 20 percent for most other hardware companies. Cook's catchphrase 
to describe his focus on the company's operational arm is: "Nobody wants to buy 
sour milk".[171][481]

During the Mac's early history Apple generally refused to adopt prevailing 
industry standards for hardware, instead creating their own.[482] This trend 
was largely reversed in the late 1990s, beginning with Apple's adoption of the 
PCI bus in the 7500/8500/9500 Power Macs. Apple has since joined the industry 
standards groups to influence the future direction of technology standards such 
as USB, AGP, HyperTransport, Wi-Fi, NVMe, PCIe and others in its products. 
FireWire is an Apple-originated standard that was widely adopted across the 
industry after it was standardized as IEEE 1394 and is a legally mandated port 
in all Cable TV boxes in the United States.[483]

Apple has gradually expanded its efforts in getting its products into the 
Indian market. In July 2012, during a conference call with investors, CEO Tim 
Cook said that he "[loves] India", but that Apple saw larger opportunities 
outside the region.[484] India's requirement that 30% of products sold be 
manufactured in the country was described as "really adds cost to getting 
product to market".[485] In October 2013, Indian Apple executives unveiled a 
plan for selling devices through instalment plans and store-within-a-store 
concepts, in an effort to expand further into the market. The news followed 
Cook's acknowledgment of the country in July when sales results showed that 
iPhone sales in India grew 400% during the second quarter of 2013.[486][487] In 
March 2016, The Times of India reported that Apple had sought permission from 
the Indian government to sell refurbished iPhones in the country.[488][489] 
However, two months later, the application was rejected, citing official 
country policy.[490][491] In May 2016, Apple opened an iOS app development 
center in Bangalore and a maps development office for 4,000 staff in 
Hyderabad.[492][493][494][495] In February 2017, Apple once again requested 
permission to sell used iPhones in the country.[496][497] The same month, 
Bloomberg reported that Apple was close to receiving permission to open its 
first retail store in the country.[498][499] In March, The Wall Street Journal 
reported that Apple would begin manufacturing iPhone models in India "over the 
next two months",[500][501] and in May, the Journal wrote that an Apple 
manufacturer had begun production of iPhone SE in the country,[502][503] while 
Apple told CNBC that the manufacturing was for a "small number" of units.[504] 
Reuters reported in December 2017, that Apple and the Indian government were 
clashing over planned increases to import taxes for components used in mobile 
phone production, with Apple having engaged in talks with government officials 
to try to delay the plans, but the Indian government sticking to its policies 
of no exemptions to its "Make in India" initiative.[505][506] The import tax 
increases went into effect a few days later, with Apple being hurt the most out 
of all phone manufacturers, having nine of out ten phones imported into the 
country, whereas main smartphone competitor Samsung produces almost all of its 
devices locally.[507] In April 2019, Apple initiated manufacturing of iPhone 7 
at its Bengaluru facility, keeping in mind demand from local customers even as 
they seek more incentives from the government of India.[508] At the beginning 
of 2020, Tim Cook announced that Apple schedules the opening of its first 
physical outlet in India for 2021, while an online store is to be launched by 
the end of the year.[509]

In May 2017, the company announced a $1 billion funding project for "advanced 
manufacturing" in the United States,[510][511] and subsequently invested $200 
million in Corning Inc., a manufacturer of toughened Gorilla Glass technology 
used in its iPhone devices.[512][513] The following December, Apple's chief 
operating officer, Jeff Williams, told CNBC that the "$1 billion" amount was 
"absolutely not" the final limit on its spending, elaborating that "We're not 
thinking in terms of a fund limit. ... We're thinking about, where are the 
opportunities across the U.S. to help nurture companies that are making the 
advanced technology — and the advanced manufacturing that goes with that — 
that quite frankly is essential to our innovation".[514][515]

Labor practices
Further information: Criticism of Apple Inc. § Labor practices
The company advertised its products as being made in America until the late 
1990s; however, as a result of outsourcing initiatives in the 2000s, almost all 
of its manufacturing is now handled abroad. According to a report by The New 
York Times, Apple insiders "believe the vast scale of overseas factories, as 
well as the flexibility, diligence and industrial skills of foreign workers, 
have so outpaced their American counterparts that "Made in the U.S.A." is no 
longer a viable option for most Apple products".[516]

In 2006, the Mail on Sunday reported on the working conditions of the Chinese 
factories where contract manufacturers Foxconn and Inventec produced the 
iPod.[517] The article stated that one complex of factories that assembled the 
iPod and other items had over 200,000 workers living and working within it. 
Employees regularly worked more than 60 hours per week and made around $100 per 
month. A little over half of the workers' earnings was required to pay for rent 
and food from the company.[518][519][520][521]

Apple immediately launched an investigation after the 2006 media report, and 
worked with their manufacturers to ensure acceptable working conditions.[522] 
In 2007, Apple started yearly audits of all its suppliers regarding worker's 
rights, slowly raising standards and pruning suppliers that did not comply. 
Yearly progress reports have been published since 2008.[523] In 2011, Apple 
admitted that its suppliers' child labor practices in China had worsened.[524]

The Foxconn suicides occurred between January and November 2010, when 18[525] 
Foxconn (Chinese: 富士康) employees attempted suicide, resulting in 14 
deaths—the company was the world's largest contract electronics manufacturer, 
for clients including Apple, at the time.[525][526][527] The suicides drew 
media attention, and employment practices at Foxconn were investigated by 
Apple.[528] Apple issued a public statement about the suicides, and company 
spokesperson Steven Dowling said:

[Apple is] saddened and upset by the recent suicides at Foxconn ... A team from 
Apple is independently evaluating the steps they are taking to address these 
tragic events and we will continue our ongoing inspections of the facilities 
where our products are made.[529]

The statement was released after the results from the company's probe into its 
suppliers' labor practices were published in early 2010. Foxconn was not 
specifically named in the report, but Apple identified a series of serious 
labor violations of labor laws, including Apple's own rules, and some child 
labor existed in a number of factories.[529] Apple committed to the 
implementation of changes following the suicides.[530]

Also in 2010, workers in China planned to sue iPhone contractors over poisoning 
by a cleaner used to clean LCD screens. One worker claimed that he and his 
coworkers had not been informed of possible occupational illnesses.[531] After 
a high suicide rate in a Foxconn facility in China making iPads and iPhones, 
albeit a lower rate than that of China as a whole,[532] workers were forced to 
sign a legally binding document guaranteeing that they would not kill 
themselves.[533] Workers in factories producing Apple products have also been 
exposed to n-hexane, a neurotoxin that is a cheaper alternative than alcohol 
for cleaning the products.[534][535][536]

A 2014 BBC investigation found excessive hours and other problems persisted, 
despite Apple's promise to reform factory practice after the 2010 Foxconn 
suicides. The Pegatron factory was once again the subject of review, as 
reporters gained access to the working conditions inside through recruitment as 
employees. While the BBC maintained that the experiences of its reporters 
showed that labor violations were continuing since 2010, Apple publicly 
disagreed with the BBC and stated: "We are aware of no other company doing as 
much as Apple to ensure fair and safe working conditions".[530]

In December 2014, the Institute for Global Labour and Human Rights published a 
report which documented inhumane conditions for the 15,000 workers at a Zhen 
Ding Technology factory in Shenzhen, China, which serves as a major supplier of 
circuit boards for Apple's iPhone and iPad. According to the report, workers 
are pressured into 65-hour work weeks which leaves them so exhausted that they 
often sleep during lunch breaks. They are also made to reside in "primitive, 
dark and filthy dorms" where they sleep "on plywood, with six to ten workers in 
each crowded room." Omnipresent security personnel also routinely harass and 
beat the workers.[537][538]

In 2019, there were reports stating that some of Foxconn's managers had used 
rejected parts to build iPhones, and that Apple was investigating the 
issue.[539]

Environmental practices and initiatives
Apple Energy
Apple Energy, LLC is a wholly owned subsidiary of Apple Inc. that sells solar 
energy. As of June 6, 2016, Apple's solar farms in California and Nevada have 
been declared to provide 217.9 megawatts of solar generation 
capacity.[540][541] In addition to the company's solar energy production, Apple 
has received regulatory approval to construct a landfill gas energy plant in 
North Carolina. Apple will use the methane emissions to generate 
electricity.[542] Apple's North Carolina data center is already powered 
entirely with energy from renewable sources.[543]

Energy and resources
Following a Greenpeace protest, Apple released a statement on April 17, 2012, 
committing to ending its use of coal and shifting to 100% renewable clean 
energy.[544][545] By 2013, Apple was using 100% renewable energy to power their 
data centers. Overall, 75% of the company's power came from clean renewable 
sources.[546]

In 2010, Climate Counts, a nonprofit organization dedicated to directing 
consumers toward the greenest companies, gave Apple a score of 52 points out of 
a possible 100, which puts Apple in their top category "Striding".[547] This 
was an increase from May 2008, when Climate Counts only gave Apple 11 points 
out of 100, which placed the company last among electronics companies, at which 
time Climate Counts also labeled Apple with a "stuck icon", adding that Apple 
at the time was "a choice to avoid for the climate-conscious consumer".[548]

In May 2015, Greenpeace evaluated the state of the Green Internet and commended 
Apple on their environmental practices saying, "Apple's commitment to renewable 
energy has helped set a new bar for the industry, illustrating in very concrete 
terms that a 100% renewable Internet is within its reach, and providing several 
models of intervention for other companies that want to build a sustainable 
Internet."[549]

As of 2016, Apple states that 100% of its U.S. operations run on renewable 
energy, 100% of Apple's data centers run on renewable energy and 93% of Apple's 
global operations run on renewable energy.[550] However, the facilities are 
connected to the local grid which usually contains a mix of fossil and 
renewable sources, so Apple carbon offsets its electricity use.[551][552] The 
Electronic Product Environmental Assessment Tool (EPEAT) allows consumers to 
see the effect a product has on the environment. Each product receives a Gold, 
Silver, or Bronze rank depending on its efficiency and sustainability. Every 
Apple tablet, notebook, desktop computer, and display that EPEAT ranks achieves 
a Gold rating, the highest possible. Although Apple's data centers recycle 
water 35 times,[553] the increased activity in retail, corporate and data 
centers also increase the amount of water use to 573 million US gal (2.2 
million m3) in 2015.[554]

During an event on March 21, 2016, Apple provided a status update on its 
environmental initiative to be 100% renewable in all of its worldwide 
operations. Lisa P. Jackson, Apple's vice president of Environment, Policy and 
Social Initiatives who reports directly to CEO, Tim Cook, announced that as of 
March 2016, 93% of Apple's worldwide operations are powered with renewable 
energy. Also featured was the company's efforts to use sustainable paper in 
their product packaging; 99% of all paper used by Apple in the product 
packaging comes from post-consumer recycled paper or sustainably managed 
forests, as the company continues its move to all paper packaging for all of 
its products.[555][556] Apple working in partnership with Conservation Fund, 
have preserved 36,000 acres of working forests in Maine and North Carolina. 
Another partnership announced is with the World Wildlife Fund to preserve up to 
1,000,000 acres (4,000 km2) of forests in China. Featured was the company's 
installation of a 40 MW solar power plant in the Sichuan province of China that 
was tailor-made to coexist with the indigenous yaks that eat hay produced on 
the land, by raising the panels to be several feet off of the ground so the 
yaks and their feed would be unharmed grazing beneath the array. This 
installation alone compensates for more than all of the energy used in Apple's 
Stores and Offices in the whole of China, negating the company's energy carbon 
footprint in the country. In Singapore, Apple has worked with the Singaporean 
government to cover the rooftops of 800 buildings in the city-state with solar 
panels allowing Apple's Singapore operations to be run on 100% renewable 
energy. Liam was introduced to the world, an advanced robotic disassembler and 
sorter designed by Apple Engineers in California specifically for recycling 
outdated or broken iPhones. Reuses and recycles parts from traded in 
products.[557]

Apple announced on August 16, 2016, that Lens Technology, one of its major 
suppliers in China, has committed to power all its glass production for Apple 
with 100 percent renewable energy by 2018. The commitment is a large step in 
Apple's efforts to help manufacturers lower their carbon footprint in 
China.[558] Apple also announced that all 14 of its final assembly sites in 
China are now compliant with UL's Zero Waste to Landfill validation. The 
standard, which started in January 2015, certifies that all manufacturing waste 
is reused, recycled, composted, or converted into energy (when necessary). 
Since the program began, nearly, 140,000 metric tons of waste have been 
diverted from landfills.[559][better source needed]

Toxins
Following further campaigns by Greenpeace,[560] in 2008, Apple became the first 
electronics manufacturer to fully eliminate all polyvinyl chloride (PVC) and 
brominated flame retardants (BFRs) in its complete product line.[561][562] In 
June 2007, Apple began replacing the cold cathode fluorescent lamp (CCFL) 
backlit LCD displays in its computers with mercury-free LED-backlit LCD 
displays and arsenic-free glass, starting with the upgraded MacBook 
Pro.[563][564][565][566] Apple offers comprehensive and transparent information 
about the CO2e, emissions, materials, and electrical usage concerning every 
product they currently produce or have sold in the past (and which they have 
enough data needed to produce the report), in their portfolio on their 
homepage. Allowing consumers to make informed purchasing decisions on the 
products they offer for sale.[567] In June 2009, Apple's iPhone 3GS was free of 
PVC, arsenic, and BFRs.[563][568] All Apple products now have mercury-free 
LED-backlit LCD displays, arsenic-free glass, and non-PVC cables. All Apple 
products have EPEAT Gold status and beat the latest Energy Star guidelines in 
each product's respective regulatory category.[563][569]

In November 2011, Apple was featured in Greenpeace's Guide to Greener 
Electronics, which ranks electronics manufacturers on sustainability, climate 
and energy policy, and how "green" their products are. The company ranked 
fourth of fifteen electronics companies (moving up five places from the 
previous year) with a score of 4.6/10.[570][571] Greenpeace praises Apple's 
sustainability, noting that the company exceeded its 70% global recycling goal 
in 2010. It continues to score well on the products rating with all Apple 
products now being free of PVC plastic and BFRs. However, the guide criticizes 
Apple on the Energy criteria for not seeking external verification of its 
greenhouse gas emissions data and for not setting out any targets to reduce 
emissions.[572] In January 2012, Apple requested that its cable maker, Volex, 
begin producing halogen-free USB and power cables.[573]

Green bonds
In February 2016, Apple issued a US$1.5 billion green bond (climate bond), the 
first ever of its kind by a U.S. tech company. The green bond proceeds are 
dedicated to the financing of environmental projects.[574]

Finance
See also: List of mergers and acquisitions by Apple
Apple is the world's largest information technology company by revenue, the 
world's largest technology company by total assets,[575] and the world's 
second-largest mobile phone manufacturer after Samsung.[576][577]

In its fiscal year ending in September 2011, Apple Inc. reported a total of 
$108 billion in annual revenues—a significant increase from its 2010 revenues 
of $65 billion—and nearly $82 billion in cash reserves.[578] On March 19, 
2012, Apple announced plans for a $2.65-per-share dividend beginning in fourth 
quarter of 2012, per approval by their board of directors.[579]

The company's worldwide annual revenue in 2013 totaled $170 billion.[580] In 
May 2013, Apple entered the top ten of the Fortune 500 list of companies for 
the first time, rising 11 places above its 2012 ranking to take the sixth 
position.[581] As of 2016, Apple has around US$234 billion of cash and 
marketable securities, of which 90% is located outside the United States for 
tax purposes.[582]

Apple amassed 65% of all profits made by the eight largest worldwide smartphone 
manufacturers in quarter one of 2014, according to a report by Canaccord 
Genuity. In the first quarter of 2015, the company garnered 92% of all 
earnings.[583]

On April 30, 2017, The Wall Street Journal reported that Apple had cash 
reserves of $250 billion,[584] officially confirmed by Apple as specifically 
$256.8 billion a few days later.[585]

As of August 3, 2018, Apple was the largest publicly traded corporation in the 
world by market capitalization. On August 2, 2018, Apple became the first 
publicly traded U.S. company to reach a $1 trillion market value.[12][13] Apple 
was ranked #4 on the 2018 Fortune 500 rankings of the largest United States 
corporations by total revenue.[586]

Year	Revenue
in mil. USD	Net income
in mil. USD	Total assets
in mil. USD	Employees
2000[587]	7,983	786	6,803	
2001[588]	5,363	−25	6,021	
2002[589]	5,742	65	6,298	
2003[590]	6,207	69	6,815	
2004[591]	8,279	274	8,050	
2005[592]	13,931	1,328	11,516	14,800
2006[593]	19,315	1,989	17,205	17,800
2007[594]	24,578	3,495	25,347	21,600
2008[595]	37,491	6,119	36,171	32,000
2009[596]	42,905	8,235	47,501	34,300
2010[597]	65,225	14,013	75,183	46,600
2011[598]	108,249	25,922	116,371	60,400
2012[599]	156,508	41,733	176,064	72,800
2013[600]	170,910	37,037	207,000	80,300
2014[601]	182,795	39,510	231,839	92,600
2015[602]	233,715	53,394	290,345	110,000
2016[603]	215,639	45,687	321,686	116,000
2017[604]	229,234	48,351	375,319	123,000
2018[605]	265,595	59,531	365,725	132,000
Tax practices
Further information: Criticism of Apple Inc. § Tax practices, EU illegal State 
aid case against Apple in Ireland, and Leprechaun economics
Apple has created subsidiaries in low-tax places such as Ireland, the 
Netherlands, Luxembourg, and the British Virgin Islands to cut the taxes it 
pays around the world. According to The New York Times, in the 1980s Apple was 
among the first tech companies to designate overseas salespeople in high-tax 
countries in a manner that allowed the company to sell on behalf of low-tax 
subsidiaries on other continents, sidestepping income taxes. In the late 1980s, 
Apple was a pioneer of an accounting technique known as the "Double Irish with 
a Dutch sandwich," which reduces taxes by routing profits through Irish 
subsidiaries and the Netherlands and then to the Caribbean.[606]

British Conservative Party Member of Parliament Charlie Elphicke published 
research on October 30, 2012,[607] which showed that some multinational 
companies, including Apple Inc., were making billions of pounds of profit in 
the UK, but were paying an effective tax rate to the UK Treasury of only 3 
percent, well below standard corporation tax. He followed this research by 
calling on the Chancellor of the Exchequer George Osborne to force these 
multinationals, which also included Google and The Coca-Cola Company, to state 
the effective rate of tax they pay on their UK revenues. Elphicke also said 
that government contracts should be withheld from multinationals who do not pay 
their fair share of UK tax.[608]

Apple Inc. claims to be the single largest taxpayer to the Department of the 
Treasury of the United States of America with an effective tax rate of 
approximately of 26% as of the second quarter of the Apple fiscal year 
2016.[609] In an interview with the German newspaper FAZ in October 2017, Tim 
Cook stated, that Apple is the biggest taxpayer worldwide.[610]

In 2015, Reuters reported that Apple had earnings abroad of $54.4 billion which 
were untaxed by the IRS of the United States. Under U.S. tax law governed by 
the IRC, corporations don't pay income tax on overseas profits unless the 
profits are repatriated into the United States and as such Apple argues that to 
benefit its shareholders it will leave it overseas until a repatriation holiday 
or comprehensive tax reform takes place in the United States.[611][612]

On July 12, 2016 the Central Statistics Office of Ireland announced that 2015 
Irish GDP had grown by 26.3%, and 2015 Irish GNP had grown by 18.7%.[613] The 
figures attracted international scorn, and were labelled by Nobel-prize winning 
economist, Paul Krugman, as leprechaun economics. It was not until 2018 that 
Irish economists could definitively prove that the 2015 growth was due to Apple 
restructuring its controversial double Irish subsidiaries (Apple Sales 
International), which Apple converted into a new Irish capital allowances for 
intangible assets tax scheme (expires in January 2020). The affair required the 
Central Bank of Ireland to create a new measure of Irish economic growth, 
Modified GNI* to replace Irish GDP, given the distortion of Apple's tax 
schemes. Irish GDP is 143% of Irish Modified GNI*.

On August 30, 2016, after a two-year investigation, the EU Competition 
Commissioner concluded Apple received "illegal State aid" from Ireland. The EU 
ordered Apple to pay 13 billion euros ($14.5 billion), plus interest, in unpaid 
Irish taxes for 2004–2014.[614] It is the largest tax fine in history.[615] 
The Commission found that Apple had benefitted from a private Irish Revenue 
Commissioners tax ruling regarding its double Irish tax structure, Apple Sales 
International (ASI).[616] Instead of using two companies for its double Irish 
structure, Apple was given a ruling to split ASI into two internal 
"branches".[617] The Chancellor of Austria, Christian Kern, put this decision 
into perspective by stating that "every Viennese cafe, every sausage stand pays 
more tax in Austria than a multinational corporation".[618]

As of April 24, 2018, Apple agreed to start paying €13 billion in back taxes 
to the Irish government, the repayments will be held in an escrow account while 
Apple and the Irish government continue their appeals in EU courts.[619]

Board of directors
See also: Category:Directors of Apple Inc..
As of October 26, 2019 the following individuals sit on the board of Apple 
Inc.[620]

Arthur D. Levinson (chairman)
Tim Cook (executive director and CEO)
James A. Bell (non-executive director)
Al Gore (non-executive director)
Andrea Jung (non-executive director)
Ronald Sugar (non-executive director)
Susan Wagner (non-executive director)
Executive management
See also: Category:Apple Inc. executives.
As of March 29, 2019 the management of Apple Inc. includes:[620]

Tim Cook (CEO)
Luca Maestri (senior vice president and CFO)
Jeff Williams (chief operating officer (COO))
Jonathan Ive, KBE (chief design officer (CDO)) (outgoing)[621][622]
Katherine L. Adams (senior vice president and general counsel)
Eddy Cue (senior vice president – internet software and services)
Craig Federighi (senior vice president – software engineering)
John Giannandrea (senior vice president – machine learning and AI strategy)
Deirdre O'Brien (senior vice president – retail and people)
Dan Riccio (senior vice president – hardware engineering)
Phil Schiller (senior vice president – worldwide marketing)
Johny Srouji (senior vice president – hardware technologies)
Sabih Khan (senior vice president – operations)
Steven Dowling (vice president – communications)
Lisa P. Jackson (vice president – environment, policy, and social 
initiatives)
Isabel Ge Mahe (vice president and managing director – Greater China)
Tor Myhren (vice president – marketing communications)
Adrian Perica (vice president – corporate development)
Litigation
Main article: Apple Inc. litigation
Apple has been a participant in various legal proceedings and claims since it 
began operation.[623] In particular, Apple is known for and promotes itself as 
actively and aggressively enforcing its intellectual property interests. Some 
litigation examples include Apple v. Samsung, Apple v. Microsoft, Motorola 
Mobility v. Apple Inc., and Apple Corps v. Apple Computer. Apple has also had 
to defend itself against charges on numerous occasions of violating 
intellectual property rights. Most have been dismissed in the courts as shell 
companies known as patent trolls, with no evidence of actual use of patents in 
question.[624] On December 21, 2016, Nokia announced that in the U.S. and 
Germany, it has filed a suit against Apple, claiming that the latter's products 
infringe on Nokia's patents.[625][626] Most recently, in November 2017, the 
United States International Trade Commission announced an investigation into 
allegations of patent infringement in regards to Apple's remote desktop 
technology; Aqua Connect, a company that builds remote desktop software, has 
claimed that Apple infringed on two of its patents.[627]

Privacy stance
Apple has a notable pro-privacy stance, actively making privacy-conscious 
features and settings part of its conferences, promotional campaigns, and 
public image.[628][629][630] With its iOS 8 mobile operating system in 2014, 
the company started encryption all contents of iOS devices through users' 
passcodes, making it impossible for the company to provide customer data to law 
enforcement requests seeking such information.[631] With the popularity rise of 
cloud storage solutions, Apple began a technique in 2016 to do deep learning 
scans for facial data in photos on the user's local device and encrypting the 
content before uploading it to Apple's iCloud storage system.[632] It also 
introduced "differential privacy", a way to collect crowdsourced data from many 
users, while keeping individual users anonymous, in a system that Wired 
described as "trying to learn as much as possible about a group while learning 
as little as possible about any individual in it".[633] Users are explicitly 
asked if they want to participate, and can actively opt-in or opt-out.[634]

However, Apple aids law enforcement in criminal investigations by providing 
iCloud backups of users' devices,[635][636] and the company's commitment to 
privacy has been questioned by its efforts to promote biometric authentication 
technology in its newer iPhone models, which don't have the same level of 
constitutional privacy as a passcode in the United States.[637]

Charitable causes
Apple is a partner of (PRODUCT)RED, a fundraising campaign for AIDS charity. In 
November 2014, Apple arranged for all App Store revenue in a two-week period to 
go to the fundraiser,[638] generating more than US$20 million,[639][640] and in 
March 2017, it released an iPhone 7 with a red color finish.[641][642]

Apple contributes financially to fundraisers in times of natural disasters. In 
November 2012, it donated $2.5 million to the American Red Cross to aid relief 
efforts after Hurricane Sandy,[643] and in 2017 it donated $5 million to relief 
efforts for both Hurricane Irma and Hurricane Harvey,[644] as well as for the 
2017 Central Mexico earthquake.[645] The company has also used its iTunes 
platform to encourage donations, including, but not limited to, help the 
American Red Cross in the aftermath of the 2010 Haiti earthquake,[646] followed 
by similar procedure in the aftermath of the 2011 Japan earthquake,[647] 
Typhoon Haiyan in the Philippines in November 2013,[648] and European migrant 
crisis in September 2015.[649] Apple emphasizes that it does not incur any 
processing or other fees for iTunes donations, sending 100% of the payments 
directly to relief efforts, though it also acknowledges that the Red Cross does 
not receive any personal information on the users donating and that the 
payments may not be tax deductible.[650]

On April 14, 2016, Apple and the World Wide Fund for Nature (WWF) announced 
that they have engaged in a partnership to, "help protect life on our planet." 
Apple released a special page in the iTunes App Store, Apps for Earth. In the 
arrangement, Apple has committed that through April 24, WWF will receive 100% 
of the proceeds from the applications participating in the App Store via both 
the purchases of any paid apps and the In-App Purchases. Apple and WWF's Apps 
for Earth campaign raised more than $8 million in total proceeds to support 
WWF's conservation work. WWF announced the results at WWDC 2016 in San 
Francisco.[651][652][653]

During the 2019–20 coronavirus pandemic outbreak, Apple's CEO Cook announced 
that the company will be donating "millions" of masks to health workers in the 
United States and Europe.[654]

Criticism and controversies

This section should include a better summary of Criticism of Apple Inc.. See 
Wikipedia:Summary style for information on how to properly incorporate it into 
this article's main text. (November 2017)
Main article: Criticism of Apple Inc.

PRISM: a clandestine surveillance program under which the NSA collects user 
data from companies like Facebook and Apple.[655]
Apple has been criticized for alleged unethical business practices such as 
anti-competitive behavior, rash litigation,[656] dubious tax tactics, 
production methods involving the use of sweatshop labor,[657][658][659] 
customer service issues involving allegedly misleading warranties and 
insufficient data security, and its products' environmental footprint. Critics 
have claimed that Apple products combine stolen and/or purchased designs that 
Apple claims are its original creations.[660][661] It has been criticized for 
its alleged collaboration with the U.S. surveillance program PRISM.[662][663]

Apple's issues regarding music over the years include those with the European 
Union regarding iTunes,[664] trouble over updating the Spotify app on Apple 
devices[665] and collusion with record labels.[666]

Apple has faced scrutiny for its tax practices, including using a Double Irish 
Arrangement to reduce the amount of taxes it pays.[667] A 2013 US Senate report 
claimed that Apple hadn't paid corporate taxes for five years due to its deals 
with the Irish government.[668] In 2016, the European Union ordered Apple to 
pay a fine for its actions.[669]

In 2018–19, Apple faced criticism for its failure to approve NVIDIA web 
drivers for GPU installed on legacy Mac Pro machines up to mid 2012 5,1 running 
macOS Mojave 10.14. Without Apple approved NVIDIA web drivers, Apple users are 
faced with replacing their NVIDIA cards with a competing supported brand, such 
as AMD Radeon from the list recommended by Apple.

In June 2019, Apple issued a recall for its 2015 MacBook Pro Retina 15" 
affecting 432,000 units after reports of batteries catching fire. The recall 
was criticized as waiting times for replacements were up to 3 weeks and the 
company didn't provide alternative replacements or repair 
options.[670][671][672]

Ireland's Data Protection Commission in Ireland also launched a privacy 
investigation to examine whether Apple complied with the EU's GDPR law 
following an investigation into how the company processes personal data with 
targeted ads on its platform.[673][674]

In July 2019, following a campaign by the "right to repair" movement, 
challenging Apple's tech repair restrictions on devices, the FTC held a 
workshop to establish the framework of a future nationwide Right to Repair 
rule. The movement argues Apple is preventing consumers from legitimately 
fixing their devices at local repair shops which is having a negative impact on 
consumers.[675][676][677]

The United States Department of Justice also began a review of big tech firms 
to establish whether they could be unlawfully stifling competition in a broad 
antitrust probe in 2019.[678][679]

In December 2019, a report found that the iPhone 11 Pro continues tracking 
location and collecting user data even after users have disabled location 
services. In response, an Apple engineer said the Location Services icon 
"appears for system services that do not have a switch in settings."[680]

In January 2020, US President Donald Trump slammed Apple for refusing to unlock 
two iPhones of a Saudi national, Mohammed Saeed Alshamrani, who shot and killed 
three American sailors and injured eight others in the Naval air base. The 
Pensacola shooting was declared an "act of terrorism" by the FBI, but Apple 
denied to crack the phones citing its data privacy policy.[681]

On 16 March 2020, France fined Apple € 1.1bn for colluding with two 
wholesalers to stifle competition and keep prices high by handicapping 
independent resellers. The arrangement created aligned prices for Apple 
products such as iPads and personal computers for about half the French retail 
market. According to the French regulators, the abuses occurred between 2005 
and 2017, but were first discovered after a complaint by an independent 
reseller, eBizcuss, in 2012.[682]

See also
List of Apple Inc. media events
Pixar
References
 "Consolidated Statements for Q4 FY19" (PDF). Apple Inc. November 1, 2019. 
Retrieved November 1, 2019.
 "Apple 10-K Report FY2019" (PDF). September 28, 2019. Retrieved January 20, 
2020.
 Taylor, Harriet (August 30, 2016). "How Apple managed to pay such a low tax 
rate in Ireland". CNBC. Retrieved January 9, 2017.
 "Apple Looks to Services to Move Beyond iPhone Price Ceiling". Bloomberg.com. 
Bloomberg L.P. January 13, 2020.
 Koblin, John (March 25, 2018). "Apple Goes to Hollywood. Will Its Story Have a 
Happy Ending?". The New York Times.
 Rivas, Teresa. "Ranking The Big Four Tech Stocks: Google Is No. 1, Apple Comes 
In Last". www.barrons.com. Archived from the original on December 28, 2018. 
Retrieved December 27, 2018.
 Ritholtz, Barry (October 31, 2017). "The Big Four of Technology". Bloomberg. 
Archived from the original on June 26, 2019. Retrieved December 27, 2018.
 "What is GAFA (the Big Four)? - Definition from WhatIs.com". WhatIs.com. 
Retrieved March 5, 2020.
 "I Never Left Apple". Officially Woz. January 3, 2018. Retrieved October 2, 
2018.
 Rice, Valerie (April 15, 1985). "Unrecognized Apple II Employees Exit". 
InfoWorld. p. 35. Retrieved November 6, 2017.
 "Huawei beats Apple to become second-largest smartphone maker". The Guardian. 
August 3, 2018. Retrieved August 3, 2018.
 Salinas, Sara (August 2, 2018). "Apple just hit a $1 trillion market cap". 
CNBC. Retrieved August 2, 2018.
 Davies, Rob (August 2, 2018). "Apple becomes world's first trillion dollar 
company". The Guardian. Retrieved August 2, 2018.
 "Apple Inc, Form 10-K, Annual Report, Filing Date Nov 3, 2017". 
secdatabase.com. Retrieved April 23, 2018.
 "Apple Retail Store – Store List". Apple. Retrieved August 23, 2018.
 "Apple Now Has 1.3 Billion Active Devices Worldwide". Retrieved August 23, 
2018.
 Linzmayer 2004, pp. 6–8.
 Gibbs, Samuel (December 5, 2014). "Steve Wozniak: Apple starting in a garage 
is a myth". The Guardian. Archived from the original on April 25, 2015. 
Retrieved November 12, 2019.
 Linzmayer, Owen W. "Apple Confidential: The Real Story of Apple Computer, 
Inc". The Denver Post. Archived from the original on March 20, 2012.
 Williams, Rhiannon (April 1, 2015). "Apple celebrates 39th year on April 1". 
The Telegraph. Telegraph Media Group. Retrieved July 9, 2017.
 "Apple co-founder tells his side of the story". The Sydney Morning Herald. 
Fairfax Media. September 28, 2006. Retrieved July 9, 2017.
 "A Chat with Computing Pioneer Steve Wozniak". NPR. September 29, 2006. 
Retrieved July 9, 2017.
 O'Grady 2009, pp. 2–3.
 "The Homebrew Computer Club". Computer History Museum. Retrieved July 9, 2017.
 Kahney, Leander. Rebuilding an Apple From the Past, Wired, November 19, 2002. 
Archived March 18, 2014, at the Wayback Machine
 Federal Reserve Bank of Minneapolis. "Consumer Price Index (estimate) 
1800–". Retrieved January 1, 2020.
 "Building the digital age". BBC News. November 15, 2007. Retrieved January 19, 
2008.
 "Apple I". Computer History Museum. Archived from the original on March 26, 
2007. Retrieved January 19, 2008.
 Game Makers (TV Show): Apple II. Originally aired January 6, 2005.
 "Picture of original ad featuring US666.66 price".
 Wozniak, Steve; Smith, Gina (2006). iWoz: Computer Geek to Cult Icon: How I 
Invented the Personal Computer, Co-Founded Apple, and Had Fun Doing It. W. W. 
Norton & Company. ISBN 978-0-393-06143-7. OCLC 502898652.
 Blazeski, Goran (November 25, 2017). "Apple-1, Steve Wozniak's hand-built 
creation, was Apple's first official product, priced at $666.66". The Vintage 
News. Retrieved November 24, 2019.
 Linzmayer 2004, p. 10.
 "Frequently Asked Questions". Apple Inc. Retrieved January 19, 2020.
 Luo, Benny (September 12, 2013). "Ronald Wayne: On Co-founding Apple and 
Working With Steve Jobs". Next Shark. Retrieved July 9, 2017.
 Simon, Dan (June 24, 2010). "The gambling man who co-founded Apple and left 
for $800". CNN. Retrieved July 9, 2017.
 "Apple chronology". CNNMoney. January 6, 1998. Retrieved May 2, 2017.
 Gilbert, Ben (December 26, 2016). "Where are the first 10 Apple employees 
today?". Business Insider. Retrieved May 2, 2017.
 Infinite Loop Malone, Michael S. (1999). Infinite loop: how the world's most 
insanely great computer company went insane. New York: Currency/Doubleday. p. 
157. ISBN 978-0-385-48684-2. OCLC 971131326.
 McCracken, Harry (April 1, 2016). "Apple's sales grew 150x between 
1977–1980". Fast Company. Retrieved May 2, 2017.
 Linzmayer 2004, p. 12.
 Linzmayer 2004, pp. 13–15.
 Weyhrich, Steven (April 21, 2002). "Apple II History Chapter 4". Retrieved 
August 18, 2008.
 Bagnall, Brian (2005). On the Edge: The Spectacular Rise and Fall of 
Commodore. Variant Press. pp. 109–112. ISBN 978-0-9738649-0-8.
 Personal Computer Market Share: 1975–2004 Archived June 6, 2012, at the 
Wayback Machine The figures show Mac higher, but that is not a single model.
 O'Grady 2009, p. 6.
 Landley, Rob (September 18, 2000). "Fool.com: How Xerox Forfeited the PC War". 
The Motley Fool. Archived from the original on July 23, 2008. Retrieved August 
12, 2008.
 Brooks, Alex (March 30, 2006). "Apple at 30 – 1976 to 1986". World of Apple. 
Archived from the original on October 20, 2008. Retrieved May 2, 2017.
 Abell, John C. (January 19, 2010). "Jan. 19, 1983: Apple Gets Graphic With 
Lisa". Wired. Retrieved May 2, 2017.
 "Steve Wozniak on Newton, Tesla, and why the original Macintosh was a 'lousy' 
product". June 27, 2013. Archived from the original on March 12, 2016. 
Retrieved June 25, 2018.
 Hormby, Thomas. A history of Apple's Lisa, 1979–1986, Low End Mac, October 
6, 2005. Retrieved March 2, 2007.
 Deffree, Suzanne (December 12, 2018). "Apple IPO makes instant millionaires, 
December 12, 1980". Retrieved May 16, 2019.
 Dilger, Daniel Eran (December 12, 2013). "Apple, Inc. stock IPO created 300 
millionaires 33 years ago today". AppleInsider. Retrieved April 18, 2017.
 Harvey, Brian (1994). "Is Programing Obsolete?". Electrical Engineering and 
Computer Sciences, University of California, Berkeley. Archived from the 
original on October 5, 2013. Retrieved June 14, 2013.
 Friedman, Ted. "Apple's 1984: The Introduction of the Macintosh in the 
Cultural History of Personal Computers". Archived from the original on October 
14, 2012.
 Maney, Kevin (January 28, 2004). "Apple's '1984' Super Bowl commercial still 
stands as watershed event". USA Today. Retrieved April 18, 2017.
 Leopold, Todd (February 3, 2006). "Why 2006 isn't like '1984'". CNN. Retrieved 
April 18, 2017.
 "The greatest commercials of all time". TV Guide. CBS Interactive. October 12, 
1999. Archived from the original on October 12, 1999. Retrieved April 18, 2017.
 Taube, Aaron (January 22, 2014). "How The Greatest Super Bowl Ad Ever – 
Apple's '1984' – Almost Didn't Make It To Air". Business Insider. Retrieved 
April 18, 2017.
 Linzmayer 2004, p. 98.
 Swaine 2014, pp. 441–443.
 Isaacson, Walter (2015). Steve Jobs. Simon and Schuster. ISBN 9781501127625. 
pp. 186–187
 Hertzfeld, Andy (2005). Revolution in The Valley: The Insanely Great Story of 
How the Mac Was Made. O'Reilly Media. ISBN 9780596007195.
 Linzmayer 2004, p. 156.
 Isaacson 2015, pp. 153–154.
 Gallo, Carmine (January 22, 2014). "How Steve Jobs And Bill Gates Inspired 
John Sculley To Pursue The 'Noble Cause". Forbes. Retrieved March 31, 2019.
 Schlender, Brent; Tetzeli, Rick (2016). Becoming Steve Jobs: The Evolution of 
a Reckless Upstart into a Visionary Leader. Crown Business; Reprint edition. 
ISBN 9780385347426. pp.87–92
 Linzmayer 2004, pp. 156–157.
 Spector, G (September 24, 1985). "Apple's Jobs Starts New Firm, Targets 
Education Market". PC Week. p. 109.
 "CNN.com Video". CNN.
 Apple's Other Steve (Stock Research) Archived October 19, 2006, at the Wayback 
Machine March 2, 2000, The Motley Fool.
 Linzmayer 2004, pp. 158–159.
 "When was desktop publishing invented?". About.com. Archived from the original 
on April 20, 2007. Retrieved April 30, 2007.
 Carlton, Jim (1997). Apple: The inside story of intrigue, egomania, and 
business blunders. New York: Random House. ISBN 978-0-8129-2851-8.
 Swaine, Michael (2014). Fire in the Valley: The Birth and Death of the 
Personal Computer. Pragmatic Bookshelf. ISBN 9781680503524. pp. 359–363
 Linzmayer 2004, p. 184–185.
 Linzmayer 2004, p. 160.
 Linzmayer 2004, p. 128.
 Hormby, Thomas (February 22, 2006). "Growing Apple with the Macintosh: The 
Sculley years". Low End Mac. Retrieved March 2, 2007.
 "MacAddict". MacAddict. No. 89. January 2004. Retrieved April 1, 2017.
 Lee, Timothy B. (June 5, 2012). "The Five Most Expensive Apple Computers In 
History". Forbes. Retrieved July 8, 2017.
 "The Apple IIGS, Cont". Apple II History. July 10, 2002. Archived from the 
original on September 12, 2008. Retrieved July 8, 2017.
 Edwards, Benj (January 18, 2013). "30 years of the Apple Lisa and the Apple 
IIe". Macworld. International Data Group. Retrieved July 8, 2017.
 "Exclusive: New pics of Apple's unreleased tablet prototype from 1992 – and 
the Mac that flew on the Space Shuttle". stuff.tv. Retrieved April 14, 2016.
 "Macintosh Performa". Vectronics Apple World. Archived from the original on 
April 19, 2013. Retrieved November 29, 2010.
 "1990–1995: Why the World Went Windows". Roughly Drafted. Archived from the 
original on January 4, 2012. Retrieved August 12, 2008.
 Hormby, Thomas. The Apple vs. Microsoft GUI lawsuit, Low End Mac, August 25, 
2006. Retrieved March 2, 2007.
 "Michael Spindler: The Peter Principle at Apple". Archived from the original 
on September 8, 2008. Retrieved August 12, 2008.
 "1990–1995: Hitting the Wall". Roughly Drafted. Archived from the original 
on September 24, 2008. Retrieved August 14, 2008.
 "Power Macintosh 6100". Retrieved August 12, 2008.
 Chaffin, Bryan. "Former Apple CEO Gil Amelio Lands A New CEO Job | The Mac 
Observer", The Mac Observer, February 6, 2001. Retrieved August 15, 2008.
 "Apple Computer, Inc. Finalizes Acquisition of NeXT Software Inc". Apple Inc. 
February 7, 1997. Archived from the original on July 24, 2001. Retrieved June 
25, 2006.
 Thompson, Ben (February 5, 2018). "Apple's Middle Age". Stratechery. Retrieved 
March 31, 2019.
 Apple Computer, Inc. Finalizes Acquisition of NeXT Software Inc. at the 
Wayback Machine (archive index), Apple Inc., February 7, 1997. Retrieved June 
25, 2006.
 John Arlidge (March 17, 2014). "Jonathan Ive Designs Tomorrow". Time. Time 
Inc. Retrieved March 22, 2014.
 Microsoft and Apple Affirm Commitment to Build Next Generation Software for 
Macintosh Microsoft, August 6, 1997.
 Harreld, Heather (January 5, 1997). "Apple gains tech, agency customers in 
Next deal". Federal Computer Week. Archived from the original on December 6, 
2008. Retrieved August 15, 2008.
 "Apple unveils new marketing strategy". Knight Ridder/Tribune News Service. 
November 10, 1997. Archived from the original on November 13, 2008. Retrieved 
August 15, 2008.
 Grossman, Lev. The Apple Of Your Ear, Time, January 12, 2007. Retrieved 
February 1, 2007.
 Wilson, Greg. Private iCreator is genius behind Apple's polish, New York Daily 
News, January 14, 2007. Retrieved February 1, 2007.
 Apple Canada Inc (January 5, 1999). "800,000 iMacs Sold in First 139 Days". 
Archived from the original on November 8, 2014. Retrieved January 26, 2008.
 "Why Apple Bounced Back". Roughly Drafted. October 25, 2006. Retrieved 
November 8, 2014.
 "A new beginning or swan song for Final Cut Pro X". GR Reporter. GRRreporter 
Ltd. June 7, 2013. Retrieved November 8, 2014.
 Matt Bell, Mark Wherry (September 2002). "APPLE/EMAGIC TAKEOVER The Inside 
Story Of The Deal That Changed The Music World". Sound On Sound. SOS 
Publications Group. Archived from the original on November 8, 2014. Retrieved 
November 8, 2014.
 "Apple to acquire Spruce Technologies". Broadcast. Retrieved April 6, 2018.
 "Spruce Technologies Inc.: Private Company Information – Bloomberg". 
Bloomberg L.P. Retrieved April 6, 2018.
 Seff, Jonathan (May 1, 2001). "The Song Is Over for SoundJam". Macworld. 
International Data Group. Retrieved December 16, 2017.
 Jade, Kasper (January 8, 2001). "Apple Acquires SoundJam, Programmer for 
iMusic". AppleInsider. Retrieved April 2, 2019.
 Steve Jobs (January 9, 2001). Steve Jobs Keynote Macworld 2001 SF (Stevenote). 
San Francisco: YouTube. Event occurs at 1:48:15. Retrieved April 2, 2019. The 
digital lifestyle era, driven by applications like iMovie and our two new ones 
today: iMusic [sic]...
 Sasser, Cabel (2007). "The True Story of Audion". panic.com. Panic Inc.
 Chaffin, Bryan. "Apple Shake: Apple Buys Nothing Real, A High End Compositing 
Software Maker", The Mac Observer, February 7, 2002. Retrieved August 15, 2008.
 Deitrich, Andy (February 2, 2004). "Garage Band". Ars Technica. Retrieved 
March 23, 2017.
 Apple Introduces iPhoto, Apple Inc., January 7, 2002. Retrieved October 30, 
2015.
 "An Exclusive Look at Mac OS 9". Egg Freckles. Egg Freckles. February 24, 
2014. Archived from the original on February 29, 2012. Retrieved March 23, 
2014.
 "Apple Stores 2001–2003". IFO Apple Store. Archived from the original on 
September 27, 2011. Retrieved October 7, 2011.
 Apple enjoys ongoing iPod demand, BBC News, January 18, 2006. Retrieved April 
27, 2007.
 Cantrell, Amanda. Apple's remarkable comeback story, CNN, March 29, 2006. 
Retrieved March 2, 2007.
 Chacksfield, Marc (June 19, 2008). "iTunes hits 5 billion downloads". 
TechRadar. Future plc. Retrieved May 24, 2017.
 Skillings, Jon (June 19, 2008). "Apple's iTunes hits 5 billion mark". CNET. 
Retrieved May 24, 2017.
 Griggs, Brandon; Leopold, Todd (April 26, 2013). "How iTunes changed music, 
and the world". CNN. Retrieved May 24, 2017.
 Arthur, Charles (April 28, 2013). "iTunes is 10 years old today. Was it the 
best idea Apple ever had?". The Guardian. Retrieved May 24, 2017.
 Apple to Use Intel Microprocessors Beginning in 2006, Apple Inc., June 6, 
2005. Retrieved March 2, 2007.
 Johnson, Bobbie (August 10, 2006). "Bye-bye Power Mac... hello Mac Pro". The 
Guardian. Retrieved March 23, 2017.
 "Apple Unveils New MacBook Featuring Intel Core Duo Processors". Apple Inc. 
May 16, 2006.
 "In Major Shift, Apple Builds Its Own Team to Design Chips". The Wall Street 
Journal. April 30, 2009.
 Hesseldahl, Arik (April 5, 2006). "News Flash: Apple Introduces 'Boot Camp' To 
Run Windows XP on Macs". BusinessWeek. Bloomberg L.P. Retrieved August 18, 
2008.
 Gamet, Jeff (January 16, 2006). Apple Passes Dell's Market Cap, The 
MacObserver. Retrieved March 2, 2007.
 Markoff, John (January 16, 2006). "Michael Dell Should Eat His Words, Apple 
Chief Suggests". The New York Times. Retrieved April 3, 2019.
 Singh, Jai (October 6, 1997). "Dell: Apple should close shop". CNET. Retrieved 
March 23, 2017.
 "Apple revamps iBook. Network World (May, 2001)", Network World, May 2, 2001. 
Retrieved August 19, 2008. Archived January 1, 2009, at the Wayback Machine
 Magee, Mike (January 26, 2002). "iMac "All-in-One" is a trinity", The 
Inquirer. Retrieved August 19, 2008.
 "Drop the Computer". The Economist. Economist Group. January 11, 2007. 
Retrieved May 24, 2017.
 "What's In A Name Change? Look At Apple". Forbes. January 25, 2007. Retrieved 
May 24, 2017.
 "Apple Announces The iPhone". MacRumors. January 9, 2007. Retrieved May 24, 
2017.
 Arrington, Michael (January 9, 2007). "Apple Announces iPhone, Stock Soars". 
TechCrunch. AOL. Retrieved May 24, 2017.
 "Apple Announces Apple TV (Formerly 'iTV')". MacRumors. January 9, 2007. 
Retrieved May 24, 2017.
 "Apple TV Coming to Your Living Room". Apple Inc. January 9, 2007. Retrieved 
May 24, 2017.
 Miller, Paul (July 25, 2007). "Apple sold 270,000 iPhones in the first 30 
hours". Engadget. Retrieved May 24, 2017.
 Oyedele, Akin (March 21, 2016). "Here's how Apple shares do right after the 
new iPhone launches". Business Insider. Retrieved May 24, 2017.
 "Apple Inc. Watch Shows 'Innovation Is Back'". Benzinga. September 9, 2014. 
Retrieved October 10, 2014.
 Sandoval, Greg (April 16, 2007). "Apple exhibits Final Cut Studio 2". CNET. 
Retrieved March 23, 2017.
 Block, Ryan (February 6, 2007). "A letter from Steve Jobs on DRM: let's get 
rid of it". Engadget. AOL. Retrieved March 23, 2017.
 Dalrymple, Jim (April 2, 2007). "Apple, EMI offer higher-quality DRM free 
downloads". Macworld. Retrieved November 29, 2010.
 "Changes Coming to the iTunes Store". Apple Inc. January 6, 2009. Retrieved 
March 23, 2014.
 Flandez, Raymund (August 5, 2008). "Programmers Jockey for iPhone Users at 
Apple Site". The Wall Street Journal. Retrieved August 16, 2008.
 McLaughlin, Kevin (August 11, 2008). "Apple's Jobs Gushes Over App Store 
Success". The Channel Wire. Archived from the original on March 1, 2010. 
Retrieved August 16, 2008.
 Chen, Brian (October 21, 2008). "Jobs: Apple Is Third Largest Handset 
Supplier". Wired. Retrieved March 23, 2014.
 "Chunkier Sidekick to Replace Jobs at Macworld". DoesWhat. December 16, 2008. 
Retrieved March 23, 2014.
 "Apple Announces Its Last Year at Macworld" (Press release). Apple Inc. 
December 16, 2008. Retrieved March 23, 2014.
 Jobs, Steve (January 14, 2009). "Apple Media Advisory" (Press release). Apple 
Inc. Retrieved March 23, 2014.
 "Apple Inc, Form 10-Q, Quarterly Report, Filing Date Apr 23, 2009". 
secdatabase.com. Retrieved March 8, 2013.
 "Apple reports the best non-holiday quarter in its history". Betanews. April 
22, 2009. Retrieved November 22, 2010.
 "Apple iPad reaches 1 million sales faster than iPhone". Reuters. Retrieved 
September 10, 2018.
 "Apple passes Microsoft to be biggest tech company". BBC News. May 27, 2010. 
Archived from the original on May 29, 2010. Retrieved May 29, 2010.
 "Apple Presents iPhone 4" (Press release). Apple Inc. Archived from the 
original on September 3, 2011.
 Beaumont, Claudine (June 24, 2010). "Apple iPhone 4: Full review". The 
Telegraph. Telegraph Media Group. ISSN 0307-1235. Retrieved September 10, 2018.
 Topolsky, Joshua (September 7, 2010). "iPod touch review (2010)". Engadget. 
AOL. Retrieved March 23, 2017.
 "Apple Reinvents iPod nano With Multi-Touch Interface" (Press release). Apple 
Inc. September 1, 2010. Archived from the original on November 15, 2010. 
Retrieved November 11, 2010.
 Bell, Donald (September 7, 2010). "Apple iPod Shuffle 2010 (2GB) review". 
CNet. Retrieved April 4, 2019.
 Mintz, Jessica; Robertson, Jordan. "Apple unveils new TV box for renting 
movies, shows". Yahoo! News. Yahoo!. Archived from the original on September 2, 
2010. Retrieved September 2, 2010.
 Ostrow, Adam (October 13, 2010). "Apple Shares Hit $300". Mashable. AOL. 
Retrieved April 18, 2017.
 Albanesius, Chloe (October 20, 2010). "Apple Unveils iLife 11 with New iPhoto, 
iMovie, GarageBand". PC Magazine. Ziff Davis. Retrieved April 18, 2017.
 Madway, Gabriel (October 20, 2010). "Apple shows off iPad-inspired Mac 
laptop". Reuters. Retrieved April 18, 2017.
 Muchmore, Michael (January 6, 2011). "Apple's Mac App Store: Hands On". PC 
Magazine. Ziff Davis. Retrieved January 6, 2011.
 "Apple boss Steve Jobs takes 'medical leave'". BBC News. January 17, 2011. 
Archived from the original on January 19, 2011. Retrieved January 17, 2011.
 Indvik, Lauren (May 9, 2011). "Apple Now World's Most Valuable Brand". 
Mashable. Retrieved October 7, 2011.
 Helft, Miguel (June 6, 2011). "Apple Unveils a 'Cloud' Music and Storage 
Service". The New York Times. Retrieved June 7, 2011.
 Cieply, Michael (March 7, 2011). "A Film About Capitalism, and (Surprise) It's 
a Love Story". The New York Times. Retrieved November 8, 2014.
 Gobry, Pascal-Emmanuel (July 4, 2011). "Apple's Exclusive Supply Chain Of 
Advanced Technology Is Literally Years Ahead Of Anyone Else On The Planet". 
Business Insider. Retrieved October 7, 2011.
 Elmer, Philip (July 5, 2011). "How Apple became a monopsonist – Apple 2.0". 
Fortune. CNN. Archived from the original on February 3, 2014. Retrieved October 
7, 2011.
 "Apple's Supply-Chain Secret? Hoard Lasers". BusinessWeek. Bloomberg L.P. 
Archived from the original on November 4, 2011. Retrieved November 4, 2011. The 
iPhone maker spends lavishly on all stages of the manufacturing process, giving 
it a huge operations advantage
 "Apple holding more cash than USA". BBC News. July 29, 2011. Retrieved October 
7, 2011.
 Primack, Doug. "Fallen Apple: Steve Jobs resigns". Fortune. CNN. Archived from 
the original on September 26, 2011. Retrieved August 24, 2011.
 "Meet Apple's Board of Directors". Ethiopian Review. August 25, 2011. Archived 
from the original on September 28, 2012. Retrieved October 7, 2011.
 Griggs, Brandon (October 6, 2011). "Steve Jobs, Apple founder, dies". CNN. 
Retrieved March 27, 2017.
 Hess, Ken (October 5, 2011). "October 5th, 2011. The day Apple died". ZDNet. 
CBS Interactive. Retrieved March 27, 2017.
 "Apple Reinvents Textbooks with iBooks 2 for iPad – New iBooks Author Lets 
Anyone Create Stunning iBooks Textbooks" (Press release). Apple Inc. January 
19, 2012. Retrieved February 22, 2012.
 "Steve Jobs' Plans to Disrupt the Textbook Industry. How Disruptive Were They? 
| Inside Higher Ed". www.insidehighered.com. Retrieved November 10, 2017.
 Ziegler, Chris (October 4, 2011). "iPhone 4S announced, available October 14th 
starting at $199". The Verge. Retrieved March 23, 2017.
 Parr, Ben (October 4, 2011). "Apple Announces iPhone 4S". Mashable. Retrieved 
March 23, 2017.
 Savov, Vlad (September 12, 2012). "Apple announces 4-inch iPhone 5 with LTE, 
Lightning connector, September 21st release date". The Verge. Retrieved March 
23, 2017.
 Shimpi, Anand Lal (September 12, 2012). "Apple iPhone 5: Announced". 
AnandTech. Purch Group. Retrieved March 23, 2017.
 Mossberg, Walter (March 15, 2012). "New iPad: a Million More Pixels Than 
HDTV". The Wall Street Journal. Retrieved March 15, 2012.
 Lowensohn, Josh (March 7, 2012). "Apple iPad live blog (Wednesday, March 7)". 
CNET. Retrieved March 23, 2017.
 Wood, Molly (October 23, 2012). "The new 'new iPad': Lightning strikes again". 
CNET. Retrieved March 23, 2017.
 Dudley-Nicholson, Jennifer (October 24, 2012). "Apple unveils new iPad Mini, 
updated iPad and new Macs". Herald Sun.
 Stein, Scott (October 5, 2012). "Apple iPhone 5 review". CNET. Retrieved March 
23, 2017.
 "Apple Sells Three Million iPads in Three Days" (Press release). Apple Inc. 
November 5, 2012. Retrieved February 22, 2013.
 Brown, Rich (November 11, 2013). "Apple Mac Mini with Fusion Drive review". 
CNET. Retrieved March 23, 2017.
 Svensson, Peter. "Apple Sets Record for Company Value at $624B". Associated 
Press. Archived from the original on August 22, 2012. Retrieved August 20, 
2012.
 "Apple awarded $1bn in damages from Samsung in US court". BBC News. August 25, 
2012. Retrieved August 25, 2012.
 "Judge strikes $450 million from $1 billion damages award in Apple v. Samsung: 
second trial needed". FOSS Patents. Retrieved March 1, 2013.
 "HTC and Apple Settle Patent Dispute" (Press release). Apple Inc. November 10, 
2012. Retrieved February 22, 2013.
 Reisinger, Don (November 12, 2012). "Apple predicted to generate up to $280 
million a year in HTC deal". CNET. Retrieved March 23, 2017.
 Seward, Zachary M. (April 5, 2014). "The Steve Jobs email that outlined 
Apple's strategy a year before his death". Quartz. Retrieved April 7, 2014.
 "Apple's interactive augmented reality system identifies real-world objects, 
allows screen sharing". AppleInsider.
 Gupta, Poornima (July 2, 2013). "Apple hires former Yves Saint Laurent CEO for 
'special projects'". Reuters. Retrieved August 24, 2013.
 Roberts, Andrew (October 15, 2013). "Burberry Designer Bailey to Become CEO as 
Ahrendts Goes to Apple". Bloomberg L.P. Retrieved October 15, 2013.
 Kastrenakes, Jacob (February 5, 2019). "Apple retail chief Angela Ahrendts is 
leaving in April". The Verge. Retrieved April 18, 2019.
 Garside, Juliette (August 9, 2013). "Apple, Google and AT&T meet Obama to 
discuss NSA surveillance concerns". The Guardian. Retrieved March 23, 2017.
 Romm, Tony. "Apple's Tim Cook, tech executives meet with Barack Obama to talk 
surveillance". Politico. Retrieved August 12, 2013.
 Kerr, Dara (February 3, 2014). "Tim Cook lands in Turkey, could he be planning 
an iPad deal?". CNET. Retrieved March 23, 2017.
 Etherington, Darrell (January 27, 2014). "Apple's 51M iPhones, 26M iPads And 
4.8M Macs In Q1 2014 Set A Record, But Growth Slows". TechCrunch. AOL. 
Retrieved June 30, 2017.
 Cunningham, Andrew (January 27, 2014). "Apple breaks revenue, iPhone, and iPad 
records in Q1 of 2014". Ars Technica. Retrieved June 30, 2017.
 Steele, Billy (May 28, 2014). "Apple acquires Beats Electronics for $3 
billion". Engadget. AOL. Retrieved March 23, 2017.
 Welch, Chris (May 28, 2014). "Apple confirms it's buying Beats for $3 
billion". The Verge. Retrieved March 23, 2017.
 "2013 – Previous Years – Best Global Brands – Best Brands – 
Interbrand". Interbrand. Omnicom Group. Retrieved September 3, 2016.
 "2014 – Previous Years – Best Global Brands – Best Brands – 
Interbrand". Interbrand. Omnicom Group. Retrieved September 3, 2016.
 "2015 – Previous Years – Best Global Brands – Best Brands – 
Interbrand". Interbrand. Omnicom Group. Retrieved October 8, 2016.
 "Rankings – 2016 – Best Global Brands – Best Brands – Interbrand". 
Interbrand. Omnicom Group. Retrieved October 8, 2016.
 "Rankings – 2017 – Best Global Brands – Best Brands – Interbrand". 
Interbrand. Omnicom Group. Retrieved April 18, 2019.
 "Rankings – 2018 – Best Global Brands – Best Brands – Interbrand". 
Interbrand. Omnicom Group. Retrieved April 18, 2019.
 Statt, Nick (January 26, 2016). "1 billion Apple devices are in active use 
around the world". The Verge. Retrieved May 24, 2017.
 Rossignol, Joe (January 26, 2016). "Apple Now Has Over 1 Billion Active 
Devices Worldwide". MacRumors. Retrieved May 24, 2017.
 "Apple invests $1 billion in Chinese ride-hailing service Didi Chuxing". 
Reuters. May 13, 2016. Retrieved May 13, 2016.
 Isaac, Mike; Goel, Vindu (May 12, 2016). "Apple Puts $1 Billion in Didi, a 
Rival to Uber in China". The New York Times. ISSN 0362-4331. Retrieved May 13, 
2016.
 Carew, Rick; Wakabayashi, Daisuke (May 13, 2016). "Apple Invests $1 Billion in 
Didi, Uber's Rival in China". The Wall Street Journal. ISSN 0099-9660. 
Retrieved May 13, 2016.
 Efrati, Amir; Lee, Alfred (October 11, 2016). "Apple Took Board Seat at Didi 
Chuxing". The Information. Retrieved October 17, 2016.
 Vincent, James (October 12, 2016). "After investing $1 billion, Apple takes a 
board seat at 'China's Uber'". The Verge. Retrieved March 23, 2017.
 Stone, Brad; Chen, Lulu (October 6, 2016). "Uber Slayer: How China's Didi Beat 
the Ride-Hailing Superpower". Bloomberg L.P. Retrieved October 17, 2016.
 McBride, Sarah (June 6, 2016). "Apple leads Tech Industry in Fortune 500". 
Yahoo Tech. Retrieved June 6, 2016.
 Phelan, David. "Clips, The Coolest, Most Fun Thing Apple Has Done In A Long 
While". Forbes. Retrieved April 7, 2017.
 Mayo, Benjamin (May 25, 2017). "Apple transitions to Newsroom portal for press 
releases, updates executive bios page design". 9to5Mac. Retrieved May 25, 2017.
 Gartenberg, Chaim (June 5, 2017). "Apple announces HomePod speaker to take on 
Sonos". The Verge. Vox Media. Retrieved December 14, 2017.
 Lunden, Ingrid; Roof, Katie (December 8, 2017). "Sources: Apple is acquiring 
music recognition app Shazam". TechCrunch. Oath Inc. Retrieved December 14, 
2017.
 Singleton, Micah (December 11, 2017). "Apple confirms it has acquired Shazam". 
The Verge. Vox Media. Retrieved December 14, 2017.
 "EU clears Apple's purchase of song-recognition app Shazam". CNBC. September 
6, 2018. Retrieved September 7, 2018.
 Welch, Chris (September 24, 2018). "Apple completes Shazam acquisition, will 
make app ad-free for everyone". The Verge. Retrieved September 24, 2018.
 Andreeva, Nellie (November 8, 2017). "Apple Gives Reese Witherspoon-Jennifer 
Aniston Morning Show Series 2-Season Order, Confirms 'Amazing Stories' Reboot". 
Deadline Hollywood. Retrieved January 18, 2019.
 Robb, David (June 7, 2018). "Apple Signs WGA Contract As It Ramps Up Scripted 
Shows". Deadline Hollywood. Retrieved January 18, 2019.
 Andreeva, Nellie (June 15, 2018). "Oprah Winfrey Partners With Apple For 
Original Content". Deadline Hollywood. Retrieved January 18, 2019.
 Andreeva, Nellie; Petski, Denise (June 20, 2018). "Apple Teams With Sesame 
Workshop On Children's Programming Slate". Deadline Hollywood. Retrieved 
January 18, 2019.
 Andreeva, Nellie; Petski, Denise (December 14, 2018). "Apple Makes 'Peanuts' 
Deal; DHX Media To Produce New Series, Specials & Shorts With Classic 
Characters For Streamer". Deadline Hollywood. Retrieved January 18, 2019.
 Hipes, Patrick; Andreeva, Nellie (November 15, 2018). "Apple Inks Deal With 
A24 For Multiple Films As Part Of Push Into Movies". Deadline Hollywood. 
Retrieved January 18, 2019.
 Smith, Ryan. "Apple Deprecates OpenGL Across All OSes; Urges Developers to use 
Metal". AnandTech.
 "Apple quietly bought a startup that makes lenses for smart glasses, and it 
hints at the company's next big thing". Business Insider. Retrieved August 31, 
2018.
 "Apple buys start-up that makes lenses for augmented reality glasses". CNBC. 
August 29, 2018. Retrieved August 31, 2018.
 "Apple's Latest Acquisition Could Help the Tech Giant Use Data in This New 
Way". Fortune. Retrieved February 15, 2019.
 Rushe, Dominic (January 29, 2019). "Apple reports first decline in revenues 
and profits in over a decade | Apple | The Guardian". The Guardian.
 Gibbs, Samuel (January 3, 2019). "Apple's woes go far beyond the slowdown in 
the Chinese economy". the Guardian.
 McBride, Stephen. "The End Of Apple". Forbes.
 "Apple acquires talking Barbie voicetech startup PullString". TechCrunch. 
Retrieved October 29, 2019.
 Axon, Samuel (July 25, 2019). "Apple acquires Intel's 5G smartphone modem 
business for $1 billion". ars Technica. Retrieved July 31, 2019.
 Costello, Sam (October 13, 2015). "This is the Number of iPods Sold All-Time". 
Lifewire. Retrieved May 6, 2017.
 Welch, Chris (July 27, 2017). "Apple confirms iPod nano and iPod shuffle have 
been discontinued". The Verge. Retrieved September 3, 2017.
 Heater, Brian (July 27, 2017). "Apple discontinues iPod nano and shuffle and 
doubles iPod touch capacities to 32GB and 128GB". TechCrunch. AOL. Retrieved 
September 3, 2017.
 Rossignol, Joe (July 27, 2017). "Apple Discontinues iPod Nano and iPod 
Shuffle". MacRumors. Retrieved September 3, 2017.
 Apple's Chief in the Risky Land of the Handhelds The New York Times
 "Apple Reinvents the Phone with iPhone" (Press release). Apple Inc. January 9, 
2007. Retrieved October 7, 2011.
 "iPhone Premieres This Friday Night at Apple Retail Stores" (Press release). 
Apple Inc. Retrieved October 7, 2011.
 "Apple Adds New iPhone & iPod touch Models" (Press release). Apple Inc. 
February 5, 2008. Retrieved September 8, 2009.
 "Apple Introduces the New iPhone 3G" (Press release). Apple Inc. June 9, 2008. 
Retrieved October 7, 2011.
 Apple's Game Changer, Downloading Now. The New York Times, December 5, 2009. 
Retrieved April 5, 2013.
 "Apple's Revolutionary App Store Downloads Top One Billion in Just Nine 
Months". Apple Inc. April 24, 2009. Archived from the original on June 5, 2011. 
Retrieved October 7, 2011.
 Griggs, Brandon; Sutter, John D. (June 8, 2010). "Apple unveils iPhone 4, 
'biggest leap we've taken' since first model". CNN. Archived from the original 
on July 8, 2010. Retrieved July 5, 2010.
 Ward, Andrew (July 21, 2011). "Apple overtakes Nokia in smartphone stakes". 
Financial Times. Retrieved July 21, 2011.
 "iPhone 4S Availability". OS X Daily. October 5, 2011. Retrieved October 19, 
2011.
 "Siri acquired by Apple; iPhone becomes the Virtual Personal Assistant?". 
ZDNet. Archived from the original on October 16, 2011. Retrieved October 19, 
2011.
 "About Hearing Aid Compatibility (HAC) requirements for iPhone – Apple 
Support". support.apple.com. Retrieved April 3, 2016.
 "Use Live Listen with Made for iPhone hearing aids – Apple Support". 
support.apple.com. Retrieved April 3, 2016.
 "iPhone 4S First Weekend Sales Top Four Million". Apple Inc. Retrieved October 
14, 2011.
 Statistics and Facts about the iPhone. Statista, April 2013.
 Moscartello, Angela (February 20, 2013). "iPhone 5 is World's Best-Selling 
Smartphone". PC Magazine.
 "iPhone 5 Pre-Orders Top Two Million in First 24 Hours". Apple Inc. Retrieved 
January 1, 2013.
 "iPhone 5 First Weekend Sales Top Five Million". Apple Inc. Retrieved January 
1, 2013.
 "Apple Sells 9 Million New iPhones In Opening Weekend". NPR. Retrieved 
September 23, 2013.
 Poornima Gupta; Jennifer Saba (September 23, 2013). "Apple polishes forecast 
after selling 9 million new iPhones". Reuters. Retrieved September 24, 2013.
 Etherington, Darrell (July 25, 2013). "Apple Working On Location-Aware Battery 
Management For iPhone". TechCrunch. AOL. Retrieved June 25, 2017.
 Cunningham, Andrew (September 9, 2014). "Apple announces iPhone 6, iPhone 6 
Plus". Ars Technica. Retrieved March 23, 2017.
 Cunningham, Andrew (September 9, 2015). "Apple announces iPhone 6S and 6S Plus 
for $199 and $299 on-contract". Ars Technica. Retrieved March 23, 2017.
 Hern, Alex; Kiss, Jemima (March 21, 2016). "Key points of Apple's iPhone SE 
launch at a glance". The Guardian. Retrieved March 23, 2017.
 Welch, Chris (July 27, 2016). "Apple has sold over 1 billion iPhones". The 
Verge. Retrieved May 24, 2017.
 Clover, Juli (July 27, 2016). "Apple Has Sold 1 Billion iPhones". MacRumors. 
Retrieved May 24, 2017.
 Seifert, Dan (September 7, 2016). "iPhone 7 and 7 Plus announced with water 
resistance, dual cameras, and no headphone jack". The Verge. Retrieved March 
23, 2017.
 Patel, Nilay (June 21, 2016). "Taking the headphone jack off phones is 
user-hostile and stupid". The Verge. Retrieved March 23, 2017.
 Gartenberg, Chaim (September 12, 2017). "iPhone 8 and 8 Plus announced with 
wireless charging, True Tone display, A11 Bionic processor". The Verge. 
Retrieved October 11, 2017.
 Savov, Vlad (September 12, 2017). "iPhone X announced with edge-to-edge 
screen, Face ID, and no home button". The Verge. Retrieved October 11, 2017.
 Crook, Jordan (September 12, 2017). "This is the iPhone X". TechCrunch. AOL. 
Retrieved October 11, 2017.
 "iPhone Xs and iPhone Xs Max bring the best and biggest displays to iPhone". 
Apple. September 12, 2018.
 "Apple introduces iPhone XR". Apple. September 12, 2018.
 "Apple introduces dual camera iPhone 11". Apple Newsroom. Retrieved September 
11, 2019.
 "iPhone 11 Pro and iPhone 11 Pro Max: the most powerful and advanced 
smartphones". Apple Newsroom. Retrieved September 11, 2019.
 Rose, Michael (January 27, 2013). "January 27, 2010: Apple announces the 
iPad". Engadget. AOL. Retrieved April 18, 2017.
 Foresman, Chris (January 27, 2010). "Apple announces the iPad". Ars Technica. 
Retrieved April 18, 2017.
 "Apple Launches iPad". Apple Press Info. Apple Inc. January 27, 2010. 
Retrieved April 18, 2017.
 "Apple Tablet Media Event Today: "Come See Our Latest Creation"". MacRumors. 
January 27, 2010. Archived from the original on January 30, 2010. Retrieved 
January 27, 2010.
 Tony Bradley (January 29, 2010). "AT&T Beefing Up Network for iPad and 
iPhone". PC World. Archived from the original on February 1, 2010. Retrieved 
January 29, 2010.
 Helft, Miguel (March 2, 2011). "Jobs Returns to Introduce a New iPad". The New 
York Times. Retrieved March 23, 2011.
 Martin, Mel (March 18, 2011). "iPad 2 supply line affected by Japan disaster". 
TUAW – The Unofficial Apple Weblog. Retrieved October 7, 2011.
 The new iPad – View all the technical specifications. Apple Inc. Retrieved 
February 7, 2013.
 iPad – Features. Apple Inc. Retrieved February 7, 2013.
 iPad Mini – Features. Apple Inc. Retrieved February 7, 2013.
 "iPad". Apple. Retrieved January 1, 2014.
 Ingraham, Nathan (June 8, 2015). "Apple's App Store has passed 100 billion app 
downloads". The Verge. Retrieved March 23, 2017.
 Geuss, Megan (September 9, 2015). "Apple's new iPad Pro is an expansive 12.9 
inches, available in November". Ars Technica. Retrieved March 23, 2017.
 Savov, Vlad (September 9, 2015). "iPad mini 4 announced at $399, iPad mini 2 
now starts at $269". The Verge. Retrieved June 27, 2017.
 Machkovech, Sam (March 21, 2016). "Behold, the new iPad Pro—now 9.7 inches 
with "True Tone" display". Ars Technica. Retrieved March 23, 2017.
 Painter, Lewis. "All the announcements from WWDC 2017". Macworld UK. Retrieved 
June 24, 2017.
 "Apple Watch is 'world's best selling wearable' with 4.2 million shifted in 
Q2". July 21, 2015.
 Garun, Natt (September 9, 2014). "Everything Apple announced at its September 
2014 keynote". The Next Web. Retrieved March 23, 2017.
 Savov, Vlad (September 9, 2014). "Apple Watch announced: available for $349 
early next year". The Verge. Retrieved March 23, 2017.
 Machkovech, Sam (March 9, 2015). "Apple Watch starts at $349, launching April 
24". Ars Technica. Retrieved March 23, 2017.
 Gibbs, Samuel; Hern, Alex (March 9, 2015). "Apple Watch: available 24 April 
for between $349 and $17,000". The Guardian. Retrieved March 23, 2017.
 Kastrenakes, Jacob (March 9, 2015). "Apple Watch release date is April 24th, 
with pricing from $349 to over $10,000". The Verge. Retrieved March 23, 2017.
 Dillet, Romain (September 7, 2016). "Apple unveils the Apple Watch Series 2". 
TechCrunch. AOL. Retrieved October 11, 2017.
 Etherington, Darrell (September 12, 2017). "The Apple Watch Series 3 comes 
with LTE connectivity". TechCrunch. AOL. Retrieved October 11, 2017.
 Morse, Jack (September 13, 2017). "An iPhone is required to get the new Apple 
Watch with LTE connectivity working". Mashable. Retrieved October 11, 2017.
 "Apple Watch Series 4: Beautifully redesigned with breakthrough communication, 
fitness and health capabilities". Apple. September 12, 2018.
 "Apple Announces iTunes 7 with Amazing New Features" (Press release). Apple 
Inc. September 12, 2006. Archived from the original on May 15, 2011. Retrieved 
October 7, 2011.
 "YouTube Coming to Apple TV" (Press release). Apple Inc. May 30, 2007. 
Retrieved October 7, 2011.
 "Apple Introduces New Apple TV Software & Lowers Price to $229" (Press 
release). Apple Inc. January 15, 2008. Retrieved October 7, 2011.
 Miller, Ross (September 29, 2010). "Apple TV teardown reveals 8GB flash 
storage, 256MB RAM, leftover iPad parts". Engadget. AOL. Retrieved March 23, 
2017.
 "Apple unveils Apple TV+, the new home for the world's most creative 
storytellers". Apple Newsroom (Press release). Apple Inc. March 25, 2019. 
Retrieved March 28, 2019.
 "This iFixit teardown shows the HomePod is built like a tank". The Verge. 
Retrieved February 14, 2018.
 Ong, Thuy (July 31, 2017). "HomePod firmware reveals more secrets of Apple's 
smart speaker". The Verge. Retrieved September 3, 2017.
 "HomePod Review: Only Apple Devotees Need Apply". Wired.com. Retrieved 
February 14, 2018.
 "HomePod adds new features and Siri languages". Apple. September 12, 2018.
 Haselton, Todd (December 18, 2019). "Apple, Google and Amazon are cooperating 
to make your home gadgets talk to each other". CNBC. Retrieved December 19, 
2019.
 Statt, Nick (September 7, 2016). "Apple to release macOS Sierra on September 
20th". The Verge. Retrieved June 9, 2017.
 Warren, Tom (September 7, 2016). "iOS 10 will be available on September 13th". 
The Verge. Retrieved June 9, 2017.
 Tepper, Fitz (June 13, 2016). "Apple overhauls watchOS with new UI and faster 
app launching". TechCrunch. AOL. Retrieved June 9, 2017.
 Dillet, Romain (September 13, 2016). "Apple just released tvOS 10 and here's 
what's new". TechCrunch. AOL. Retrieved June 9, 2017.
 "iWork". Apple Inc. Retrieved June 9, 2017.
 Clover, Juli (April 13, 2017). "iMovie and Final Cut Pro for Mac Get Bug Fixes 
in New Updates". MacRumors. Retrieved June 9, 2017.
 Griffin, Andrew (January 19, 2017). "Apple releases huge updates for music 
apps GarageBand and Logic Pro X". The Independent. Retrieved June 9, 2017.
 Broussard, Mitchel (September 20, 2016). "macOS Server Updated for Sierra With 
New Setup Assistant Options and More". MacRumors. Retrieved June 9, 2017.
 Cunningham, Andrew (January 28, 2015). "Apple Remote Desktop admin tool is 
updated for the first time in forever". Ars Technica. Retrieved June 9, 2017.
 "iCloud". MacRumors. Retrieved June 9, 2017.
 "Apple Music". MacRumors. Retrieved June 9, 2017.
 Apple wants to start making cars as soon as 2020, Tim Higgins, Sydney Morning 
Herald
 "Logo Evolution: How Top Brands Redesigned Logos and Boosted Conversion". 
Vardot. Retrieved April 9, 2017.
 "Steve Jobs bio says Apple CEO abhorred 'corrupt' execs". CBC News. October 
20, 2011. Retrieved October 21, 2011.
 "Wired News: Apple Doin' the Logo-Motion". September 26, 2003.
 "¥ves ฿ennaïm 🌿 (@ZLOK) on Twitter". twitter.com.
 Raszl, Ivan. "Interview with Rob Janoff, designer of the Apple logo".
 "Logos that became legends: Icons from the world of advertising". The 
Independent. UK. January 4, 2008. Archived from the original on October 3, 
2009. Retrieved September 14, 2009.
 "Archived Interview with Rob Janoff". March 14, 2005. Archived from the 
original on March 14, 2005.
 Leavitt, David (2007). The Man Who Knew Too Much; Alan Turing and the 
invention of the computer. Phoenix. p. 280. ISBN 978-0-7538-2200-5.
 "Apple Computer". August 27, 1999. Archived from the original on August 27, 
1999. Retrieved January 1, 2014.
 "The Lost Apple Logos You've Never Seen". thebrainfever.
 Moses, Asher (October 7, 2011). "Who was Steve Jobs the man?". The Age. 
Melbourne. Retrieved October 7, 2011.
 "Tearful memories for Apple co-founder". The Age. Melbourne. Archived from the 
original on October 8, 2011. Retrieved October 7, 2011.
 Flynn, Laurie J. (February 6, 2007). "After Long Dispute, Two Apples Work It 
Out". The New York Times. Retrieved October 21, 2016.
 "Apple Company". Operating System Documentation Project. December 10, 2007. 
Archived from the original on September 21, 2008. Retrieved August 18, 2008.
 "Apple Think Different Campaign". The Inspiration Room Daily. October 6, 2005. 
Retrieved August 12, 2008.
 "MacWorld New York: I think, therefore iMac". Retrieved August 13, 2008.
 "Say hello to iPhone". BillDay.com. June 29, 2007. Archived from the original 
on September 7, 2008. Retrieved August 13, 2008.
 "IMac: What's in a Design, Anyway?". Wired. January 11, 2002. Archived from 
the original on March 4, 2014. Retrieved February 15, 2010.
 Williams, Ian (June 13, 2007). "UK watchdog clears Apple ads". Computing. 
Incisive Media Ltd. Archived from the original on January 10, 2008. Retrieved 
April 18, 2017.
 "Apple Power Mac ads 'misleading'". BBC News. June 11, 2004. Retrieved April 
18, 2017.
 Farber, Jim. Apple ad creates recognition for Yael Naim, New York Daily News, 
March 11, 2008.
 Liptak, Andrew (September 30, 2017). "Apple's new iPhone 8 Plus ad showcases 
its Portrait Lighting feature". The Verge. Retrieved October 11, 2017.
 Hall, Zac (February 17, 2017). "Apple launches whimsical iPad Pro ad campaign 
based on PC user tweets". 9to5Mac. Retrieved October 11, 2017.
 Chandler, Daniel (2018). Semiotics: The Basics (3rd ed.). New York, NY: 
Routledge.
 Parmentier, Richard (1994). Signs in society: Studies in semiotic 
anthropology. Indiana University Press. ISBN 0253327571.
 Oswald, L. (2015). "The structural semiotics paradigm for marketing research: 
theory, methodology, and case analysis". Semiotica. 205. 
doi:10.1515/sem-2015-0005.
 Rohan, R. "Semiotics Analysis of Apple Inc. Logo". LinkedIn SlideShare. 
Linkedin. Retrieved March 27, 2020.
 Linzmayer, O., Owen (2004). Apple confidential 2.0: The definitive history of 
the world’s most colorful company (2nd ed.). No Starch Press. ISBN 
1593270100.
 Cowin, E. "The evolution of U.S. corporate logos a semiotic analysis". 
Retrieved March 27, 2020.
 Biricik, Asli. "The role of logo design in creating brand emotion: A semiotic 
comparison of the Apple and IBM logos". Retrieved March 27, 2020.
 Riley, Alex (May 16, 2011). "Superbrands' success fuelled by sex, religion and 
gossip". BBC News. Retrieved December 28, 2011.
 Lemmons, Phil (December 1984). "Apple and Its Personal Computers". BYTE. p. 
A4.
 McConnell, Ben; Huba, Jackie. "The father of evangelism marketing". Creating 
Customer Evangelists. Archived from the original on July 25, 2003. Retrieved 
April 18, 2017.
 Webb, Alex; Gurman, Mark; Satariano, Adam (September 16, 2016). "The Apple 
Store Line Is Dying". Bloomberg L.P. Retrieved May 25, 2017.
 Kalb, Ira (September 9, 2014). "The Truth Behind The Giant Apple Store Lines". 
Business Insider. Retrieved May 25, 2017.
 Rossignol, Joe (September 24, 2015). "iPhone 6s Lines Forming at Apple Stores 
Ahead of Launch Day". MacRumors. Retrieved May 25, 2017.
 Rossignol, Joe (September 19, 2015). "Apple's Beautiful New Store in Brussels 
Opens to Long Lines and Fanfare". MacRumors. Retrieved May 25, 2017.
 Evans, Jonny (May 22, 2006). "Apple NY opening makes global headlines". 
Macworld. International Data Group. Retrieved May 25, 2017.
 Ng, Yi Shu (June 14, 2017). "Till death do us dongle: Newlyweds take their 
Apple obsession to the next level". Mashable. Retrieved June 16, 2017.
 Lang, Cady (June 14, 2017). "This Tech-Obsessed Couple Took Their Wedding 
Photos in an Apple Store". Time. Retrieved June 16, 2017.
 "Confessions of an Apple fanboy: I'm going to miss the queues". The Guardian. 
April 8, 2015. Retrieved March 23, 2017.
 Gibbs, Samuel (April 7, 2015). "Is the Apple queue dead? A leaked memo 
suggests it could be". The Guardian. Retrieved March 23, 2017.
 Fisher, Anne (March 17, 2008). "America's Most Admired Companies". Fortune. 
Vol. 157 no. 5. CNN. pp. 65–67.
 Colvin, Geoff (March 16, 2009). "The World's Most Admired Companies 2009". 
Fortune. Vol. 159 no. 5. CNN. p. 76.
 "World's Most Admired Companies". Fortune. CNN. March 2010. Archived from the 
original on March 7, 2010. Retrieved March 7, 2010.
 "World's Most Admired Companies". Fortune. CNN. November 2011. Retrieved 
November 10, 2011.
 "The World's Most Admired Companies". Fortune. Vol. 165 no. 4. March 19, 2012. 
pp. 139–140.
 Elliot, Stuart (September 29, 2013). "Apple Passes Coca-Cola as Most Valuable 
Brand". The New York Times. Retrieved October 21, 2013.
 Is Apple The World's Most Innovative Company (Still)?, Forbes, September 27, 
2013.
 Sandberg-Diment, Erik (March 19, 1985). "Apple Might Learn a Thing or Two from 
I.B.M." The New York Times. p. C4. ISSN 0362-4331. Retrieved July 3, 2017.
 "Wired News: Apple: It's All About the Brand". Wired. December 4, 2002. 
Archived from the original on November 13, 2014.
 Fried, Ian (July 12, 2002). "Are Mac users smarter?". CNET. Archived from the 
original on July 6, 2009. Retrieved March 23, 2017.
 "Computer Ownership Statistics". The NPD Group. October 5, 2009. Retrieved 
November 22, 2010.
 Musil, Steven. "Apple dedicates its homepage to International Women's Day". 
CNET. Retrieved March 9, 2020.
 Lovejoy, Ben (January 20, 2020). "Apple once more dedicates homepage to 
celebrating Martin Luther King Jr Day". 9to5Mac. Retrieved January 22, 2020.
 Lovejoy, Ben (January 21, 2019). "Apple again devotes homepage to celebrating 
Martin Luther King Jr. Day". 9to5Mac. Retrieved January 21, 2019.
 "Apple celebrates Martin Luther King Jr. Day with homepage photo & timely 
quote". 9to5Mac. January 15, 2018. Retrieved January 15, 2018.
 "Apple and Tim Cook Honor Dr. Martin Luther King, Jr". MacRumors. Retrieved 
January 16, 2017.
 "Apple homepage pays tribute to Muhammad Ali, The Greatest of All Time". 
MacDailyNews. June 5, 2016. Retrieved June 5, 2016.
 "Apple Website Honors Bill Campbell". Mac Observer. Retrieved April 19, 2016.
 "Apple Honors Dr. Martin Luther King Jr. With Homepage Tribute". MacRumors. 
Retrieved January 18, 2016.
 "Apple commemorates Martin Luther King on its homepage, encouraging employees 
to volunteer through gift matching". 9to5Mac. January 19, 2015. Retrieved 
January 19, 2015.
 Berkowitz, Joe (August 13, 2014). "Apple's Minimalist Salute To Robin Williams 
Says All It Needs To". Fast Company Co.Create. Mansueto Ventures, LLC. 
Retrieved September 15, 2014.
 Steve Kovach (December 7, 2013). "Apple's Home Page Is A Tribute To Nelson 
Mandela". Business Insider. Retrieved September 15, 2014.
 "Apple Posts Steve Jobs Tribute: "His Spirit Will Forever Be The Foundation Of 
Apple"". Cult of Mac. October 5, 2012. Retrieved October 5, 2012.
 Nick Wingfield (October 5, 2012). "With Steve Jobs Tribute, a Home Page 
Reflects Apple's Founder Again". The New York Times. Retrieved September 15, 
2014.
 Jennifer Van Grove (March 18, 2010). "Apple Pays Tribute to Board Member 
Jerome B. York". Mashable.
 Cheng, Jacqui (October 12, 2007). "Apple "bursting with pride" over Al Gore's 
Peace Prize". Ars Technica. Retrieved March 23, 2017.
 Fried, Ina (October 26, 2005). "Apple pays tribute to Rosa Parks". CNET. 
Retrieved March 23, 2017.
 Chaffin, Bryan (August 11, 2003). "Apple Remembers Gregory Hines With Think 
Different Home Page (With Screen Shot)". The Mac Observer. Retrieved September 
15, 2014.
 "When Steve Jobs and Apple put George Harrison on the Apple.com homepage". 
Edible Apple. Edible Apple. September 15, 2011. Retrieved September 15, 2014.
 Simonson, Sharon (October 2, 2005). "Apple gobbles up Cupertino office space".
 "2014: Apple to occupy seven-building Sunnyvale campus". mercurynews.com. May 
21, 2014.
 "Project Titan, SixtyEight & SG5: Inside Apple's top-secret electric car 
project". AppleInsider.
 "The Bay Area: Apple Inc". traveldk.com. Dorling Kindersley Limited. Archived 
from the original on June 18, 2008. Retrieved May 7, 2008.
 "Apple's New Headquarters Will Be Designed by Norman Foster". Inhabitat. 
Retrieved June 9, 2010.
 Reisinger, Don (October 16, 2013). "Apple's 'spaceship' HQ gets green light 
from Cupertino". CNET. Retrieved March 23, 2017.
 Reisinger, Don (September 1, 2016). "Where Apple Has Quietly Built Its Biggest 
Campus". Fortune. Retrieved July 14, 2017.
 "Apple to build new campus in Austin and add jobs across the US". Apple 
Newsroom. Retrieved December 13, 2018.
 Goel, Vindu (November 20, 2016). "How Apple Empowers, and Employs, the 
American Working Class". The New York Times. Retrieved July 14, 2017.
 "Apple to create 500 jobs in Cork". BBC News. April 20, 2012. Retrieved April 
21, 2012.
 Humphries, Conor (April 20, 2012). "Reuters News Article Discussing Addition 
of 500 new jobs to Apple's European Headquarters". Reuters. Archived from the 
original on June 18, 2012. Retrieved April 21, 2012.
 "Job Description on Apple Website describing Apple's EMEA headquarters". 
Apple.com. Apple Inc. Archived from the original on September 1, 2007. 
Retrieved April 18, 2012.
 Riegel, Ralph; Walsh, Anne-Marie (April 21, 2012). "Irish Independent Article 
Discussing Addition of 500 new jobs to Apple's EMEA Headquarters". Irish 
Independent. Retrieved April 21, 2012.
 "Article from the Irish Examiner Describing Cork as Apple's European 
Headquarters". Irish Examiner. April 18, 2012. Retrieved April 18, 2012.
 Roche, Barry. "Article from the Irish Times Describing Cork as Apple's 
European Headquarters". Irish Times. Retrieved April 18, 2012.
 "Article from the Belfast Telegraph Describing Cork as Apple's European 
Headquarters". Belfast Telegraph. Retrieved April 18, 2012.
 "Irish Examiner Article". Irish Examiner. October 6, 2011. Retrieved April 21, 
2012.
 "Bloomberg Businessweek Profile of Apple Sales International". Bloomberg 
Businessweek. Retrieved April 18, 2012.[verification needed]
 "Apple's Irish website with contact information for Apple Distribution 
International at Cork". Apple.com. Retrieved April 18, 2012.
 "Times of India Article on discussing addition of 500 jobs". Times of Malta. 
Retrieved April 21, 2012.
 O'Brien, Ciara. "Irish Times Article Discussing Addition of 500 new jobs to 
Apple's European Headquarters". Irish Times. Archived from the original on 
April 21, 2012. Retrieved April 21, 2012.
 "Our Occupiers". Stockley Park. Retrieved November 22, 2015.
 "Apple CEO Tim Cook to inaugurate new Israeli headquarters next week". The 
Jerusalem Post. Retrieved February 12, 2015.
 "Tim Cook reportedly headed to Israel for opening of new Apple offices". 
9to5Mac. February 12, 2015. Retrieved February 12, 2015.
 Donato-Weinstein, Nathan (December 14, 2015). "Exclusive: Apple buys former 
chip fab in North San Jose". Silicon Valley Business Journals. Advance 
Publications. Retrieved June 30, 2017.
 Dilger, Daniel Eran (December 14, 2015). "Apple buys former Maxim chip fab in 
North San Jose, neighboring Samsung Semiconductor". AppleInsider. Retrieved 
June 30, 2017.
 "Apple to Open 25 Retail Stores in 2001" (Press release). Apple. May 15, 2001. 
Retrieved May 27, 2017.
 "Apple Stores". MacRumors. Retrieved May 27, 2017.
 Fiegerman, Seth (May 16, 2014). "The Slow Evolution of Apple's Online Store". 
Mashable. Retrieved May 27, 2017.
 Useem, Jerry (March 8, 2007). "Apple: America's best retailer". Fortune. 
Retrieved May 27, 2017.
 "Store List". Apple Retail. Apple Inc. Retrieved December 5, 2017.
 Segal, David (June 23, 2012). "Apple's Retail Army, Long on Loyalty but Short 
on Pay". The New York Times. Retrieved May 27, 2017.
 Webb, Alex (May 19, 2016). "Inside the New Apple Retail Store Design". 
Bloomberg L.P. Retrieved April 26, 2017.
 Statt, Nick (May 19, 2016). "Apple just revealed the future of its retail 
stores". The Verge. Retrieved May 27, 2017.
 Hartmans, Avery (August 19, 2016). "Apple's retail boss wants Apple stores to 
resemble 'town squares'". Business Insider. Retrieved May 27, 2017.
 "Angela Ahrendts talks Apple store makeover, why Tim Cook hired her". CBS This 
Morning. CBS. April 25, 2017. Retrieved May 27, 2017.
 Rossignol, Joe (August 19, 2016). "Apple Opening Three Next-Generation Stores 
Over the Next Week". MacRumors. Retrieved May 27, 2017.
 Rossignol, Joe (February 6, 2017). "Apple Retail Update: Danbury Store Closes 
for Next-Generation Redesign, Dubai to Get Second Store". MacRumors. Retrieved 
May 27, 2017.
 Panzarino, Matthew (April 19, 2012). "Apple out to patent curved glass panels 
used in Shanghai Retail Store". The Next Web. Retrieved May 27, 2017.
 Simpson, Stephen D. (October 8, 2012). "How Apple's fortunes affect other 
stocks". The Globe and Mail. The Woodbridge Company. Retrieved May 27, 2017.
 Crothers, Brooke (March 29, 2012). "Is Best Buy following CompUSA, Circuit 
City to certain doom?". CNET. Retrieved May 27, 2017.
 Edwards, Jim (May 28, 2016). "NEVER MIND THE DEATH THREATS: An Apple Store 
worker tells us what it's really like working for Apple". Business Insider. 
Retrieved May 27, 2017.
 
https://www.wsj.com/articles/apple-closes-all-its-stores-outside-china-over-coronavirus-11584172214
 Mark Gurman (March 24, 2020). "Apple May Start Reopening Stores in First Half 
of April". Bloomberg L.P. Retrieved March 25, 2020.
 Deutschman, Alan. "The once and future Steve Jobs". Salon.com. Archived from 
the original on December 2, 2010. Retrieved November 22, 2010.
 Lashinsky, Adam (August 25, 2011). "How Apple works: inside the world's 
largest startup". Fortune. CNN.
 Brownlee, John (July 7, 2010). "What It's Like To Work At Apple". Cult of Mac.
 Hertzfeld, Andy. Credit Where Due,Folklore.org, January 1983. Retrieved May 
26, 2006.
 "Newton Hall of Fame!". msu.edu.
 Eisenhart, Mary. Fighting Back For Mac, MicroTimes, 1997. Retrieved May 26, 
2006.
 Hertzfeld, Andy. Leave of Absence,Folklore.org, March 1984. Retrieved May 26, 
2006.
 Kawakami, John. Apple Taps Guy Kawasaki For Apple Fellows Program, MacTech, 
September 1995. Retrieved May 26, 2006.
 Montfort, Nick. "Wired 4.10: Spawn of Atari".
 Lashinsky, Adam. "How Apple works: Inside the world's biggest startup – 
Fortune Tech". Tech.fortune.cnn.com. Retrieved December 24, 2011.
 Lashinsky, Adam (October 29, 2012). "Inside Apple's major shakeup". Fortune. 
Retrieved December 10, 2012.
 "Apple CEO gets modest 2012 pay after huge 2011". December 27, 2012.
 Leswing, Kif (October 27, 2016). "Apple added only 6,000 people last year – 
its slowest growth since 2009". Business Insider. Retrieved May 29, 2017.
 "BRIEF-Apple says had 123,000 full-time employees as of Sept. 30". Reuters. 
November 3, 2017. Retrieved November 9, 2017.
 Turton, William (June 20, 2017). "Leaked recording: Inside Apple's global war 
on leakers". The Outline. Retrieved June 20, 2017.
 Deahl, Dani (June 20, 2017). "Internal Apple presentation on how to handle 
leaks gets leaked". The Verge. Retrieved June 20, 2017.
 Mayo, Benjamin (June 20, 2017). "Report details Apple's efforts to increase 
product secrecy, more leaks from Apple campus than supply chain in 2016". 
9to5Mac. Retrieved June 20, 2017.
 Lovejoy, Ben (December 6, 2017). "Facebook named Glassdoor's 'best place to 
work' as Apple falls 48 places to #84". 9to5Mac. Retrieved December 14, 2017.
 Rossignol, Joe (December 6, 2017). "Apple Plummets to Lowest Ranking Ever in 
Glassdoor's Annual List of Best Places to Work". MacRumors. Retrieved December 
14, 2017.
 Ricker, Thomas (September 7, 2016). "First Click: Apple's greatest innovation 
is its ecosystem". The Verge. Retrieved October 8, 2016.
 Mickle, Tripp (June 7, 2017). "'I'm Not Sure I Understand'—How Apple's Siri 
Lost Her Mojo". The Wall Street Journal. Retrieved December 14, 2017.
 Hardwick, Tim (June 8, 2017). "Apple's Concern With User Privacy Reportedly 
Stifling Siri Development". MacRumors. Retrieved December 14, 2017.
 Greenberg, Andy (November 28, 2017). "Anyone can hack macOS High Sierra just 
by typing "root"". Wired. Retrieved December 5, 2017.
 Welch, Chris (November 28, 2017). "Major Apple security flaw grants admin 
access on macOS High Sierra without password". The Verge. Vox Media. Retrieved 
December 5, 2017.
 Rossignol, Joe (November 29, 2017). "Apple Releases macOS High Sierra Security 
Update to Fix Root Password Vulnerability". MacRumors. Retrieved December 5, 
2017.
 Welch, Chris (November 29, 2017). "Apple releases update to fix critical macOS 
High Sierra security issue". The Verge. Vox Media. Retrieved December 5, 2017.
 Greenberg, Andy (December 1, 2017). "macOS update accidentally undoes Apple's 
"root" bug patch". Wired. Retrieved December 5, 2017.
 Clover, Juli (December 1, 2017). "Date Bug in iOS 11.1.2 Causing Crash Loop on 
iPhones as December 2 Hits [Updated]". MacRumors. Retrieved December 5, 2017.
 Ritchie, Rene (December 2, 2017). "iPhone crashing on Dec. 2? Here's the 
fix!". iMore. Mobile Nations. Retrieved December 5, 2017.
 Dillet, Romain (December 2, 2017). "Apple releases iOS 11.2 with 'I.T' 
autocorrect fix, faster wireless charging and Apple Pay Cash". TechCrunch. Oath 
Inc. Retrieved December 5, 2017.
 Lawler, Richard (December 2, 2017). "Apple releases iOS 11.2 with Apple Pay 
Cash and a bug fix". Engadget. Oath Inc. Retrieved December 5, 2017.
 Warren, Tom (December 2, 2017). "Apple's had a shockingly bad week of software 
problems". The Verge. Vox Media. Retrieved December 5, 2017.
 Heater, Brian (December 4, 2017). "Apple Pay Cash starts rolling out to iPhone 
users in the US". TechCrunch. Oath Inc. Retrieved December 5, 2017.
 Gartenberg, Chaim (December 4, 2017). "Apple Pay Cash is rolling out for iOS 
11.2 users". The Verge. Vox Media. Retrieved December 5, 2017.
 Kingsley-Hughes, Adrian (December 4, 2017). "Something is rotten at Apple". 
ZDNet. CBS Interactive. Retrieved December 5, 2017.
 "Ruthlessness and lasers: Apple's supply chain revealed". 
Business.financialpost.com. November 9, 2011. Retrieved December 24, 2011.
 "Mac Ports". Lawlor.cs.uaf.edu. March 17, 2001. Retrieved October 7, 2011.
 "1394 Trade Association: What is 1394?". Archived from the original on April 
4, 2014.
 Lunden, Ingrid (July 24, 2012). "Apple's Feeling Europe's Economic Crisis: 
'Essentially Flat' Sales And A 'Slowdown' In Business, Says Cook". TechCrunch. 
AOL. Retrieved May 24, 2017.
 "Apple CEO Tim Cook: 'I love India, but...'". Gadgets360. NDTV. July 25, 2012. 
Retrieved May 24, 2017.
 Mukherjee, Writankar (October 4, 2013). "Apple to enter smaller Indian towns 
with iPhones, iPads". The Economic Times. The Times Group. Retrieved May 24, 
2017.
 Hong, Kaylene (July 24, 2013). "iPhone sales surge 400% YoY in India, with 
iPad sales on the rise too, says Apple CEO Tim Cook". The Next Web. Retrieved 
May 24, 2017.
 "Apple plans to sell used iPhones in India". The Times of India. March 4, 
2016. Retrieved May 24, 2017.
 Broussard, Mitchel (March 4, 2016). "Apple Submits Application to Sell Used 
iPhones in India". MacRumors. Retrieved May 24, 2017.
 Rai, Saritha (May 3, 2016). "Apple's Plan for Refurbished iPhones Is Rejected 
in India". Bloomberg Technology. Bloomberg L.P. Retrieved May 24, 2017.
 Carman, Ashley (May 3, 2016). "Apple blocked from selling used iPhones in 
India". The Verge. Retrieved May 24, 2017.
 "Apple Opens Development Office in Hyderabad" (Press release). Apple Inc. 
Retrieved October 18, 2017.
 Rai, Saritha (May 17, 2016). "Apple CEO Makes First India Trip With Billion 
Phone Sales at Stake". Bloomberg Technology. Bloomberg L.P. Retrieved May 24, 
2017.
 Byford, Sam (May 18, 2016). "Apple announces app development accelerator in 
Bangalore, India". The Verge. Retrieved May 24, 2017.
 Russell, Jon (May 17, 2016). "Apple is opening an app design and development 
accelerator in India". TechCrunch. AOL. Retrieved May 24, 2017.
 Rai, Saritha (February 6, 2017). "Apple Said to Revive Efforts to Sell Used 
iPhones in India". Bloomberg Technology. Bloomberg L.P. Retrieved May 24, 2017.
 Reisinger, Don (February 6, 2017). "Apple Is Trying Again to Sell Used iPhones 
in India". Fortune. Retrieved May 24, 2017.
 Srivastava, Shruti; Jagtiani, Sunil; Narayan, Adi (February 7, 2016). "Apple 
Said to Be on Course for Approval to Open India Stores". Bloomberg Technology. 
Bloomberg L.P. Retrieved May 24, 2017.
 Broussard, Mitchel (February 8, 2016). "Apple Close to Approval for Opening 
Retail Locations in India". MacRumors. Retrieved May 24, 2017.
 Roy, Rajesh; Purnell, Newley (March 23, 2017). "Apple to Start Making iPhones 
in India Over Next Two Months". The Wall Street Journal. Retrieved May 24, 
2017.
 Gartenberg, Chaim (March 23, 2017). "Apple reportedly to start manufacturing 
iPhones in India". The Verge. Retrieved May 24, 2017.
 Roy, Rajesh; Purnell, Newley; Mickle, Tripp (May 17, 2017). "Apple Assembles 
First iPhones in India". The Wall Street Journal. Retrieved May 24, 2017.
 Mayo, Benjamin (May 17, 2017). "Apple has started production of iPhone SE in 
India, shipping to customers later in May". 9to5Mac. Retrieved May 24, 2017.
 Haselton, Todd (May 17, 2017). "Apple begins manufacturing iPhone SE in 
India". CNBC. NBCUniversal News Group. Retrieved May 24, 2017.
 Kalra, Aditya; Miglani, Sanjeev (December 11, 2017). "Apple, India wrangle 
over import tax on mobile parts: sources". Reuters. Retrieved December 13, 
2017.
 Lovejoy, Ben (December 11, 2017). "Indian government likely to reject Apple's 
request to delay new import taxes". 9to5Mac. Retrieved December 13, 2017.
 Lovejoy, Ben (December 15, 2017). "India hikes tax on mobile phone imports in 
a move which will hurt Apple most". 9to5Mac. Retrieved December 16, 2017.
 "Apple starts iPhone 7 production in Bengaluru". livemint.com. April 2, 2019. 
Retrieved April 2, 2019.
 "Apple to open first Indian store in 2021". BBC News. February 27, 2020. 
Retrieved February 27, 2020.
 Statt, Nick (May 3, 2017). "Tim Cook says Apple is investing $1 billion in US 
manufacturing". The Verge. Vox Media. Retrieved December 14, 2017.
 Ochs, Susie (May 3, 2017). "Apple joins 'Made in America' trend with $1 
billion fund to promote U.S. manufacturing". Macworld. International Data 
Group. Retrieved December 14, 2017.
 Gartenberg, Chaim (May 12, 2017). "Apple's first target for its $1 billion US 
manufacturing fund is glass supplier Corning". The Verge. Vox Media. Retrieved 
December 14, 2017.
 Heater, Brian (May 12, 2017). "Gorilla Glass maker Corning gets $200 million 
from Apple's US manufacturing investment fund". TechCrunch. Oath Inc. Retrieved 
December 14, 2017.
 Salinas, Sara (December 13, 2017). "Apple has a $1 billion fund for US 
manufacturers, but it's ready to spend more, says COO Jeff Williams". CNBC. 
NBCUniversal News Group. Retrieved December 14, 2017.
 Miller, Chance (December 13, 2017). "Jeff Williams says Apple is prepared to 
invest more than $1B in US manufacturers". 9to5Mac. Retrieved December 14, 
2017.
 Duhigg, Charles; Bradsher, Keith (January 21, 2012). "Apple, America and a 
Squeezed Middle Class". The New York Times.
 "The Stark Reality of iPod's Chinese Factories". Daily Mail. August 18, 2006.
 Musgrove, Mike (June 16, 2006). "Sweatshop Conditions at IPod Factory 
Reported". The Washington Post.
 Kahney, Leander (June 13, 2006). "Judging Apple Sweatshop Charge". Wired. 
Condé Nast Publications. Archived from the original on June 16, 2008.
 Dean, Jason (August 11, 2007). "The Forbidden City of Terry Gou". The Wall 
Street Journal.
 Johnson, Joel (November 2, 2010). "Where the Workers Who Made Your iPhone 
Sleep at Night". Wired. Archived from the original on November 4, 2010. 
Retrieved November 13, 2010.
 Morphy, Ericka (January 31, 2008). "Apple, IT and the Specter of Sweatshop 
Labor". Mac News World.
 "Apple 2010 Supplier Responsibility Report" (PDF). Apple Inc. Retrieved March 
26, 2019.
 "Apple's child labour issues worsen". The Telegraph. London: Telegraph Media 
Group. February 15, 2011. Retrieved October 7, 2011.
 Lau, Mimi (December 15, 2010). "Struggle for Foxconn girl who wanted to die". 
South China Morning Post. Wuhan, Hubei.
 Tam, Fiona (October 11, 2010). "Foxconn factories are labour camps: report". 
South China Morning Post.
 "Foxconn worker plunges to death at China plant: report". Reuters. November 5, 
2010.
 Dean, Jason (May 27, 2010). "Suicides Spark Inquiries". The Wall Street 
Journal. Retrieved May 29, 2010.
 Foreman, William (May 26, 2010). "Tech: Apple Supplier Foxconn Suffers 10th 
Death This Year, Asks Workers To Sign Anti-Suicide Pledge". The Huffington 
Post. Retrieved December 20, 2014.
 "Apple under fire again for working conditions at Chinese factories". The 
Guardian. December 19, 2014. Retrieved March 23, 2017.
 Chen, Brian X. (May 14, 2010). "Workers Plan to Sue iPhone Contractor Over 
Poisoning". Wired.
 "Suicides at Foxconn: Light and death". The Economist. May 27, 2010. Retrieved 
April 24, 2012.
 Žižek, Slavoj; Horvat, Srećko (2014). What Does Europe Want?: The Union and 
Its Discontents. Columbia University Press. p. xxi. ISBN 978-0231171076.
 Workers poisoned while making iPhones ABC News, October 25, 2010.
 Dirty Secrets Archived May 25, 2017, at the Wayback Machine ABC News Foreign 
Correspondent, October 26, 2010.
 Occupational Safety and Health Guideline for n-Hexane Archived May 4, 2013, at 
the Wayback Machine, Occupational Safety and Health Administration (OSHA).
 Jamieson, Dave (December 23, 2014). The Factory Workers Behind Your iPhone Are 
Too Tired To Eat, Report Says. The Huffington Post. Retrieved December 24, 
2014.
 Exhaustion Has No Limit at Apple Supplier in China Archived December 24, 2014, 
at the Wayback Machine. Institute for Global Labour and Human Rights. December 
22, 2014.
 Lovejoy, Ben (December 18, 2019). "$43M fraud by Foxconn managers selling 
iPhones made from rejected parts". 9to5Mac.
 Weaver, John Fitzgerald (June 10, 2016). "Apple Energy deeper dive: Is this 
Apple running its own microgrids or more?". Electrek. Retrieved June 12, 2016.
 Weintraub, Seth (June 9, 2016). "Apple has just become an energy company, 
looks to sell excess electricity into the grid and maybe more". 9to5Mac. 
Retrieved June 12, 2016.
 "Catawba County approves lease for Apple's renewable energy center". HDR | 
Hickory Daily Record. Retrieved June 12, 2016.
 Lovejoy, Ben (June 10, 2016). "As Apple moves into the energy business, it 
gets approval to turn landfill gas into power". 9to5Mac. Retrieved June 14, 
2016.
 McMillan, Robert (May 17, 2012). "After Greenpeace Protests, Apple Promises to 
Dump Coal Power". Wired. Retrieved August 22, 2013.
 "Powering Our Facilities with Clean, Renewable Energy". Wired Magazine. 
Archived from the original on April 22, 2014. Retrieved August 22, 
2013.[verification needed]
 Burrows, Peter (March 21, 2013). "Apple Says Data Centers Now Use 100% 
Renewable Energy". Business Week. Archived from the original on December 2, 
2013. Retrieved August 30, 2013.
 "Climate Counts scorecard". Climatecounts.org. Archived from the original on 
January 4, 2009. Retrieved October 7, 2011.
 "Environmental Group Hits Apple". Information Week.
 "ClickClean". Click Clean. Greenpeace. Retrieved April 28, 2016.
 "Environment". Apple. Retrieved May 26, 2016.
 Cardwell, Diane (August 23, 2016). "Apple Becomes a Green Energy Supplier, 
With Itself as Customer". The New York Times. Archived from the original on 
November 29, 2016. Retrieved December 30, 2016. clean power often does not flow 
directly to their facilities. They typically buy the renewable energy in 
amounts to match what they draw from the grid. They're actually getting power 
from their local utility, which may be coal
 Cole, Nicki Lisa (August 5, 2015). "Why Is Apple Lying About Powering Its Data 
Centers With Renewable Energy?". Truthout. Archived from the original on June 
30, 2016. Retrieved December 30, 2016. Apple buys renewable energy certificates 
to offset its reliance on Duke's dirty energy. ..purchasing offsets is not the 
same as actually powering something with renewable energy
 "Environment – Climate Change". Why we measure our carbon footprint so 
rigorously. Apple Inc. March 2016. Retrieved March 27, 2016.
 "How Much Water Do Apple Data Centers Use?". Data Center Knowledge. June 15, 
2016. Retrieved November 5, 2016.
 "Apple Environmental Responsibility Report (2015)" (PDF). Apple Inc. Retrieved 
March 30, 2016.
 "Apple Environmental Responsibility Report 2016 Progress Report, Covering 
Fiscal Year 2015" (PDF). Apple Inc.
 "Watch the Apple Special Event". Apple. Retrieved March 30, 2016.
 Sumra, Husain. "Supplier Lens Technology Commits to 100 Percent Renewable 
Energy for Apple Manufacturing". MacRumors. Retrieved August 17, 2016.
 "Apple Announces Environmental Progress in China & Applauds Supplier 
Commitment to Clean Energy" (Press release). Apple Inc. August 17, 2016. 
Retrieved August 17, 2016.
 "iTox + iWaste". Greenpeace. Archived from the original on July 21, 2008. 
Retrieved August 12, 2008.
 "Apple – Environment – Update". Apple Inc. Archived from the original on 
November 15, 2010. Retrieved November 22, 2010.
 "Which companies are phasing out PVC and BFRs". Greenpeace International. 
Archived from the original on November 10, 2010. Retrieved January 13, 2011.
 "Apple – Environment – Environmental Progress". Archived from the original 
on November 24, 2010. Retrieved November 22, 2010.
 "Apple – A Greener Apple". Archived from the original on July 25, 2008. 
Retrieved August 12, 2008.
 "Apple – Mac – Green Notebooks". Apple Inc. 2008. Archived from the 
original on December 22, 2008. Retrieved December 24, 2008.
 "Apple: MacBook Pro Graphics". Archived from the original on June 2, 2007. 
Retrieved June 8, 2007.
 "Apple – Environment – Reports". Apple Inc.
 "iMac and the Environment". Apple Inc. Archived from the original on November 
29, 2010. Retrieved November 29, 2010.
 "Energy Star Computers Final Program Requirements" (PDF). Energy Star. EPA. 
March 2016. Retrieved March 30, 2016.
 Slivka, Eric (November 9, 2011). "Apple Jumps to Fourth in Greenpeace's 
Environmental Rankings of Electronics Companies". MacRumors. Retrieved April 
18, 2017.
 "Apple ranks fourth on Greenpeace's 'Guide to Greener Electronics'". 
AppleInsider. November 9, 2011. Retrieved April 18, 2017.
 "Apple, 4th position, 4.6/10" (PDF). Greenpeace. Retrieved April 18, 2017.
 Anderson, Ash. "Apple Power Cables to Become Even More Environmentally 
Friendly". KeyNoodle. Retrieved January 14, 2012.
 "Environment – Reports". Apple Inc. Retrieved May 28, 2016.
 Chen, Liyan (May 11, 2015). "The World's Largest Tech Companies: Apple Beats 
Samsung, Microsoft, Google". Forbes. Retrieved May 24, 2017.
 "IDC: Smartphone shipments down 6.3% in Q4 2017, Apple overtakes Samsung for 
top spot". VentureBeat. February 2, 2018. Retrieved March 10, 2018.
 "Apple Passes Samsung to Capture the Top Position in the Worldwide Smartphone 
Market While Overall Shipments Decline 6.3% in the Fourth Quarter, According to 
IDC". IDC. Archived from the original on September 20, 2018. Retrieved March 
10, 2018.
 Nuttall, Chris (December 29, 2011). "Apple in race to keep ahead in 2012". 
Financial Times.
 Tsukayama, Hayley (March 20, 2012). "FAQ: Apple's Dividend". The Washington 
Post. Retrieved March 21, 2012.
 "Annual Financials for Apple". Marketwatch. Retrieved February 2, 2014.
 Rodriguez, Salvador (May 6, 2013). "Apple makes Fortune 500's top 10 for first 
time; Facebook makes list". Los Angeles Times. Retrieved June 10, 2013.
 La Monica, Paul R. (July 22, 2015). "Apple has $203 billion in cash. Why?". 
CNNMoney. Archived from the original on August 18, 2015. Retrieved September 
28, 2015.
 Farivar, Cyrus (July 13, 2015). "Apple makes 92 percent of all smartphone 
profits". Ars Technica. Retrieved March 23, 2017.
 Mickle, Tripp (April 30, 2017). "Apple's Cash Hoard Set to Top $250 Billion". 
The Wall Street Journal. Retrieved May 24, 2017.
 Wang, Christine (May 2, 2017). "Apple's cash hoard swells to record $256.8 
billion". CNBC. Retrieved May 24, 2017.
 "Fortune 500 Companies 2018: Who Made the List". Fortune. Retrieved November 
9, 2018.
 "2000 Annual Report" (PDF).
 "2001 Annual Report" (PDF).
 "2002 Annual Report" (PDF).
 "2003 Annual Report" (PDF).
 "2004 Annual Report" (PDF).
 "2005 Annual Report" (PDF).
 "2006 Annual Report" (PDF).
 "2007 Annual Report" (PDF).
 "2008 Annual Report" (PDF).
 "2009 Annual Report" (PDF).
 "2010 Annual Report" (PDF).
 "2011 Annual Report" (PDF).
 "2012 Annual Report" (PDF).
 "2013 Annual Report" (PDF).
 "2014 Annual Report" (PDF).
 Neate, Rupert (October 27, 2015). "Apple calls 2015 'most successful year 
ever' after making reported $234bn". The Guardian. Retrieved November 10, 2018.
 "Apple just had its first annual revenue decline since 2001". The Verge. 
Retrieved November 10, 2018.
 Balakrishnan, Anita (November 2, 2017). "Apple blows past Wall Street 
expectations as the iPhone 8 becomes a surprise best-seller". CNBC. Retrieved 
November 10, 2018.
 Cohen, Jessica Kim. "Apple's revenue surpasses $260B in 2018, up 16% from last 
year: 4 things to know". www.beckershospitalreview.com. Retrieved November 10, 
2018.
 Duhigg, Charles; Kocieniewski, David (April 28, 2012). "How Apple Sidesteps 
Billions in Taxes". The New York Times. Retrieved April 29, 2012.
 Watson, Roland (October 30, 2012). "Foreign companies 'avoid billions in 
corporation tax'". The Times.
 Ebrahimi, Helia (November 2, 2012). "Foreign firms could owe UK £11bn in 
unpaid taxes". The Telegraph. London: Telegraph Media Group.
 "Investor Relations". Apple Inc. Retrieved April 28, 2016.
 Knop, Carsten (November 14, 2017). "Tim Cook im Interview: "Hoffentlich seid 
ihr Deutschen richtig stolz auf euch"" (in German). Retrieved March 26, 2019.
 Drawbaugh, Kevin; Temple-West, Patrick. "Untaxed U.S. corporate profits held 
overseas top $2.1 trillion: study". Reuters. Retrieved February 11, 2015.
 "Apple Earnings Call". Apple Inc. Retrieved April 28, 2016.
 "National Income and Expenditure Annual Results 2015". Central Statistics. 
July 12, 2016.
 Kanter, James and Scott, Mark (August 30, 2016) Apple Must Pay Billions for 
Tax Breaks in Ireland, E.U. Orders The New York Times.
 Foroohar, Rana (August 30, 2016). "Apple vs. the E.U. Is the Biggest Tax 
Battle in History". TIME.com. Retrieved November 14, 2016.
 Taylor, Cliff (September 2, 2016). "Apple's Irish company structure key to EU 
tax finding". The Irish Times. Retrieved November 14, 2016.
 "Statement by Commissioner Vestager on state aid decision that Ireland's tax 
benefits for Apple were illegal" (Press release). European Commission. August 
30, 2016. Retrieved September 2, 2016.
 "Amazon 'pays less tax than sausage stall'". BBC News. September 2, 2016. 
Retrieved September 3, 2016.
 Beesley, Arthur (April 24, 2018). "Apple to start paying €13bn to Ireland 
over back tax claim". Financial Times. Retrieved May 5, 2018.
 "Apple Leadership". Apple Inc. Retrieved March 29, 2019.
 "When Stephen Fry met Jony Ive: the self-confessed tech geek talks to Apple's 
newly promoted chief design officer" The Telegraph, May 2015.
 Eadicicco, Lisa. "Jony Ive's departure may be a sign that one of Tim Cook's 
top lieutenants is becoming even more powerful". Business Insider. Retrieved 
July 15, 2019.
 "Federal Court Cases Involving Apple, Inc". Docket Alarm, Inc. Retrieved May 
10, 2014.
 Mullin, Joe (January 26, 2016). "Patent troll VirnetX wants jury to give it a 
half-billion dollars of Apple's cash". Ars Technica. Retrieved March 23, 2017.
 Novet, Jordan (December 21, 2016). "Nokia sues Apple for patent infringement 
in the U.S. and Germany". VentureBeat. Retrieved March 23, 2017.
 Swartz, Jon (December 21, 2016). "Nokia sues Apple for patent infringement". 
USA Today. Retrieved March 23, 2017.
 Orlowski, Andrew (November 15, 2017). "US trade cops agree to investigate 
Apple's 'embrace and extend". The Register. Retrieved November 16, 2017.
 Vincent, James (June 13, 2016). "Apple promises to deliver AI smarts without 
sacrificing your privacy". The Verge. Vox Media. Retrieved December 9, 2017.
 Heisler, Yoni (May 22, 2017). "Apple is expertly trolling Android users with 
its new iPhone ads". BGR. Penske Media Corporation. Retrieved December 9, 2017.
 Greenberg, Andy (June 8, 2015). "Apple's latest selling point: how little it 
knows about you". Wired. Retrieved December 9, 2017.
 Farivar, Cyrus (September 18, 2014). "Apple expands data encryption under iOS 
8, making handover to cops moot". Ars Technica. Retrieved December 9, 2017.
 Hall, Zac (November 16, 2017). "Apple details how it performs on-device facial 
detection in latest machine learning journal entry". 9to5Mac. Retrieved 
December 9, 2017.
 Greenberg, Andy (June 13, 2016). "Apple's 'differential privacy' is about 
collecting your data – but not your data". Wired. Retrieved December 9, 2017.
 Rossignol, Joe (December 6, 2017). "Here's How Apple Improves the iOS and Mac 
User Experience While Protecting Your Privacy". MacRumors. Retrieved December 
9, 2017.
 Pagliery, Jose (February 22, 2016). "Apple promises privacy – but not on 
iCloud". CNN. Retrieved December 13, 2017.
 Cunningham, Andrew (February 24, 2016). "The case for using iTunes, not 
iCloud, to back up your iPhone". Ars Technica. Condé Nast. Retrieved December 
13, 2017.
 Robertson, Adi (September 12, 2017). "Why Face ID won't give you the legal 
protection of a passcode". The Verge. Vox Media. Retrieved December 13, 2017.
 D'Orazio, Dante (November 23, 2014). "Apple partners with app developers for 
major Product RED fundraising effort". The Verge. Retrieved April 18, 2017.
 Chmielewski, Dawn (December 17, 2014). "Apple's Holiday Product Red Campaign 
Raises $20 Million for AIDS Research". Recode. Retrieved April 18, 2017.
 Clover, Juli (December 17, 2014). "Apple's (Product) RED Holiday Campaign 
Raised $20 Million to Fight AIDS". MacRumors. Retrieved April 18, 2017.
 Miller, Chance (March 21, 2017). "Apple officially announces (RED) iPhone 7 & 
7 Plus, updated iPhone SE with double the storage". 9to5Mac. Retrieved April 
18, 2017.
 Warren, Tom (March 21, 2017). "Apple launches red iPhone 7". The Verge. 
Retrieved April 18, 2017.
 Weintraub, Seth (November 9, 2011). "Apple donates $2.5M to Hurricane Sandy 
relief". 9to5Mac. Retrieved November 18, 2012.
 "Apple donates $5M to Hand in Hand Hurricane Irma/Harvey relief, sets up 
iTunes donations". 9to5Mac. September 8, 2017. Retrieved December 13, 2017.
 Miller, Chance (September 21, 2017). "Tim Cook says Apple is donating $1 
million to earthquake recovery efforts in Mexico". 9to5Mac. Retrieved December 
13, 2017.
 Weintraub, Seth (January 14, 2010). "Apple sets up Haiti donation page in 
iTunes". 9to5Mac. Retrieved December 13, 2017.
 Gurman, Mark (March 12, 2011). "Apple now taking Red Cross donations through 
iTunes for Japan relief fund". 9to5Mac. Retrieved December 13, 2017.
 Lovejoy, Ben (November 12, 2013). "Apple invites donations to American Red 
Cross to support Philippine typhoon relief". 9to5Mac. Retrieved December 13, 
2017.
 Lovejoy, Ben (September 18, 2015). "Apple invites Red Cross donations through 
iTunes to help the Mediterranean refugee crisis [Updated]". 9to5Mac. Retrieved 
December 13, 2017.
 Miller, Chance (August 27, 2017). "Apple now accepting donations via iTunes 
for Hurricane Harvey relief efforts". 9to5Mac. Retrieved December 13, 2017.
 "Help the planet. One app at a time". World Wildlife Fund. Retrieved April 14, 
2016.
 "Environment". Apple Inc. Retrieved April 14, 2016.
 "Global Apps for Earth campaign with WWF raises more than $8M" (Press 
release). Apple Inc. Retrieved June 18, 2016.
 "Tech billionaires including Tim Cook, Elon Musk, and Mark Zuckerberg promised 
18 million masks to fight COVID-19". Business Insider. Retrieved March 23, 
2020.
 Tsotsis, Alexia (June 18, 2013). "Why Was Apple Late To The PRISM Party?". 
TechCrunch.
 Orlowski, Andrew (May 4, 2006), "Apple sues itself in the foot (again)", The 
Register
 "The Stark Reality of iPod's Chinese Factories", Daily Mail, London, August 
18, 2006
 Musgrove, Mike (June 16, 2006), "Sweatshop Conditions at IPod Factory 
Reported", The Washington Post
 Kahney, Leander (June 13, 2006), "Judging Apple Sweatshop Charge", Wired
 Dernbach, Christoph (June 12, 2008), Steve Jobs: Good artists copy great 
artists steal, YouTube, retrieved December 11, 2016
 "Mergers & Acquisitions". aaplinvestors.net. Retrieved December 11, 2016.
 Orlowski, Andrew (September 13, 2005), "Apple shot first, asked question 
later, say sued sites", The Register
 Flores, Marc, Over 5000 apps stricken from the Apple app store, new rules in 
place, MobileCrunch, February 20, 2010.
 "European Borders Fracture iTunes". PCWorld. April 4, 2007. Retrieved December 
21, 2018.
 Kafka, Peter (June 30, 2016). "Spotify says Apple won't approve a new version 
of its app because it doesn't want competition for Apple Music". Recode. 
Retrieved December 21, 2018.
 Campbell, Mikey (June 9, 2015). "Apple, record labels under scrutiny for 
collusion in New York and Connecticut". AppleInsider. Retrieved December 21, 
2018.
 Duhigg, Charles; Kocieniewski, David (April 28, 2012). "Apple's Tax Strategy 
Aims at Low-Tax States and Nations". The New York Times. ISSN 0362-4331. 
Retrieved December 28, 2018.
 "Senate Probe Finds Apple Used Unusual Tax Structure to Avoid Taxes". CNBC. 
May 20, 2013. Retrieved December 28, 2018.
 "Apple vs. the E.U. Is the Biggest Tax Battle in History". Time. Retrieved 
December 28, 2018.
 "Apple recalls older 15-inch MacBook Pros because the batteries could catch 
fire". The Verge. June 20, 2019.
 "Apple recalls some 2015–2017 15-inch MacBook Pros over battery flaw". 
VentureBeat. June 20, 2019.
 "MacBook Pro Catches Fire While Allegedly Under 'Normal Use'". Laptop Mag. 
June 3, 2019.
 "Irish Regulator Opens Third Privacy Probe Into Apple". Gadgets360. July 3, 
2019.
 "Data Protection Commission opens privacy investigation into Apple". RTE. July 
2, 2019.
 "The Government Wants to Tackle Big Tech's Repair Monopolies and Planned 
Obsolescence". Vice. July 17, 2019.
 "Can't Fix Your Smartphone? The Right-To-Repair Movement Wants To Change 
That". On Point. July 3, 2019.
 "Nixing the Fix: A Workshop on Repair Restrictions". Federal Trade Commission. 
Retrieved July 24, 2019.
 "Justice Department to Open Broad, New Antitrust Review of Big Tech 
Companies". July 23, 2019.
 "Justice Department Opens Sweeping Antitrust Review of Big Tech". Forbes. 
Retrieved July 24, 2019.
 "Apple Seems to Be Tracking iPhone 11 When Location Services Are Disabled, 
Report Finds". Gizmodo. Retrieved December 5, 2019.
 "Pensacola Shooting: Technical Feud Between FBI and Apple Repeats History". 
Raven Tribune. Retrieved January 16, 2020.
 "France hits Apple with €1.1bn antitrust fine". Financial Times. March 16, 
2020. Retrieved March 16, 2020.

 Microsoft
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
Microsoft Corporation
A square divided into four sub-squares, colored red, green, yellow and blue 
(clockwise), with the company name appearing to its right.
Building92microsoft.jpg
Building 92 on the Microsoft Redmond campus
Type
Public
Traded as	
NASDAQ: MSFT
NASDAQ-100 component
DJIA component
S&P 100 component
S&P 500 component
ISIN	US5949181045
Industry	
Software development
Computer hardware
Consumer electronics
Social networking service
Cloud computing
Video games
Internet